{
  "Phase 1: Foundational Framework and Core Orchestrator": {
    "Task 1.1: Initialize core project structure, including package.json, tsconfig.json, and dependency management.": [
      {
        "step 1": "Initialize a new Node.js project. Use the `npm init -y` command to create a default `package.json` file. This will serve as the manifest for our project, defining its metadata and dependencies."
      },
      {
        "step 2": "Install the necessary TypeScript-related development dependencies. Use npm to install `typescript`, `ts-node`, and `@types/node`. These are required for compiling and running our TypeScript code. Ensure they are saved as dev dependencies. Command: `npm install --save-dev typescript ts-node @types/node`."
      },
      {
        "step 3": "Create the TypeScript configuration file, `tsconfig.json`, in the project root. This file specifies the compiler options required to compile the project. Populate it with a modern, strict configuration suitable for a Node.js backend application. Use the following configuration: \n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"CommonJS\",\n    \"rootDir\": \"./src\",\n    \"outDir\": \"./dist\",\n    \"esModuleInterop\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"strict\": true,\n    \"skipLibCheck\": true,\n    \"resolveJsonModule\": true,\n    \"sourceMap\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"**/*.spec.ts\", \"**/*.test.ts\"]\n}"
      },
      {
        "step 4": "Create the core source directory structure as defined in `tsconfig.json`. Create a directory named `src` in the project root. Inside the `src` directory, create an initial entry point file named `index.ts`."
      },
      {
        "step 5": "Populate the `src/index.ts` file with a simple 'Hello, World!' style message to verify that the project setup works. Add the following code: `console.log('Autonomous Software Development Framework Orchestrator starting up...');`"
      },
      {
        "step 6": "Install additional development tooling for a better developer experience. Install `nodemon` as a dev dependency to automatically restart the application on file changes. Command: `npm install --save-dev nodemon`."
      },
      {
        "step 7": "Update the `scripts` section of your `package.json` file to include commands for building, starting, and developing the application. Add the following scripts:\n- `\"build\": \"tsc\"`\n- `\"start\": \"node dist/index.js\"`\n- `\"dev\": \"nodemon --watch 'src/**/*.ts' --exec 'ts-node' src/index.ts\"`"
      },
      {
        "step 8": "Create a `.gitignore` file in the project root to prevent common generated files, sensitive information, and local dependencies from being committed to version control. Add the following entries:\n/node_modules\n/dist\n.env\nnpm-debug.log\n.DS_Store"
      },
      {
        "step 9": "Install the first core production dependency, `zod`. This library will be fundamental for enforcing the 'Schema-Enforced Communication' principle outlined in the system architecture. Command: `npm install zod`."
      }
    ],
    "Task 1.2: Define all core JSON schemas (Reasoning Tree, Module Contract, Contract Document, Builder Output).": [
      {
        "step 1": "Create a new directory `src/schemas` to house all the JSON schema definitions and their corresponding TypeScript types for the project."
      },
      {
        "step 2": "Create a new file named `src/schemas/reasoning_tree.schema.ts`. In this file, define and export a JSON schema constant named `REASONING_TREE_NODE_SCHEMA`. This schema will represent a node in the hierarchical plan. According to the system overview, this plan is a tree of Goal -> Phases -> Tasks -> Steps. A node should have the following properties:\n- `type`: A string enum that can be one of 'goal', 'phase', 'task', or 'step'.\n- `description`: A string detailing the node's purpose.\n- `children`: An array of child nodes, which should recursively conform to this same schema.\nAfter defining the schema, also define and export a corresponding TypeScript type `ReasoningTreeNode` for use within the orchestrator. The root of a complete plan will be a single `ReasoningTreeNode` of type 'goal'."
      },
      {
        "step 3": "Create a new file named `src/schemas/module_contract.schema.ts`. In this file, you will define the most critical schema: `MODULE_CONTRACT_SCHEMA`. This schema is the blueprint for a single, isolated software module. Based on the system architecture document, define and export it as a JSON schema constant with the following properties:\n- `moduleName`: A string for the module's unique identifier (e.g., 'AuthenticationService').\n- `purpose`: A detailed string explaining what the module does.\n- `dependencies`: An array of strings, where each string is the `moduleName` of another module this one depends on.\n- `constructorParams`: An array of objects, each with `name` (string) and `type` (string, e.g., 'AuthenticationService'), describing parameters for dependency injection.\n- `publicAPI`: An object defining the exposed parts, containing:\n    - `functionSignatures`: An array of strings, each representing a public function signature (e.g., 'login(user: User): Promise<string>').\n    - `dataStructures`: An array of strings, each a self-contained TypeScript interface or type definition required by the public API (e.g., 'interface User { id: string; }').\n- `internalLogic`: An object describing the private implementation details, containing:\n    - `promptInstructions`: An array of strings containing detailed, step-by-step implementation instructions derived from the reasoning tree.\n    - `acceptanceTests`: An array of strings describing high-level acceptance criteria or test cases (e.g., 'Should return a JWT on successful login').\nFinally, define and export the corresponding TypeScript type `ModuleContract`."
      },
      {
        "step 4": "Create a new file named `src/schemas/contract_document.schema.ts`. This schema will represent the full architectural plan, which is a collection of module contracts. In this file:\n1. Import the `MODULE_CONTRACT_SCHEMA` constant and the `ModuleContract` type from `./module_contract.schema.ts`.\n2. Define and export a JSON schema constant named `CONTRACT_DOCUMENT_SCHEMA`. This schema must describe an object with a single top-level property: `modules`, which is an array where each item must conform to the imported `MODULE_CONTRACT_SCHEMA`.\n3. Define and export the corresponding TypeScript type `ContractDocument`."
      },
      {
        "step 5": "Create a new file named `src/schemas/builder_output.schema.ts`. This file will define the schema for the JSON object returned by a Module Builder agent after it generates code. In this file:\n1. Define and export a JSON schema constant named `BUILDER_OUTPUT_SCHEMA`. It must describe an object with two required string properties:\n    - `implementationCode`: A string to hold the generated source code for the module.\n    - `testCode`: A string to hold the generated test suite for the module.\n2. Define and export the corresponding TypeScript type `BuilderOutput`."
      },
      {
        "step 6": "To ensure clean and manageable imports throughout the application, create an `index.ts` file inside the `src/schemas` directory. This file should act as a barrel file, exporting all the schemas and types you've just created from `reasoning_tree.schema.ts`, `module_contract.schema.ts`, `contract_document.schema.ts`, and `builder_output.schema.ts`. Use the `export * from './filename';` syntax for each file."
      }
    ],
    "Task 1.3: Implement a configuration module for managing API keys, model names, and system settings.": [
      {
        "step 1": "Create a new directory named 'config' inside the 'src' directory. This directory will house all the configuration-related logic for the application."
      },
      {
        "step 2": "Install the necessary libraries for type-safe configuration management. Use pip to install 'pydantic' and 'pydantic-settings'. Add these libraries to your 'requirements.txt' file."
      },
      {
        "step 3": "Create a new file named 'settings.py' inside the 'src/config' directory. In this file, you will define the application's configuration model using Pydantic."
      },
      {
        "step 4": "In 'src/config/settings.py', define a class named 'Settings' that inherits from 'pydantic_settings.BaseSettings'. This class will define all the configuration variables for the system. Include the following fields:\n- `OPENAI_API_KEY`: Use `pydantic.SecretStr` for this to prevent accidental exposure in logs.\n- `PLANNER_MODEL_NAME`: A `str` with a default value of 'gpt-4-turbo-preview'.\n- `ARCHITECT_MODEL_NAME`: A `str` with a default value of 'gpt-4-turbo-preview'.\n- `BUILDER_MODEL_NAME`: A `str` with a default value of 'gpt-4-turbo-preview'.\n- `CORRECTOR_MODEL_NAME`: A `str` with a default value of 'gpt-4-turbo-preview'.\n- `MAX_CORRECTION_RETRIES`: An `int` with a default value of 3.\n- `PROJECT_DIR`: A `str` with a default value of 'generated_projects'.\n\nConfigure the class's `model_config` to load variables from a '.env' file."
      },
      {
        "step 5": "At the bottom of 'src/config/settings.py', create a single, globally accessible instance of your `Settings` class. Name this instance 'settings'. This will act as a singleton that can be imported throughout the application."
      },
      {
        "step 6": "Create a file named '__init__.py' inside the 'src/config' directory. In this file, import the 'settings' instance from your 'settings.py' module and expose it for easier importing. The file should contain the line: `from .settings import settings`."
      },
      {
        "step 7": "In the root directory of the project, create a file named '.env.example'. This file will serve as a template for users. List all the environment variables defined in your `Settings` class, especially the required `OPENAI_API_KEY`, with placeholder values. For example: `OPENAI_API_KEY=\"your_api_key_here\"`."
      },
      {
        "step 8": "Ensure that the actual '.env' file is not committed to version control. Add a line with `.env` to your project's `.gitignore` file."
      },
      {
        "step 9": "Create a new test file 'tests/test_config.py'. Write unit tests using 'pytest' to verify the configuration module. Use 'pytest.monkeypatch' to set and unset environment variables during tests. Your tests should cover:\n1. Successful loading of settings from environment variables.\n2. Pydantic raising a `ValidationError` if a required variable like `OPENAI_API_KEY` is not set.\n3. Verification that default values are correctly applied when corresponding environment variables are not set."
      },
      {
        "step 10": "Update the main `README.md` file. Add a new 'Configuration' section that explains how to set up the project. Instruct the user to copy `.env.example` to `.env` and fill in their `OPENAI_API_KEY`."
      }
    ],
    "Task 1.4: Create a generic LLM client abstraction for handling API requests, responses, and retries.": [
      {
        "step 1": "First, set up the directory structure for the new LLM client module. Create a new directory `src/llm_client`. Inside this directory, create the following empty files: `__init__.py`, `base.py`, `config.py`, and `openai_client.py`. This structure will house the abstraction, the concrete implementation, and its configuration."
      },
      {
        "step 2": "In `src/llm_client/config.py`, define a Pydantic model named `LLMConfig` to hold the configuration for the LLM client. It should include fields for `api_key` (using `SecretStr` for security), `model_name` (e.g., 'gpt-4-turbo'), `temperature`, and `max_tokens`. Make all fields optional with sensible defaults, except for the `api_key`."
      },
      {
        "step 3": "In `src/llm_client/base.py`, define the abstract interface for all LLM clients. Create an abstract base class `LLMClient` using Python's `abc` module. It should have an `__init__` that accepts an `LLMConfig` object. Define an abstract method `create_chat_completion(self, messages: list[dict], response_model: type[BaseModel] | None = None) -> BaseModel | str`. This method will be the core function for interacting with the LLM, with `response_model` being an optional Pydantic model to enforce the structure of the LLM's output."
      },
      {
        "step 4": "Now, create the concrete implementation for OpenAI. In `src/llm_client/openai_client.py`, create a class `OpenAIClient` that inherits from `LLMClient`. Implement the `__init__` method to accept the `LLMConfig` and initialize the official `openai` client. For the `create_chat_completion` method, implement the basic logic to call the OpenAI API. If a `response_model` is provided, add a system message instructing the model to respond in JSON format matching the provided schema. Hint: You can get the schema from `response_model.model_json_schema()`."
      },
      {
        "step 5": "Enhance the `OpenAIClient` with robust error handling and automatic retries. Install the `tenacity` library. Use the `@retry` decorator from `tenacity` on the `create_chat_completion` method (or a private helper method that makes the actual API call). Configure the retry logic to handle specific exceptions like `openai.RateLimitError`, `openai.APITimeoutError`, and `openai.APIStatusError` (for 5xx codes). Use `wait_exponential` for the backoff strategy and `stop_after_attempt` to limit the number of retries."
      },
      {
        "step 6": "Implement the response parsing and validation logic within `create_chat_completion` in `openai_client.py`. After receiving the response from the OpenAI API, check if a `response_model` was provided. If so, attempt to parse the JSON content of the response into an instance of that Pydantic model. Wrap this parsing logic in a `try...except` block to catch `pydantic.ValidationError` and `json.JSONDecodeError`. If parsing fails, you should raise a custom exception, for example, `LLMResponseParsingError`. If no `response_model` is given, return the raw string content of the response."
      },
      {
        "step 7": "Create custom exceptions to improve error signaling. In a new file `src/llm_client/exceptions.py`, define a base exception `LLMClientError` and a more specific exception `LLMResponseParsingError` that inherits from it. Use these exceptions in your `openai_client.py` implementation."
      },
      {
        "step 8": "Prepare for testing. Create a new test file `tests/test_llm_client.py`. In this file, set up a `unittest.TestCase` class. You will need to mock the OpenAI API to avoid making real network calls. Use `unittest.mock.patch` to mock the `openai.resources.chat.completions.Completions.create` method."
      },
      {
        "step 9": "Write unit tests in `tests/test_llm_client.py` to cover the main success scenarios. First, test the case where a `response_model` is provided. Configure your mock to return a valid JSON string that matches the schema of a sample Pydantic model. Assert that the client returns a correctly parsed Pydantic object. Second, test the case where no `response_model` is provided and assert that the raw string response is returned."
      },
      {
        "step 10": "Write unit tests to verify the error handling and retry logic. Configure your mock to raise `openai.RateLimitError` on the first call and then return a successful response on the second call. Use `mock_obj.call_count` to assert that the API call was attempted twice. Also, write a test where the mock returns a malformed JSON string and assert that your custom `LLMResponseParsingError` is raised."
      },
      {
        "step 11": "Finally, clean up the module's public interface. In `src/llm_client/__init__.py`, import and export the key classes and exceptions so they can be easily accessed from other parts of the application. For example: `from .base import LLMClient`, `from .openai_client import OpenAIClient`, `from .config import LLMConfig`, `from .exceptions import LLMClientError, LLMResponseParsingError`. Review all new files for proper type hinting, docstrings, and code clarity."
      }
    ],
    "Task 1.5: Implement the main Orchestrator class/script to manage the sequential flow through all four stages.": [
      {
        "step 1": "Create a new file at `src/types.ts`. Based on the system architecture document, define the core data structures and interfaces that will be passed between stages. Create placeholder interfaces for `ReasoningTree`, `ModuleContract`, `GeneratedModule`, and `FinalProject`. For `ModuleContract`, include fields like `moduleName`, `purpose`, `dependencies`, `functionSignatures`, `promptInstructions`, and `acceptanceTests` as described in the 'Contract Generation' stage."
      },
      {
        "step 2": "Create the main orchestrator file at `src/orchestrator.ts`. Define an `Orchestrator` class. Import the types from `src/types.ts`. The class should have private properties to hold the artifacts from each stage (e.g., `private reasoningTree: ReasoningTree | null = null;`, `private contracts: ModuleContract[] | null = null;`, etc.). Also, create an empty public async method `run(initialGoal: string): Promise<void>`."
      },
      {
        "step 3": "To simulate the work of different agents without implementing their full logic yet, create a file at `src/agents.ts`. In this file, define placeholder 'Agent' classes for each stage: `PlannerAgent`, `InterfaceArchitectAgent`, `ModuleBuilderAgent`, and `CorrectorAndIntegratorAgent`. Each class should have a single public async `execute` method. These methods should return mock data that conforms to the interfaces in `src/types.ts`. For example, `InterfaceArchitectAgent.execute` should accept a mock `ReasoningTree` and return a hardcoded array of `ModuleContract` objects."
      },
      {
        "step 4": "In the `Orchestrator` class, implement the private methods for the first two stages: `private async runStage0_Planning(goal: string): Promise<void>` and `private async runStage1_ContractGeneration(): Promise<void>`. In each method, instantiate the corresponding agent from `src/agents.ts`, call its `execute` method, store the result in the appropriate class property (e.g., `this.reasoningTree = ...`), and add console logs to indicate the start and successful completion of the stage. Wrap the logic in a `try...catch` block for error handling."
      },
      {
        "step 5": "In the `Orchestrator` class, implement the private methods for the final two stages: `private async runStage2_Implementation(): Promise<void>` and `private async runStage3_Integration(): Promise<void>`. For `runStage2_Implementation`, simulate the parallel nature by iterating through `this.contracts` and calling the `ModuleBuilderAgent.execute` method for each, collecting the results. For `runStage3_Integration`, pass the collected modules to the `CorrectorAndIntegratorAgent.execute` method. As before, add logging and error handling for each stage."
      },
      {
        "step 6": "Now, fully implement the main `public async run(initialGoal: string)` method in the `Orchestrator` class. This method will act as the master controller. It should call the private stage methods (`runStage0_Planning`, `runStage1_ContractGeneration`, etc.) in the correct sequential order. Ensure that the output of one stage is available for the next (by checking that the corresponding class property is not null). Wrap the entire sequence of calls in a single `try...catch` block to handle any failure during the process and log a final 'Orchestration completed' or 'Orchestration failed' message."
      },
      {
        "step 7": "Create a main entry point file at `src/main.ts`. This script will initialize and run the entire system. Import the `Orchestrator` class. Instantiate it, define a sample high-level user goal (e.g., 'Build a real-time chat application with a React frontend'), and invoke the `orchestrator.run(goal)` method. Add a top-level `.catch()` to the invocation to handle any unhandled promise rejections from the orchestrator."
      },
      {
        "step 8": "To make the simulation tangible, create a utility for saving artifacts. Create `src/utils.ts` and add an async function `saveArtifact(filePath: string, data: any): Promise<void>` that uses the `fs/promises` module to write data to the disk as a formatted JSON string. Create a directory named `build` at the root of the project. Update the `Orchestrator` stage methods to call this utility after each stage successfully completes, saving `reasoning_tree.json`, `contracts.json`, etc., into the `build` directory. This will allow you to inspect the mock data flow."
      }
    ],
    "Task 1.6: Implement a file-based artifact management system for passing data between stages (e.g., reasoning_tree.json, contracts.json).": [
      {
        "step 1": "Create a new directory `src/core`. Inside this new directory, create a Python file named `artifact_manager.py`. This module will encapsulate all file-based artifact handling logic."
      },
      {
        "step 2": "In `src/core/artifact_manager.py`, define a class named `ArtifactManager`. The constructor `__init__(self, base_path: str)` should accept the root directory path for storing artifacts. It should store this path as an instance attribute and immediately ensure the directory exists using `os.makedirs(base_path, exist_ok=True)`. Import the necessary modules: `os`, `json`, and `typing`."
      },
      {
        "step 3": "Within the `ArtifactManager` class, implement a `save` method with the signature `save(self, filename: str, data: typing.Union[dict, list]) -> None`. This method will take a filename and a JSON-serializable Python object, construct the full file path by joining the `base_path` with the `filename`, and write the data to that file as a pretty-printed JSON string (use `json.dump` with `indent=2` for readability). Include a comprehensive docstring and type hints."
      },
      {
        "step 4": "In the `ArtifactManager` class, implement a `load` method with the signature `load(self, filename: str) -> typing.Union[dict, list]`. This method will take a filename, construct the full file path, read the file, and parse it from JSON into a Python object using `json.load`. It should be designed to propagate the `FileNotFoundError` if the artifact does not exist, as the calling code will need to handle this case. Include a comprehensive docstring and type hints."
      },
      {
        "step 5": "Create a new test file at `tests/test_artifact_manager.py`. Using the `pytest` framework, write unit tests for the `ArtifactManager`. Use the `tmp_path` pytest fixture to supply a temporary directory to the `ArtifactManager` constructor during tests. Your test suite must verify the following behaviors: 1. The artifact directory is successfully created upon class initialization. 2. The `save` method creates a file with the correct content. 3. The `load` method correctly retrieves and deserializes the content from a saved file. 4. The `load` method correctly raises a `FileNotFoundError` when attempting to access a non-existent file."
      },
      {
        "step 6": "Integrate the `ArtifactManager` into the main system orchestrator, located at `src/core/orchestrator.py`. If this file doesn't exist, create a basic placeholder class `Orchestrator` within it. Perform the following refactoring: \n1. Import `ArtifactManager` from `src.core.artifact_manager`. \n2. In the `Orchestrator`'s `__init__` method, create an instance attribute `self.artifacts = ArtifactManager('artifacts')` to manage I/O. \n3. Modify the logic for Stage 0 (Planning). Instead of returning its result (`reasoning_tree_data`) in memory, it should now use `self.artifacts.save('reasoning_tree.json', reasoning_tree_data)` to persist the output. \n4. Modify the logic for Stage 1 (Contract Generation). Instead of receiving the `reasoning_tree` as a method argument, it must now load its input at the beginning of its execution using `reasoning_tree_data = self.artifacts.load('reasoning_tree.json')`. \n5. At the conclusion of Stage 1, save its output (`contracts_data`) to a file using `self.artifacts.save('contracts.json', contracts_data)`."
      }
    ],
    "Task 1.7: Integrate the 'Hierarchical Planner' logic as Stage 0 of the orchestrator.": [
      {
        "step 1": "First, establish the directory structure for Stage 0. Create a new directory: `src/stages/stage_0_planning`. This directory will house all the logic related to the hierarchical planning process."
      },
      {
        "step 2": "Recreate the core data models from the Hierarchical Planner inside the new directory. Create a file `src/stages/stage_0_planning/models.py`. In this file, define Pydantic models for `Step`, `Task`, `Phase`, and `Goal`. These models should form a nested structure, where a `Goal` contains a list of `Phases`, a `Phase` contains a list of `Tasks`, and a `Task` contains a list of `Steps`. Ensure the final top-level model, which will represent the entire reasoning tree, is clearly defined."
      },
      {
        "step 3": "Now, implement the core planning logic. Create a file `src/stages/stage_0_planning/planner.py`. In this file, create a `PlannerAgent` class. This class will be responsible for interacting with the LLM. It should have three primary methods: `plan_phases(goal: str) -> list[str]`, `plan_tasks(phase_description: str) -> list[str]`, and `plan_steps(task_description: str) -> list[str]`. These methods will take a high-level description and return a list of decomposed items. Use placeholder logic for now if direct LLM integration is not yet available."
      },
      {
        "step 4": "Create an orchestrator for Stage 0 that uses the models and the planner agent to build the complete reasoning tree. In `src/stages/stage_0_planning/planner.py`, create a function `generate_plan(goal: str) -> Goal`. This function will: \n1. Take the initial user goal.\n2. Instantiate `PlannerAgent`.\n3. Call `plan_phases` to get phase descriptions.\n4. For each phase description, call `plan_tasks` to get task descriptions.\n5. For each task description, call `plan_steps` to get step descriptions.\n6. Assemble the results into the nested `Goal` Pydantic model you defined in `models.py`."
      },
      {
        "step 5": "Create a formal interface for the entire stage that the main system orchestrator will interact with. Create a new file `src/stages/stage_0_planning/main.py`. In this file, define a class `PlanningStage`. This class should have a single public method: `execute(goal: str, output_path: str) -> str`. This method will: \n1. Call the `generate_plan` function from the previous step.\n2. Serialize the resulting `Goal` model to a JSON string.\n3. Save the JSON string to a file named `reasoning_tree.json` at the provided `output_path`.\n4. Return the full path to the created file."
      },
      {
        "step 6": "Integrate the `PlanningStage` into the main system orchestrator. Open the main orchestrator file (e.g., `src/orchestrator.py`). Import `PlanningStage` from `src/stages/stage_0_planning/main.py`. In the orchestrator's main execution flow, instantiate `PlanningStage` and call its `execute` method, passing the user's initial goal. Log the path of the returned `reasoning_tree.json` artifact."
      },
      {
        "step 7": "Review all the new files you've created for Stage 0 (`models.py`, `planner.py`, `main.py`). Identify all new dependencies (e.g., `pydantic`, `openai`) and add them to the project's dependency management file (`pyproject.toml` or `requirements.txt`)."
      },
      {
        "step 8": "Finally, create an integration test to verify that Stage 0 works as expected within the orchestrator. Create a test file, for example `tests/test_stage_0.py`. The test should: \n1. Define a sample user goal (e.g., 'build a simple CLI calculator').\n2. Call the main orchestrator's run method with this goal.\n3. Assert that a `reasoning_tree.json` file is created in the expected output directory.\n4. Optionally, load the JSON file and assert that its basic structure is not empty and contains the top-level goal. Use mocks for the LLM calls in `PlannerAgent` to ensure the test is fast and deterministic."
      }
    ],
    "Task 1.8: Implement the 'Interface Architect' agent (Stage 1) to generate the contract document from the planner's output.": [
      {
        "step 1": "First, we need to define the data structures that govern Stage 1. Create a new file at `src/schemas/contract.schema.ts`. Using the `zod` library, define a schema for a single `ModuleContract` and an all-encompassing `CONTRACT_DOCUMENT_SCHEMA` which will be an array of `ModuleContract` schemas. The `ModuleContract` schema should include fields like `name` (string), `purpose` (string), `dependencies` (array of strings), `constructorParams` (array of objects with name and type), `publicAPI` (array of objects representing functions), `promptInstructions` (string), and `acceptanceTests` (array of strings). Ensure all fields are strictly typed."
      },
      {
        "step 2": "Now, create the agent responsible for generating the contract. Create a new file at `src/agents/architect.agent.ts`. Define a class `InterfaceArchitectAgent`. This class should have a constructor that accepts an LLM client (e.g., an instance of `OpenAI`). It will also need a primary public method: `generateContract(reasoningTree: object): Promise<any>`. For now, leave the method implementation empty. Import the schemas you defined in the previous step."
      },
      {
        "step 3": "The core of the architect agent is the detailed system prompt that guides the LLM. Inside `src/agents/architect.agent.ts`, create a comprehensive `const SYSTEM_PROMPT`. This prompt must instruct the LLM to act as an expert 'Interface Architect'. It should explain that its input is a detailed procedural plan (`reasoning_tree.json`) and its output must be a JSON array of `ModuleContract` objects. Clearly instruct it on how to: 1. Analyze the plan to identify distinct, independent modules. 2. Determine dependencies between modules. 3. Translate procedural steps into declarative `promptInstructions` for a future coding agent. 4. Formulate plain-English `acceptanceTests` from the plan's requirements. 5. Strictly adhere to the JSON output format."
      },
      {
        "step 4": "Implement the `generateContract` method in `InterfaceArchitectAgent`. This method should: 1. Take the `reasoningTree` object as input. 2. Construct the user prompt by stringifying the `reasoningTree`. 3. Make a call to the LLM using the `SYSTEM_PROMPT` and the user prompt. Configure the LLM call to use its JSON output mode. 4. Receive the response, parse the JSON string into an object, and handle any potential JSON parsing errors. For now, return the parsed object directly."
      },
      {
        "step 5": "Create the validation logic as described in the system architecture. Create a new file `src/validation/contract.validator.ts`. Implement a function `validateContractDocument(contractData: unknown): { success: boolean; data?: any; error?: any }`. This function should first use `CONTRACT_DOCUMENT_SCHEMA.safeParse(contractData)` for schema validation. If that succeeds, it must then perform the 'Architectural Integrity' check: create a Set of all module names and iterate through each module's dependencies, ensuring every dependency string corresponds to a name in the Set. Return a structured result indicating success or failure with detailed errors."
      },
      {
        "step 6": "Integrate the validator into the `InterfaceArchitectAgent`. Modify the `generateContract` method in `architect.agent.ts`. After parsing the JSON response from the LLM, pass the resulting object to your new `validateContractDocument` function. If the validation fails, throw a specific, informative error that includes the validation issues. If it succeeds, return the validated contract document. This ensures the agent's output is always guaranteed to be valid."
      },
      {
        "step 7": "Now, let's wire the new agent into the main orchestrator. In your primary orchestration file (e.g., `src/main.ts` or `src/orchestrator.ts`), add the logic for 'Stage 1'. After successfully generating the `reasoning_tree.json` from Stage 0: 1. Instantiate the `InterfaceArchitectAgent`. 2. Load the `reasoning_tree.json` file. 3. Call the `architectAgent.generateContract()` method with the reasoning tree data. 4. Use a try/catch block to handle potential errors from the agent. 5. Upon success, save the validated contract document to the project's output directory as `contract.json`. Log the progress clearly to the console."
      },
      {
        "step 8": "Finally, write unit tests to ensure the robustness of your new components. Create a test file `src/validation/contract.validator.test.ts`. Write tests for `validateContractDocument` covering a valid contract, a schema-invalid contract, and a contract with a dangling dependency (an architectural integrity failure). Then, create `src/agents/architect.agent.test.ts`. Mock the LLM client and test the `InterfaceArchitectAgent`, ensuring it correctly calls the LLM, parses the response, and invokes the validator, throwing an error when the validator fails."
      }
    ],
    "Task 1.9: Implement the automated contract validation logic (Schema, Internal Consistency, Graph Validation).": [
      {
        "step 1": "Create two new files for the contract validation logic: `src/validation/contractValidator.ts` for the implementation and `src/validation/contractValidator.test.ts` for the unit tests. Also, ensure the `ajv` library is installed as a project dependency by running `npm install ajv`. This library will be used for JSON Schema validation."
      },
      {
        "step 2": "In `src/validation/contractValidator.ts`, begin by implementing the schema validation logic. Import the `CONTRACT_DOCUMENT_SCHEMA` from `src/schemas/contractSchema.ts` and the `ModuleContract` type. Create an exported function `validateSchema(data: unknown): { valid: boolean; errors: string[] | null }`. This function should use `ajv` to validate the input data against the `CONTRACT_DOCUMENT_SCHEMA` and return a structured result indicating success or failure with detailed error messages."
      },
      {
        "step 3": "Implement the internal consistency validation logic. Create an exported function `validateInternalConsistency(contract: ModuleContract): string[]`. This function will take a single, schema-validated `ModuleContract` object and return an array of error strings. The validation should check for the following: for every parameter listed in `constructorParams`, ensure a corresponding module is listed in the `dependencies` array. If a constructor parameter's type does not match a dependency's `moduleName`, it should be flagged as an error. For example, if a constructor param is `{ name: 'logger', type: 'Logger' }`, there must be a dependency `{ moduleName: 'Logger', ... }`."
      },
      {
        "step 4": "Implement the architectural integrity (graph) validation logic. Create an exported function `validateArchitecturalIntegrity(contracts: ModuleContract[]): string[]`. This function will take the entire array of module contracts and return an array of error strings. First, create a `Set` of all module names defined in the document for efficient lookups. Then, iterate through each contract and its list of `dependencies`. For each dependency, verify that its `moduleName` exists in the set of defined module names. If a dependency points to a module that is not defined in the contract document, add a descriptive error message to the results."
      },
      {
        "step 5": "Create the main orchestrator function for validation. Define an exported function `validateContractDocument(document: unknown): { isValid: boolean; errors: string[] }`. This function will orchestrate the entire validation process by calling the previously created functions in sequence: 1. Call `validateSchema`. If it fails, return immediately with the schema validation errors. 2. If the schema is valid, cast the document to `ModuleContract[]`. 3. Aggregate errors from `validateInternalConsistency` (run for each module) and `validateArchitecturalIntegrity` (run on the whole document). 4. Return a final result object where `isValid` is true only if no errors were found in any stage."
      },
      {
        "step 6": "Now, implement comprehensive unit tests in `src/validation/contractValidator.test.ts`. Use a testing framework like Jest. Create mock contract documents for your tests. Your test suite should cover: \n- **Schema Validation:** Test with valid data, data missing required fields, and data with incorrect types. \n- **Internal Consistency:** Test a valid module, and a module where a `constructorParam` has no matching `dependency`. \n- **Architectural Integrity:** Test a valid document with interconnected modules, and a document where a module has a dependency that does not correspond to any other module in the document. \n- **Orchestrator Function:** Test the main `validateContractDocument` to ensure it correctly aggregates errors from all validation stages and returns the expected `isValid` status."
      },
      {
        "step 7": "Finally, review and refactor the code in `src/validation/contractValidator.ts`. Add TSDoc comments to all exported functions, explaining their purpose, parameters, and return values. Ensure that all error messages generated by the validators are clear, specific, and provide enough context to help a user (or another AI agent) understand what needs to be fixed in the contract document."
      }
    ],
    "Task 1.10: Implement the 'Module Builder' agent (Stage 2) to generate code and tests from a single contract.": [
      {
        "step 1": "Create a new directory `src/agents/module_builder` to house all components related to the Module Builder agent. Also, create an empty `src/agents/module_builder/__init__.py` file to mark it as a Python package."
      },
      {
        "step 2": "In the `src/agents/module_builder` directory, create a new file named `schemas.py`. Define a Pydantic model `ModuleGenerationOutput` in this file. This model will enforce the structure of the LLM's response. It must contain two fields: `implementationCode: str` and `testCode: str`."
      },
      {
        "step 3": "Create the main agent file at `src/agents/module_builder/agent.py`. Define a class named `ModuleBuilderAgent`. This class should be initialized with an LLM client instance (e.g., `def __init__(self, llm_client)`), which will be used to communicate with the language model."
      },
      {
        "step 4": "Inside the `ModuleBuilderAgent` class, define a constant or a private method to generate the system prompt. This prompt is critical. It must instruct the LLM to act as an expert software engineer on a team, specializing in writing production-quality, dependency-injected TypeScript code. Emphasize that it must only use information from the provided contract and that its output must be a JSON object conforming to the `ModuleGenerationOutput` schema. Include the 'CRITICAL RULES' about not making assumptions and adhering strictly to the contract."
      },
      {
        "step 5": "Implement the main public method `generate_module(self, contract: ModuleContract) -> ModuleGenerationOutput` within the `ModuleBuilderAgent` class. This method will orchestrate the code generation process for a single module."
      },
      {
        "step 6": "Inside the `generate_module` method, construct the user prompt. This prompt should clearly state the goal: to generate the implementation and test code for the module defined in the provided contract. It should then include the full JSON representation of the `contract` object. Use f-strings or a template engine for clarity. Hint: `contract.model_dump_json(indent=2)` is a good way to serialize the Pydantic model."
      },
      {
        "step 7": "Still within `generate_module`, make the call to the LLM client. Pass the system prompt, the user prompt, and specify that the response format should be a JSON object conforming to the `ModuleGenerationOutput` Pydantic schema. Assume your LLM client has a feature for enforcing JSON output based on a Pydantic model."
      },
      {
        "step 8": "Add robust error handling to the `generate_module` method. If the LLM response fails to parse as JSON or does not validate against the `ModuleGenerationOutput` schema, raise a custom exception (e.g., `ModuleGenerationError`) that includes the reason for the failure. This is crucial for the orchestrator to handle retries."
      },
      {
        "step 9": "Create a unit test file `tests/agents/test_module_builder_agent.py`. Write a test case to verify the `ModuleBuilderAgent`'s behavior. Use `unittest.mock` to mock the LLM client. Create a sample `ModuleContract` instance to use as input for your tests."
      },
      {
        "step 10": "In your test file, write a specific test to ensure `generate_module` calls the mocked LLM client with the correctly formatted system and user prompts. Assert that the user prompt contains the JSON representation of the sample contract."
      },
      {
        "step 11": "Write another test case where you configure the mocked LLM client to return a valid JSON string that matches the `ModuleGenerationOutput` schema. Call `generate_module` and assert that it returns a correctly parsed `ModuleGenerationOutput` object with the expected code and test strings."
      },
      {
        "step 12": "Finally, write a test case for failure. Configure the mocked LLM client to return malformed JSON or a JSON object with missing fields. Assert that a call to `generate_module` raises the expected custom exception (e.g., `ModuleGenerationError`)."
      }
    ],
    "Task 1.11: Add parallel execution logic to the orchestrator to invoke multiple Module Builder agents concurrently.": [
      {
        "step 1": "First, analyze the existing orchestrator codebase. Locate the specific function or method responsible for executing 'Stage 2: Module Implementation'. This method likely iterates through an array of `ModuleContract` objects and calls a `ModuleBuilder` agent for each one sequentially using `await` inside a loop. Identify the file and the function signature that will be refactored."
      },
      {
        "step 2": "Refactor the identified method to enable parallel execution. Modify the logic from a sequential `for...of` loop to a parallel approach using `Promise.allSettled`. Use the `Array.prototype.map` function on the input `ModuleContract[]` array to create an array of promises, where each promise represents the asynchronous call to the `ModuleBuilder` agent for one contract. Then, use `await Promise.allSettled(...)` to wait for all agent calls to complete."
      },
      {
        "step 3": "After awaiting `Promise.allSettled`, process the results. The result will be an array of settlement objects. Create logic to iterate through this array. Separate the results into two new collections: one for successfully generated modules (where `status` is 'fulfilled') and one for build failures (where `status` is 'rejected'). For failures, make sure to capture the error `reason` and associate it with the contract that failed."
      },
      {
        "step 4": "Update the data structures and logging within the orchestrator. Ensure that the collections of successful and failed modules are stored appropriately in the orchestrator's state to be used by 'Stage 3: Verification, Correction, and Integration'. Add clear logging to indicate the start of the parallel build process, the total number of modules to be built, and a summary of the results (e.g., 'Build complete: 5 modules succeeded, 1 module failed.')."
      },
      {
        "step 5": "Create or update unit tests to validate the new parallel execution logic. Use a mocking framework (like Jest) to mock the `ModuleBuilder` agent. The test should: \n1. Provide a list of mock contracts (e.g., three contracts).\n2. Configure the mock to simulate both successful resolutions and at least one rejection.\n3. Assert that the `ModuleBuilder` agent was called for every contract.\n4. Verify that the orchestrator correctly processes the results from `Promise.allSettled`, correctly populating the 'successful' and 'failed' module collections."
      }
    ],
    "Task 1.12: Implement the 'Integrator' agent (Stage 3) to perform topological sort and generate the main entry point file.": [
      {
        "step 1": "Create a new file at `src/agents/integrator.ts`. In this file, define a class named `IntegratorAgent`. This class will be responsible for assembling the final project. Import the `ModuleContract` and `VerifiedModule` types from `src/types/contract.ts` (you may need to create a `VerifiedModule` type which is essentially a `ModuleContract` plus the `implementationCode` and `testCode` strings). The `IntegratorAgent` should have a constructor that accepts an array of `VerifiedModule` objects."
      },
      {
        "step 2": "Inside the `IntegratorAgent` class, implement a private method `_buildDependencyGraph(modules: VerifiedModule[]): Map<string, string[]>`. This method should create an adjacency list representation of the module dependency graph. The keys of the map should be the module names, and the values should be an array of their dependency names, extracted from the `dependencies` field of each module's contract."
      },
      {
        "step 3": "Implement a private method `_topologicalSort(graph: Map<string, string[]>): string[]`. This method will perform a topological sort on the dependency graph built in the previous step. You can use Kahn's algorithm or a DFS-based approach. The method should return an array of module names in the correct instantiation order. Crucially, it must detect cyclic dependencies and throw an `Error` if a cycle is found, as this indicates an unresolvable architectural flaw."
      },
      {
        "step 4": "Implement a method `generateMainEntryPoint(sortedModules: string[], allModules: VerifiedModule[]): string`. This method will generate the content for the main entry point file (e.g., `src/main.ts`). It should iterate through the `sortedModules` array. For each module name, it must: 1. Generate an import statement like `import { ${moduleName} } from './${moduleName}';`. 2. Generate an instantiation statement like `const ${moduleName.toLowerCase()} = new ${moduleName}(...dependencies);`. The dependencies to be injected into the constructor must be determined from the `constructorParams` field in the module's contract."
      },
      {
        "step 5": "Implement a method `generatePackageJson(projectName: string, modules: VerifiedModule[]): string`. This method will generate the content for the `package.json` file. It should create a basic structure including `name`, `version`, `main`, and `scripts` (e.g., a 'start' script: 'ts-node src/main.ts'). For now, include `typescript`, `ts-node`, and `@types/node` as default devDependencies. In the future, this could be expanded to parse dependencies from module code."
      },
      {
        "step 6": "Implement a method `generateTsConfig(): string`. This method will generate a standard `tsconfig.json` file content as a string. Configure it with reasonable defaults for a modern Node.js project, including `\"target\": \"ES2020\"`, `\"module\": \"commonjs\"`, `\"rootDir\": \"./src\"`, `\"outDir\": \"./dist\"`, and `\"strict\": true`."
      },
      {
        "step 7": "Implement a public method `assembleProject(projectName: string, userGoal: string): Map<string, string>`. This method will orchestrate the entire integration process. It should: 1. Call `_buildDependencyGraph` and `_topologicalSort`. 2. Call the generation methods for `main.ts`, `package.json`, and `tsconfig.json`. 3. Create a map where keys are file paths (e.g., `src/main.ts`, `package.json`, `src/moduleA.ts`, `src/moduleA.test.ts`) and values are the string contents of those files. Include all the verified module implementation and test files in this map. This map represents the complete project file system."
      },
      {
        "step 8": "Create a new test file at `src/agents/integrator.test.ts`. Write comprehensive unit tests for the `IntegratorAgent` class using a testing framework like Jest. Your tests should cover the following scenarios: 1. A valid set of modules is topologically sorted correctly. 2. An error is thrown for a direct cyclic dependency (A -> B, B -> A). 3. An error is thrown for a transitive cyclic dependency (A -> B, B -> C, C -> A). 4. `generateMainEntryPoint` produces a correctly formatted string with proper imports and dependency-injected instantiations. 5. The `assembleProject` method returns a map containing all expected file paths and non-empty content."
      }
    ],
    "Task 1.13: Implement the final file assembler to create the complete project directory structure with all generated files.": [
      {
        "step 1": "Create a new file `src/core/models/file_system_models.py`. In this file, define the Pydantic models that will represent the inputs for the file assembler. Create two models: `VerifiedModule` with fields `module_name: str`, `implementation_code: str`, and `test_code: str`; and `IntegratorOutput` with fields `main_ts: str`, `package_json: str`, `tsconfig_json: str`, and `readme_md: str`. These models will ensure type-safe data handling."
      },
      {
        "step 2": "Create a new file `src/core/file_assembler.py`. Define a class `FileAssembler`. This class will be responsible for creating the final project directory structure. Import the models from `file_system_models.py`. The class should be initialized with a base output directory path (e.g., `output_dir: Path`)."
      },
      {
        "step 3": "In the `FileAssembler` class, create a primary public method: `assemble_project(self, modules: List[VerifiedModule], integrator_output: IntegratorOutput) -> Path`. This method will orchestrate the entire file assembly process. As the first step within this method, implement the logic to create the necessary directory structure: the root project directory, a `src` subdirectory, and a `src/__tests__` subdirectory. Use the `pathlib` library for all path operations and ensure directories are created with `exist_ok=True`."
      },
      {
        "step 4": "Within the `assemble_project` method, implement the logic to write the module files. Iterate through the `modules` list. For each `VerifiedModule`, write the `implementation_code` to `src/{module.module_name}.ts` and the `test_code` to `src/__tests__/{module.module_name}.test.ts`. Ensure you handle file paths correctly using `pathlib`."
      },
      {
        "step 5": "Next, still within the `assemble_project` method, implement the logic to write the files generated by the integrator. Use the `integrator_output` object to write `package.json`, `tsconfig.json`, and `readme.md` to the root of the project directory. Write the `main_ts` content to `src/main.ts`."
      },
      {
        "step 6": "Add a new public method to the `FileAssembler` class: `create_archive(self, source_dir: Path, archive_name: str) -> Path`. This method should take the path to the assembled project directory and create a `.zip` archive of it. Use the `shutil.make_archive` function for this. The `assemble_project` method should call this new method at the end and return the path to the generated zip file."
      },
      {
        "step 7": "Create a new test file `tests/core/test_file_assembler.py`. Write comprehensive unit tests for the `FileAssembler` class using `pytest`. Use the `tmp_path` fixture to create a temporary directory for testing. Your tests should: \n1. Instantiate `FileAssembler`.\n2. Create mock `VerifiedModule` and `IntegratorOutput` objects.\n3. Call `assemble_project`.\n4. Verify that the expected directory structure (`src`, `src/__tests__`) was created.\n5. Verify that all files (`module.ts`, `module.test.ts`, `main.ts`, `package.json`, etc.) exist in the correct locations.\n6. Verify that the content of at least one of each type of file matches the mock input data."
      },
      {
        "step 8": "Refactor the `FileAssembler` and its tests for clarity and robustness. Add comprehensive docstrings to all classes and methods, explaining their purpose, arguments, and return values. Ensure logging is added to the `assemble_project` method to provide visibility into the assembly process (e.g., 'Creating directory structure...', 'Writing module file: logger.ts...', 'Project assembly complete.')."
      }
    ]
  },
  "Phase 2: Implementation of Planning and Contract Generation (Stages 0 & 1)": {
    "1.1: Set up the main project structure and the high-level orchestrator to manage the pipeline stages.": [
      {
        "step 1": "Initialize a new Node.js project using TypeScript. Install necessary dependencies for the project foundation and for schema validation."
      },
      {
        "step 2": "Create the core directory structure to organize the project. This structure will house schemas, core logic like the orchestrator, individual pipeline stages, and generated output artifacts."
      },
      {
        "step 3": "Define the JSON schemas that enforce the structure of data passed between stages. Start by creating the schemas for the `reasoning_tree.json` (output of Stage 0) and the `ModuleContract[]` (output of Stage 1) using the 'zod' library. Place these in the `src/schemas/` directory."
      },
      {
        "step 4": "Define a generic TypeScript interface for a pipeline stage. This will ensure that all stages (Planning, Contract Generation, etc.) have a consistent `execute` method, making them interchangeable and easy to manage within the orchestrator. Create this in a new `src/core/` directory."
      },
      {
        "step 5": "Create the main `Orchestrator` class. This class will be responsible for managing the entire pipeline, executing each stage in the correct order, and passing data between them. For now, create a skeleton class with a `run` method in `src/core/Orchestrator.ts`."
      },
      {
        "step 6": "Create a placeholder implementation for Stage 0, the `PlanningStage`. This class should implement the `IStage` interface. For now, its `execute` method will not call an LLM but will instead return a hardcoded, mock `reasoning_tree` object that conforms to the schema you created. It should also save this mock object to `artifacts/reasoning_tree.json`."
      },
      {
        "step 7": "Create a placeholder implementation for Stage 1, the `ContractStage`. This class will also implement the `IStage` interface. Its `execute` method will take the reasoning tree data as input and return a hardcoded array of mock `ModuleContract` objects conforming to your contract schema. Save the output to `artifacts/contracts.json`."
      },
      {
        "step 8": "Integrate the placeholder stages into the `Orchestrator`. In the `run` method of the orchestrator, instantiate and execute the `PlanningStage` and `ContractStage` sequentially. Ensure the output of the planning stage is passed as the input to the contract stage. Log the final output to the console."
      },
      {
        "step 9": "Create the main application entry point file, `src/index.ts`. This file will instantiate the `Orchestrator` and invoke its `run` method with a sample high-level user goal. This will make the project runnable."
      },
      {
        "step 10": "Finalize the project setup by adding essential configuration files. Create a `.gitignore` file to exclude `node_modules` and `dist`. Also, create a basic `README.md` file explaining what the project is, how to install dependencies (`npm install`), and how to run the orchestrator (`npx ts-node src/index.ts`)."
      }
    ],
    "1.2: Integrate the existing 'Hierarchical Planner' project as the engine for Stage 0.": [
      {
        "step 1": "First, let's establish a clean directory structure for the planning stage within the new project. Create a new top-level directory `src/stage-0-planning`. Inside this new directory, create three subdirectories: `services`, `schemas`, and `utils`."
      },
      {
        "step 2": "Now, copy the core logic files from the existing 'Hierarchical Planner' project. Locate the source files responsible for the planning logic (e.g., `Planner.ts`, `PhasePlanner.ts`, `TaskPlanner.ts`, `StepPlanner.ts`) and copy them into the newly created `src/stage-0-planning/services` directory. Copy any associated helper/utility functions into `src/stage-0-planning/utils`."
      },
      {
        "step 3": "The planner's output is governed by strict schemas. Copy the TypeScript/Zod schema definition files for the `reasoning_tree` and its sub-components from the planner project into the `src/stage-0-planning/schemas` directory. Create an `index.ts` file within `src/stage-0-planning/schemas` to export all the schema types and definitions for easy importing elsewhere in the project."
      },
      {
        "step 4": "To integrate the planner into the main application, we need a clean interface. Create a new file named `planning.facade.ts` inside `src/stage-0-planning`. In this file, define a class named `PlanningFacade`. This class will serve as the single entry point for the orchestrator to interact with the entire planning stage."
      },
      {
        "step 5": "Implement the `PlanningFacade` class. It should have a constructor that accepts any necessary configuration, such as an LLM client instance (e.g., an OpenAI client), to facilitate dependency injection. Create a single public async method: `generatePlan(userGoal: string): Promise<ReasoningTree>`. The type `ReasoningTree` should be imported from your schemas. Inside this method, adapt the logic from the original planner's main execution script. You will need to instantiate the various planner classes you copied in step 2, pass them the LLM client from the constructor, and chain them together to perform the full Goal -> Phases -> Tasks -> Steps decomposition. The method must return the final plan, validated against its Zod schema."
      },
      {
        "step 6": "Merge the dependencies from the 'Hierarchical Planner' project. Open the `package.json` from the original planner project, identify all its `dependencies` and `devDependencies` (e.g., `zod`, LLM SDKs), and add them to the `package.json` of this new project. After updating the file, run `npm install` to fetch and install the new packages."
      },
      {
        "step 7": "Refactor the copied planner services for better modularity. Go through the files in `src/stage-0-planning/services` and remove any direct instantiation of LLM clients or hardcoded access to environment variables for API keys or model names. Modify the constructors of these classes to accept the LLM client as a parameter, which will be passed down from the `PlanningFacade`."
      },
      {
        "step 8": "To verify the integration, create a test script. Create a new file at `src/scripts/run-planning-stage.ts`. In this script, import the `PlanningFacade`. Instantiate your LLM client (you can use a library like `dotenv` to load API keys from a `.env` file for this script). Then, instantiate `PlanningFacade` with the client. Define a sample `userGoal` (e.g., 'Build a real-time chat application with a React frontend'). Call the `generatePlan` method and log the resulting JSON plan to the console. Also, save the output to a file named `reasoning_tree.json` in the project's root directory to inspect the artifact."
      },
      {
        "step 9": "Finalize the integration by performing a cleanup and review. Add comprehensive JSDoc comments to the `PlanningFacade` class and its public `generatePlan` method, clearly documenting its purpose, parameters, and return value. Double-check that all file imports across the new `stage-0-planning` module use correct relative paths. Finally, delete any extraneous files that were copied from the old project but are no longer needed (e.g., its old `README.md`, `package.json`, `.gitignore`, or CLI entry point)."
      }
    ],
    "1.3: Implement the Stage 0 orchestrator logic, including the sequence of prompts for Goal-to-Phases, Phase-to-Tasks, and Task-to-Steps decomposition.": [
      {
        "step 1": "First, set up the directory structure and define the data schemas for Stage 0. Create a new directory `src/stages/stage_0_planning`. Inside this directory, create a file named `schemas.py`. In `schemas.py`, use the Pydantic library to define the data models for the hierarchical plan. You will need four classes: `Step`, `Task`, `Phase`, and `PlanningTree`. The structure should be hierarchical: `PlanningTree` contains a list of `Phase` objects, each `Phase` contains a list of `Task` objects, and each `Task` contains a list of `Step` objects. Each class should have an `id` and a `description` field. For example, `Step(id: str, description: str)`."
      },
      {
        "step 2": "Now, create the prompt templates that will be used to guide the LLM at each decomposition stage. In the `src/stages/stage_0_planning` directory, create a new file named `prompts.py`. Define three string constants in this file: `GOAL_TO_PHASES_PROMPT`, `PHASE_TO_TASKS_PROMPT`, and `TASK_TO_STEPS_PROMPT`. Each prompt must explicitly instruct the LLM to return a JSON object with a specific key (e.g., `phases`, `tasks`, `steps`) containing a list of objects, each with `id` and `description` fields. This is critical for enforcing the schema."
      },
      {
        "step 3": "To ensure modularity, create a simple abstraction for interacting with the LLM. Create a new directory `src/llm`. Inside it, create `llm_adapter.py`. Define an abstract base class `LLMAdapter` with an abstract method `generate_json(prompt: str) -> dict`. Then, create a concrete implementation, for instance, `MockLLMAdapter`, that inherits from `LLMAdapter`. For now, this mock adapter can return pre-defined, structured JSON responses that match the expected output for each prompt type. This will allow you to build and test the orchestrator logic without making real LLM calls."
      },
      {
        "step 4": "Create the main orchestrator for Stage 0. In `src/stages/stage_0_planning`, create a file named `planner.py`. Define a class `Stage0Planner`. The constructor should accept an instance of `LLMAdapter`. The class should have a main public method `execute(user_goal: str) -> PlanningTree` and private helper methods for each decomposition stage: `_generate_phases`, `_generate_tasks_for_phase`, and `_generate_steps_for_task`. Initialize an empty `PlanningTree` object in the `execute` method."
      },
      {
        "step 5": "Implement the `_generate_phases(self, user_goal: str) -> list[Phase]` method in the `Stage0Planner` class. This method should: 1. Format the `GOAL_TO_PHASES_PROMPT` with the `user_goal`. 2. Call the `generate_json` method of the `llm_adapter`. 3. Parse the returned dictionary, access the `phases` key, and use Pydantic's `parse_obj_as` to validate and convert the list of dictionaries into a list of `Phase` model instances. Include robust error handling for JSON parsing and Pydantic validation errors."
      },
      {
        "step 6": "Implement the `_generate_tasks_for_phase(self, phase: Phase) -> list[Task]` method. This method will be called within a loop in the main `execute` method. It should: 1. Format the `PHASE_TO_TASKS_PROMPT` with the `phase.description`. 2. Call the `llm_adapter.generate_json` method. 3. Parse and validate the response to produce a list of `Task` objects, similar to the previous step. Handle potential errors gracefully."
      },
      {
        "step 7": "Implement the `_generate_steps_for_task(self, task: Task) -> list[Step]` method. This will be called in a nested loop for each task. It should: 1. Format the `TASK_TO_STEPS_PROMPT` with the `task.description`. 2. Call the `llm_adapter.generate_json` method. 3. Parse and validate the response to produce a list of `Step` objects. Ensure error handling is in place."
      },
      {
        "step 8": "Complete the implementation of the main `execute(self, user_goal: str) -> PlanningTree` method in `Stage0Planner`. This method orchestrates the entire process. It should: 1. Call `_generate_phases` to get the initial list of phases. 2. Iterate through each generated `Phase`, call `_generate_tasks_for_phase` for it, and assign the resulting list of `Task` objects to the phase. 3. Perform a nested iteration through each `Task` within each `Phase`, call `_generate_steps_for_task`, and assign the resulting `Step` objects to the task. 4. Return the fully populated `PlanningTree` object."
      },
      {
        "step 9": "Create a main entry point to run the Stage 0 planner and generate the `reasoning_tree.json` artifact. Create a `main.py` file in the root directory of the project. This script should: 1. Instantiate the `MockLLMAdapter` and the `Stage0Planner`. 2. Define a sample `user_goal` string (e.g., 'Build a real-time chat application with a React frontend'). 3. Call the `planner.execute()` method with the goal. 4. Serialize the returned `PlanningTree` object to a JSON file named `reasoning_tree.json` in a new `./artifacts` directory, using the `.model_dump_json(indent=2)` method from Pydantic for pretty-printing."
      },
      {
        "step 10": "Write unit tests to ensure the `Stage0Planner` works correctly. Create a `tests/` directory at the project root, and inside it, a `test_stage_0_planning.py` file. Use the `pytest` framework and `unittest.mock`. Your tests should: 1. Mock the `LLMAdapter`. 2. Configure the mock to return specific JSON payloads for each type of prompt. 3. Call the `planner.execute()` method. 4. Assert that the resulting `PlanningTree` object is structured correctly and contains the expected data based on the mock responses. This validates your parsing, validation, and tree-building logic independently of the LLM."
      }
    ],
    "1.4: Define the final JSON schema for the `reasoning_tree.json` artifact and implement the logic to assemble and save it.": [
      {
        "step 1": "Create a new file at `src/schemas/planning_schemas.py`. In this file, define the Pydantic models that will represent the structure of the `reasoning_tree.json` artifact. This ensures all planning output conforms to a strict, verifiable schema. The hierarchy should be `ReasoningTree` -> `Phase` -> `Task` -> `Step`."
      },
      {
        "step 2": "Implement the Pydantic models in `src/schemas/planning_schemas.py`. Use the `pydantic.BaseModel` and `pydantic.Field` for clear definitions. The models should be as follows:\n\n- `Step`: Contains `step_number: int` and `description: str`.\n- `Task`: Contains `task_number: str` (e.g., '1.1', '2.3'), `description: str`, and `steps: list[Step]`.\n- `Phase`: Contains `phase_number: int`, `description: str`, and `tasks: list[Task]`.\n- `ReasoningTree`: This is the root model. It should contain `goal: str` and `phases: list[Phase]`."
      },
      {
        "step 3": "Create a new file at `src/planning/tree_assembler.py`. Define a class named `TreeAssembler` that will be responsible for progressively building the reasoning tree object in memory during the planning stage."
      },
      {
        "step 4": "Implement the `TreeAssembler` class in `src/planning/tree_assembler.py`. It should manage the state of the planning process.\n\n- **Imports:** Import the Pydantic models from `src/schemas/planning_schemas.py`.\n- **`__init__`:** The constructor should accept the high-level `goal: str` and initialize empty lists to hold the plan's components (e.g., `self.phases = []`).\n- **State Management:** Implement methods to add components to the tree. These methods will be called by the planner as it generates each part of the plan:\n  - `add_phase(self, description: str)`: Appends a new `Phase` to `self.phases`. It should automatically assign the correct `phase_number`.\n  - `add_task(self, description: str)`: Appends a new `Task` to the *current* phase. It should automatically assign the correct `task_number` (e.g., '1.1', '1.2').\n  - `add_step(self, description: str)`: Appends a new `Step` to the *current* task. It should automatically assign the correct `step_number`.\n- **Hint:** You will need to maintain internal counters to track the current phase, task, and step numbers to ensure they are assigned sequentially."
      },
      {
        "step 5": "Implement the final assembly and serialization method in the `TreeAssembler` class. \n\n- **Method:** Create a method `build_and_save(self, output_path: str = 'reasoning_tree.json')`.\n- **Logic:** This method should:\n  1. Construct the final `ReasoningTree` Pydantic object using the `goal` and the `self.phases` list that have been built up.\n  2. Serialize the `ReasoningTree` object to a nicely formatted JSON string. Hint: Use the `.model_dump_json(indent=2)` method from Pydantic.\n  3. Write the resulting JSON string to the specified `output_path`."
      },
      {
        "step 6": "Refactor the main planning orchestrator logic (likely in a file such as `src/planning/planner.py` or `src/orchestrator.py`) to use the new `TreeAssembler`.\n\n1. At the start of the planning process, instantiate `TreeAssembler` with the user's goal.\n2. In the loop where you generate phases, replace the existing logic with a call to `assembler.add_phase()`.\n3. Similarly, in the nested loops for tasks and steps, call `assembler.add_task()` and `assembler.add_step()` respectively.\n4. At the very end of the planning stage, after all loops are complete, make a single call to `assembler.build_and_save()` to generate the `reasoning_tree.json` file."
      },
      {
        "step 7": "Create a new test file at `tests/planning/test_tree_assembler.py`. Write comprehensive unit tests for the `TreeAssembler` class to ensure its correctness.\n\n- **Setup:** Use `pytest`.\n- **Test Cases:**\n  1. Test the constructor initializes the `goal` and `phases` list correctly.\n  2. Write a test to add a phase, a task, and a step, then verify the internal state of the assembler object is as expected.\n  3. Write a more complex test that adds multiple phases, tasks, and steps to ensure the numbering logic (`phase_number`, `task_number`, `step_number`) is correct.\n  4. Test the `build_and_save` method. Use `pytest`'s `tmp_path` fixture to create a temporary directory. Call the method, then read the generated file and parse it as JSON. Assert that the resulting data structure matches the expected tree structure and content."
      }
    ],
    "1.5: Define the detailed `ModuleContract` and `CONTRACT_DOCUMENT_SCHEMA` JSON schemas for Stage 1.": [
      {
        "step 1": "Create a new directory `src/schemas` to house all JSON schema definitions for the project. Inside this new directory, create a file named `contract.schema.ts`."
      },
      {
        "step 2": "In `contract.schema.ts`, begin defining the JSON schema for an individual `ModuleContract`. This will be the most detailed part of our schema definitions. Create a constant named `moduleContractSchemaDefinition`. For now, define it as a plain TypeScript object. Use the JSON Schema specification format. Start by defining the basic properties: `name` (a unique non-empty string), `purpose` (a non-empty string describing the module's goal), and `dependencies` (an array of unique strings, where each string is the `name` of another module)."
      },
      {
        "step 3": "Expand the `moduleContractSchemaDefinition` by defining the schemas for the module's interface: `dataStructures`, `functionSignatures`, and `publicAPI`. \n- `dataStructures`: An array of objects, where each object has a required `name` (string) and a required `definition` (string, intended to hold a full TypeScript type or interface definition).\n- `functionSignatures`: An array of objects, where each object must have a required `name` (string), a `params` array (of objects with `name` and `type` string properties), and a `returnType` (string).\n- `publicAPI`: An array of strings, where each string must correspond to a `name` in the `functionSignatures` array. This explicitly defines the module's public surface."
      },
      {
        "step 4": "Complete the `moduleContractSchemaDefinition` by adding the properties that guide the implementation and verification stages: `constructorParams`, `promptInstructions`, and `acceptanceTests`.\n- `constructorParams`: An array of objects, each with a required `name` (string, the parameter name) and `type` (string, the name of the dependency module to be injected).\n- `promptInstructions`: An array of non-empty strings containing detailed, step-by-step instructions for the code generation agent.\n- `acceptanceTests`: An array of non-empty strings, each describing a specific acceptance criterion or a high-level test case that the generated code must pass."
      },
      {
        "step 5": "Now, create the main schema for the entire contract document. In the same file, define a new constant named `contractDocumentSchema`. This schema will define the root object. It should use the `$defs` keyword to contain the `moduleContractSchemaDefinition` you just created under the key `moduleContract`. The main schema object should have a `type` of `object` and define two required properties: `schemaVersion` (a string) and `modules` (an array). The `modules` array's `items` should use a `$ref` to point to `#/$defs/moduleContract`."
      },
      {
        "step 6": "Thoroughly review both schema definitions (`moduleContractSchemaDefinition` and `contractDocumentSchema`). Add a descriptive `description` property to every single field and sub-field. These descriptions are critical context for other developers and for the AI agents that will interact with these schemas. Explain the purpose of each field, its expected format, and its role in the overall system architecture."
      },
      {
        "step 7": "To finalize the file and make it easily usable within the TypeScript project, export both the `contractDocumentSchema` and the `moduleContractSchemaDefinition` constants. Apply a TypeScript `as const` assertion to both schema objects. This will provide strong, static type inference for any part of the orchestrator that imports and uses these schemas, preventing typos and improving overall code quality."
      }
    ],
    "1.6: Implement the 'Interface Architect' agent, including the core prompt that transforms the `reasoning_tree.json` into the contract document.": [
      {
        "step 1": "Create a new file at `src/schemas/contract.schema.ts`. In this file, use the `zod` library to define the schemas for the contract generation stage. Define a `ModuleContractSchema` that includes fields for `name`, `purpose`, `dependencies` (an array of strings), `constructorParams` (an array of objects with `name` and `type`), `publicAPI` (an array of objects with `name` and `signature`), `dataStructures` (an array of objects with `name` and `schema`), `promptInstructions` (a string), and `acceptanceTests` (a string). Then, define a `ContractDocumentSchema` as an array of `ModuleContractSchema`. Export both schemas."
      },
      {
        "step 2": "Create the main agent file at `src/agents/architect.agent.ts`. Define a class named `InterfaceArchitectAgent`. This class will be responsible for orchestrating the transformation of the reasoning tree into a contract document. It should have a constructor and a primary public method `generateContract(reasoningTree: object): Promise<object>`. For now, leave the method implementation empty."
      },
      {
        "step 3": "Inside the `InterfaceArchitectAgent` class, create a private constant named `SYSTEM_PROMPT`. This multi-line string will be the core instruction for the LLM. It must clearly explain the goal: to act as an 'Interface Architect' and convert a detailed procedural plan (`reasoning_tree.json`) into a formal, declarative `ContractDocument`. The prompt should instruct the LLM to: \n1. Analyze the entire plan to identify logical, independent software modules. \n2. For each module, populate all fields of the `ModuleContractSchema`. \n3. Pay close attention to inferring dependencies between modules. \n4. Translate the low-level steps from the reasoning tree into high-level `promptInstructions` and `acceptanceTests` for each module. \n5. Emphasize that the final output MUST be a JSON object that strictly conforms to the `ContractDocumentSchema`."
      },
      {
        "step 4": "Implement a private validation method `_validateContractDocument(document: any): object` within the `InterfaceArchitectAgent` class. This method will perform the post-generation checks. First, implement the 'Schema Adherence' check by using `ContractDocumentSchema.parse(document)`. If parsing fails, throw a detailed error. If it succeeds, return the parsed document."
      },
      {
        "step 5": "Enhance the `_validateContractDocument` method to perform 'Architectural Integrity' validation. After the initial schema check passes, add logic to verify the dependency graph. Create a `Set` of all module names defined in the document. Then, iterate through each module and its `dependencies` array. For each dependency listed, ensure it exists in the module name `Set`. If a dependency is not found, throw an error indicating an invalid architectural reference."
      },
      {
        "step 6": "Now, implement the main `generateContract` method. This method should: \n1. Take the `reasoningTree` object as input. \n2. Create the user prompt by stringifying the `reasoningTree` object. \n3. Make a call to a hypothetical LLM service (e.g., `llm.generateJson(this.SYSTEM_PROMPT, userPrompt)`), which you can mock for now. \n4. Pass the JSON response from the LLM to your `_validateContractDocument` method. \n5. If validation is successful, return the validated contract document. If any part fails, catch the error and log it appropriately before re-throwing or handling it."
      },
      {
        "step 7": "Create a new test file at `src/agents/architect.agent.test.ts`. In this file, create a mock `reasoning_tree.json` object that describes a simple two-module project (e.g., a 'Database' module and a 'UserService' module that depends on the 'Database'). This mock data will serve as the input for your tests."
      },
      {
        "step 8": "In `src/agents/architect.agent.test.ts`, write unit tests for the `InterfaceArchitectAgent`. Your tests should mock the LLM call. \n1. Test the successful path: provide the mock reasoning tree, have the mocked LLM return a valid contract document, and assert that the `generateContract` method returns it without errors. \n2. Test schema validation failure: have the mocked LLM return a malformed JSON object (e.g., with a missing `name` field in a module) and assert that `generateContract` throws a `ZodError`. \n3. Test architectural validation failure: have the mocked LLM return a contract document where one module depends on a module name that is not defined. Assert that `generateContract` throws your custom architectural error."
      }
    ],
    "1.7: Implement the Stage 1 post-generation validation module, including checks for schema adherence, internal consistency, and architectural integrity (dependency graph validation).": [
      {
        "step 1": "Create the necessary files and define the main validation function structure. Create a new file at `src/stages/1_contract_generation/validation/contractValidator.ts`. Inside this file, import the necessary types, specifically `ModuleContract` and the Zod schema `CONTRACT_DOCUMENT_SCHEMA` from `src/llm/schemas.ts`. Then, define an exported function `validateContractDocument(document: unknown): ModuleContract[]`. This function will serve as the entry point for all validation logic. For now, leave the implementation empty but add a JSDoc comment outlining its purpose: to perform full validation of a module contract document, including schema adherence, architectural integrity, and internal consistency."
      },
      {
        "step 2": "Implement the complete validation logic within the `validateContractDocument` function. The implementation should follow these sequential checks: 1. **Schema Validation**: Use the `CONTRACT_DOCUMENT_SCHEMA.safeParse(document)` method from Zod to validate the input against the schema. If parsing fails, throw a detailed error including the Zod error issues. If it succeeds, use the parsed data for the subsequent checks. 2. **Architectural Integrity (Dependency Graph)**: Create a `Set<string>` of all module names from the parsed document for efficient lookups. Then, iterate through each module in the document. For each module, iterate through its `dependencies` array. For each dependency name, check if it exists in the module name `Set`. If a dependency is not found, throw a specific `Error` stating which module has an undefined dependency. 3. **Internal Consistency (Constructor vs. Dependencies)**: Inside the same loop, for each module, compare its `dependencies` array with its `constructorParams` array. The set of names in `dependencies` must be identical to the set of names in `constructorParams`. If they do not match (either a missing param for a dependency or an extra param), throw a specific `Error` indicating the mismatch for that module."
      },
      {
        "step 3": "Create a corresponding test file `src/stages/1_contract_generation/validation/contractValidator.test.ts`. Write a comprehensive suite of unit tests for the `validateContractDocument` function using your preferred testing framework (e.g., Jest). Your tests must cover the following scenarios: 1. **Valid Contract**: A test case with a valid, multi-module contract document that should pass validation successfully. 2. **Invalid Schema**: A test case with a document that does not conform to the `CONTRACT_DOCUMENT_SCHEMA` (e.g., missing a required field), and assert that it throws an error. 3. **Missing Dependency**: A test case where a module lists a dependency that is not defined as another module in the document, and assert that it throws the correct architectural integrity error. 4. **Inconsistent Constructor Params**: A test case where a module's `constructorParams` do not perfectly match its `dependencies` list, and assert that it throws the correct internal consistency error."
      },
      {
        "step 4": "Review and refine the implementation in `contractValidator.ts`. Ensure all functions and type definitions have clear JSDoc comments explaining their purpose, parameters, and return values or thrown errors. Focus on the clarity and specificity of the error messages. For example, instead of 'Invalid dependency', use 'Validation Error in module \"MyApi\": Dependency \"MyDatabase\" is not defined in the contract document.' Refactor the code for readability and maintainability, ensuring the validation logic is easy to follow."
      }
    ],
    "1.8: Implement the end-to-end flow connecting Stage 0 and Stage 1, where the `reasoning_tree.json` is passed to the Interface Architect and the validated contract document is saved as the final output of the phase.": [
      {
        "step 1": "Create a new file `src/main_orchestrator.py` which will serve as the primary entry point for running the end-to-end flow from user goal to a validated contract document."
      },
      {
        "step 2": "In `src/main_orchestrator.py`, import the necessary components: `HierarchicalPlanner` from `src.planning.planner`, `InterfaceArchitect` from `src.architect.architect`, and any required schema definitions. Sketch out a main function that defines the high-level sequence: 1. Get user goal, 2. Run Planner (Stage 0), 3. Run Architect (Stage 1 Generation), 4. Run Validator (Stage 1 Validation), 5. Save output. Use placeholder function calls for now."
      },
      {
        "step 3": "Implement the Stage 0 integration. Create a function `run_planning_stage(goal: str) -> str` in `main_orchestrator.py`. This function should instantiate the `HierarchicalPlanner`, execute its planning process with the provided goal, and return the file path to the generated `reasoning_tree.json`. Ensure the planner's execution is encapsulated and doesn't terminate the script."
      },
      {
        "step 4": "Implement the Stage 1 generation integration. Create a function `run_architect_stage(reasoning_tree_path: str) -> list` in `main_orchestrator.py`. This function should read the content of `reasoning_tree.json`, instantiate the `InterfaceArchitect`, and call it to generate the contract document. It should parse the LLM's JSON output and return the raw, unvalidated contract document as a Python list of dictionaries."
      },
      {
        "step 5": "Create a dedicated module for validation. Create a new file `src/architect/validator.py`. Inside this file, define a class named `ContractValidator`. The constructor `__init__` should accept the contract document (the list of module contracts) as an argument and store it."
      },
      {
        "step 6": "Implement schema validation within the `ContractValidator` class. Add a method `validate_schema()`. This method should import the `CONTRACT_DOCUMENT_SCHEMA` from `src.schemas.contract` and use the `jsonschema` library to validate the contract document stored in the instance. If validation fails, it should raise a `jsonschema.ValidationError`. Hint: You may need to install `jsonschema` (`pip install jsonschema`)."
      },
      {
        "step 7": "Implement architectural integrity validation in `ContractValidator`. Add a method `validate_dependencies()`. This method must perform a graph validation check: 1. Create a set of all `moduleName` values from the contract document. 2. Iterate through each module and its `dependencies` list. 3. For each dependency, verify that its `moduleName` exists in the set of defined module names. 4. If a dependency points to a non-existent module, raise a custom `ValueError` with a clear error message (e.g., \"Validation Error: Module 'X' has a dependency on 'Y', but module 'Y' is not defined in the contract.\")."
      },
      {
        "step 8": "Integrate the validation logic into the main orchestrator flow. In `main_orchestrator.py`, after generating the contract document, instantiate `ContractValidator` with it. Call the `validate_schema()` and `validate_dependencies()` methods. Use a try-except block to catch potential validation errors and print a user-friendly failure message before exiting."
      },
      {
        "step 9": "Finalize the orchestrator script by adding a command-line interface and file output. Use the `typer` library to make the main function runnable from the command line, accepting the user goal as an argument. Add `logging` to provide status updates for each stage (e.g., 'Running planner...', 'Generating contract...', 'Validating contract...'). If all steps succeed, save the validated contract document to `build/contract_document.json`. Ensure the `build` directory is created if it doesn't exist."
      },
      {
        "step 10": "Update the project's `README.md` file. Add a new section titled 'Running the Full Pipeline (Stages 0 & 1)' that explains how to use the new `main_orchestrator.py` script. Include a clear example command, like `python -m src.main_orchestrator \"Build a simple real-time chat API\"`."
      }
    ]
  },
  "Phase 3: Parallel Module Code Generation (Stage 2)": {
    "Develop the Stage 2 orchestrator logic to load the validated `ModuleContract[]` document.": [
      {
        "step 1": "First, set up the necessary file structure for Stage 2. Create a new directory `src/stages/stage2_module_implementation`. Inside this directory, create two new files: `orchestrator.ts` and `orchestrator.test.ts`. Concurrently, create a directory `src/schemas` if it doesn't exist, and add a new file inside it named `contract.schema.ts`."
      },
      {
        "step 2": "In `src/schemas/contract.schema.ts`, define the core data structure for our contracts. Create and export a TypeScript interface named `ModuleContract`. Based on the system architecture document, this interface must include the following properties: `name` (string), `purpose` (string), `functionSignatures` (string[]), `dataStructures` (string[]), `publicAPI` (string[]), `dependencies` (string[]), `constructorParams` (string[]), `promptInstructions` (string), and `acceptanceTests` (string). Add clear JSDoc comments to describe the purpose of the interface and each of its properties."
      },
      {
        "step 3": "Now, let's add robust, runtime schema validation using the `zod` library. If you haven't already, add `zod` as a project dependency. In `src/schemas/contract.schema.ts`, create and export a Zod schema constant named `ModuleContractSchema` that mirrors the structure of the `ModuleContract` interface. Then, create and export a `ContractDocumentSchema` constant defined as `z.array(ModuleContractSchema)` to validate the entire contract document."
      },
      {
        "step 4": "In `src/stages/stage2_module_implementation/orchestrator.ts`, create the function for loading the contracts. Import the `ModuleContract` interface and the `ContractDocumentSchema` from `../schemas/contract.schema.ts`. Define and export an `async` function named `loadAndValidateContracts` that accepts a `filePath: string` and is typed to return a `Promise<ModuleContract[]>`. This function will be the entry point for Stage 2 processing."
      },
      {
        "step 5": "Implement the core logic for the `loadAndValidateContracts` function. Use the `fs/promises` module from Node.js to read the file content at the given `filePath`. Parse the resulting string as JSON. Then, use the `ContractDocumentSchema.parse()` method to validate the parsed data. If validation succeeds, return the data. This ensures that any data returned from this function is guaranteed to match the contract schema."
      },
      {
        "step 6": "Implement comprehensive error handling within `loadAndValidateContracts`. Wrap the entire file reading, parsing, and validation logic in a `try...catch` block. The `catch` block should handle different types of errors: file system errors (e.g., file not found), JSON parsing errors, and Zod validation errors. For each error type, log a specific, informative message and re-throw a new, user-friendly `Error` that clearly explains the failure (e.g., 'Failed to load contracts: File not found at path...' or 'Failed to validate contracts: Data does not conform to schema.')."
      },
      {
        "step 7": "Now, write unit tests to verify the 'happy path' for your loader. In `src/stages/stage2_module_implementation/orchestrator.test.ts`, use a testing framework like Jest. Create a mock `contracts.json` file in a temporary test directory. This file should contain a valid array of two or more module contract objects. Write a test case that calls `loadAndValidateContracts` with the path to this mock file and asserts that the returned value is an array that deeply equals the mock data."
      },
      {
        "step 8": "Finally, write unit tests to ensure your error handling is robust. In `orchestrator.test.ts`, add three more test cases: 1. Test that `loadAndValidateContracts` throws the expected error when given a path to a non-existent file. 2. Test that it throws the expected error when the file contains malformed JSON. 3. Test that it throws the expected Zod-related error when the file contains valid JSON that fails schema validation (e.g., a contract object is missing the `name` property). Use `expect(...).rejects.toThrow()` to assert that the function correctly rejects the promise with an appropriate error for each case."
      }
    ],
    "Implement the parallel execution mechanism to spawn a 'Module Builder' agent for each contract concurrently.": [
      {
        "step 1": "Create a new file named `stage_2_module_implementation.py`. This file will contain all the logic for Stage 2: Parallel Module Implementation. Also, create a corresponding test file `test_stage_2_module_implementation.py`."
      },
      {
        "step 2": "In `stage_2_module_implementation.py`, define the necessary Pydantic models to represent the data structures for this stage. You will need: \n1. A `GeneratedModule` model with fields: `module_name: str`, `implementation_code: str`, and `test_code: str`. \n2. A `Stage2Output` model that holds a list of `GeneratedModule` objects: `modules: List[GeneratedModule]`. \nHint: You will be reading `ModuleContract` objects. Assume the Pydantic models for the contract (e.g., `ModuleContract`, `FunctionSignature`, etc.) are available to be imported from a `shared_types.py` file. If they are not, create them based on the Stage 1 description."
      },
      {
        "step 3": "Create a helper function `_build_module_builder_prompt(contract: ModuleContract) -> str`. This function will take a single module contract and construct the detailed, multi-part prompt for the LLM. The prompt should include:\n- A clear role for the AI (e.g., 'You are an expert software developer specializing in creating modular, production-ready TypeScript code.').\n- The full JSON representation of the `ModuleContract`.\n- The 'System Context' and 'CRITICAL RULES' mentioned in the system architecture, such as instructions to write dependency-injected code, not to call other modules directly, and to return only a specific JSON object.\n- A final instruction specifying the exact output format: a JSON object with two keys, `implementationCode` and `testCode`."
      },
      {
        "step 4": "Implement an asynchronous function `generate_module_code_async(contract: ModuleContract, llm_client: Any) -> GeneratedModule`. This function will be responsible for generating the code for a *single* module. Its logic should be:\n1. Call `_build_module_builder_prompt` to get the prompt for the given contract.\n2. Use the provided `llm_client` to make an asynchronous API call to the LLM with the generated prompt.\n3. Receive the response and parse it as JSON into a dictionary.\n4. Validate the dictionary contains the keys `implementationCode` and `testCode`.\n5. Instantiate and return a `GeneratedModule` object containing the `module_name` from the contract and the code from the response.\n6. Implement robust error handling for API failures and JSON parsing errors, raising a custom exception if a module cannot be generated."
      },
      {
        "step 5": "Implement the main orchestration function `run_stage_2_parallel(contracts: List[ModuleContract]) -> Stage2Output`. This function will manage the concurrent execution. \n1. It should be an `async` function.\n2. Initialize an LLM client (assume a client class like `LLMClient` is available to be instantiated).\n3. Create a list of awaitable tasks by calling `asyncio.create_task(generate_module_code_async(contract, llm_client))` for each contract in the input list.\n4. Use `asyncio.gather(*tasks, return_exceptions=True)` to execute all tasks concurrently and collect the results. Using `return_exceptions=True` is crucial for handling failures in individual generation tasks without stopping the entire process.\n5. Process the results from `gather`. Separate the successful `GeneratedModule` objects from any exceptions that were returned.\n6. Log any errors for failed modules.\n7. Return a `Stage2Output` object containing only the successfully generated modules."
      },
      {
        "step 6": "In `test_stage_2_module_implementation.py`, write unit tests for the `run_stage_2_parallel` function. Use `unittest.mock.AsyncMock` to mock the `llm_client` and the `generate_module_code_async` function. Your tests should cover:\n1. The happy path where all modules are generated successfully.\n2. A scenario where one of the module generation tasks fails (returns an exception). Verify that the orchestrator correctly handles the exception and returns the successfully generated modules.\n3. An edge case with an empty list of contracts as input."
      },
      {
        "step 7": "Finally, create a main execution block (`if __name__ == '__main__':`) in `stage_2_module_implementation.py`. This block should:\n1. Load the `ModuleContract[]` document from a file (e.g., `module_contracts.json`).\n2. Use `asyncio.run()` to execute the `run_stage_2_parallel` function with the loaded contracts.\n3. Serialize the resulting `Stage2Output` object to a new JSON file named `generated_modules.json`.\n4. Print a summary to the console, indicating how many modules were generated successfully and how many failed."
      }
    ],
    "Design and finalize the master prompt template for the 'Module Builder' agent, which will combine the system context, critical rules, and a specific module contract.": [
      {
        "step 1": "Create a new file named `prompt_components.ts`. In this file, define a TypeScript constant named `SYSTEM_CONTEXT` as a multi-line string. This string will serve as the introductory part of the prompt, establishing the agent's persona. It should clearly state that the agent is an expert TypeScript developer, part of a larger autonomous system, and its role is to implement a single, specific module based on a provided contract. Emphasize that it is a specialist on a team and should not act like a general-purpose assistant."
      },
      {
        "step 2": "In the same `prompt_components.ts` file, define another TypeScript constant named `CRITICAL_RULES` as a multi-line string. This section will list the non-negotiable rules for code generation. Include the following rules, formatted clearly (e.g., using markdown list syntax): 1. Strict Contract Adherence (implement only what's in the contract), 2. Dependency Injection (all external dependencies must be injected via the constructor), 3. Production-Ready Code (clean, typed, commented, and robust), 4. Comprehensive Test Generation (write Jest tests for all acceptance criteria in the contract), 5. No Hardcoded Values (use constructor parameters or constants), and 6. Isolated Implementation (write code as if it's the only module you know about, besides its direct dependencies)."
      },
      {
        "step 3": "In `prompt_components.ts`, define a third TypeScript constant named `IO_FORMAT_INSTRUCTIONS` as a multi-line string. This section will detail the input and output format. For the input, explain that the agent will receive a `ModuleContract` JSON object and briefly describe its key fields (`moduleName`, `purpose`, `functionSignatures`, `dependencies`, `promptInstructions`, `acceptanceTests`). For the output, specify that the response MUST be a single, raw JSON object conforming to the schema `{ \"implementationCode\": \"...\", \"testCode\": \"...\" }`. Explicitly forbid any explanatory text, greetings, or markdown formatting outside of the JSON object itself."
      },
      {
        "step 4": "Create a new file named `prompt_template_builder.ts`. In this file, import the constants from `prompt_components.ts`. Create a function `buildModuleBuilderPrompt(moduleContract: object): string`. This function should take a JavaScript object representing a single `ModuleContract` as input. It will construct the final, complete prompt string by combining `SYSTEM_CONTEXT`, `CRITICAL_RULES`, the stringified `moduleContract` (nicely formatted with `JSON.stringify(moduleContract, null, 2)`), and `IO_FORMAT_INSTRUCTIONS`. Use a clear structure with markdown headings like '## Your Role', '## Critical Rules', '## Your Task: Module Contract', and '## Required Output Format'."
      },
      {
        "step 5": "To test the template builder, create a mock `ModuleContract` object. Create a new file named `mock_contracts.ts`. In this file, define and export a constant named `mockLoggerContract`. This object should represent a contract for a simple `Logger` module. It should have no dependencies, a constructor that accepts a `logLevel` string, a single public method `log(level: string, message: string): void`, and at least two `acceptanceTests` describing scenarios like 'should log messages that meet the configured log level' and 'should not log messages below the configured log level'."
      },
      {
        "step 6": "Create a new file named `main.ts` to demonstrate the complete process. In this file, import `buildModuleBuilderPrompt` from `prompt_template_builder.ts` and `mockLoggerContract` from `mock_contracts.ts`. Call the `buildModuleBuilderPrompt` function with the `mockLoggerContract` as input. Finally, print the resulting complete prompt string to the console. This will serve as a final validation that the template is assembled correctly and is ready to be sent to an LLM."
      }
    ],
    "Implement the 'Module Builder' agent, responsible for taking a single contract, constructing the full prompt, and calling the LLM API.": [
      {
        "step 1": "Create a new file at `src/agents/module_builder_agent.ts`. In this file, define the necessary data structures for the agent's operation. First, define and export a Zod schema named `GeneratedModuleSchema` which validates an object with two string properties: `implementationCode` and `testCode`. Then, derive a TypeScript type `GeneratedModule` from this schema. You will also need the `ModuleContract` type, which you can import from `src/schemas/contract.schema.ts`."
      },
      {
        "step 2": "In `src/agents/module_builder_agent.ts`, define two string constants, `SYSTEM_CONTEXT` and `CRITICAL_RULES`. Populate these constants with the core instructions for the code generation LLM, based on the system's philosophy. The `SYSTEM_CONTEXT` should explain that the AI is a specialist developer on a team, receiving a formal contract to implement a single module. The `CRITICAL_RULES` should enforce key principles like writing production-ready, dependency-injected code, not making assumptions beyond the contract, including comprehensive comments, and strictly adhering to the JSON output format."
      },
      {
        "step 3": "Define and export the `ModuleBuilderAgent` class in `src/agents/module_builder_agent.ts`. It should have a constructor that accepts an instance of the OpenAI client as a dependency. Create a public async method `generateModule(contract: ModuleContract): Promise<GeneratedModule>`. Also, add a private method signature for `_constructPrompt(contract: ModuleContract): string` which will be implemented next."
      },
      {
        "step 4": "Implement the private `_constructPrompt` method within the `ModuleBuilderAgent` class. This method should take a `ModuleContract` object and return a single, formatted string. It must assemble the final prompt by concatenating the `SYSTEM_CONTEXT`, `CRITICAL_RULES`, a clear instruction to implement the provided contract, the `ModuleContract` object itself (stringified as a JSON block), and a final instruction specifying the required JSON output format (`{ \"implementationCode\": \"...\", \"testCode\": \"...\" }`)."
      },
      {
        "step 5": "Implement the core logic of the `generateModule` method. This method should: 1. Call `_constructPrompt` to get the full prompt string. 2. Use the injected OpenAI client to make a chat completion request. Ensure you enable the client's JSON mode to guarantee a JSON response. 3. Retrieve the JSON string content from the LLM's response. 4. Use a try-catch block to parse the JSON string. If parsing fails, throw a custom error. 5. Use the `GeneratedModuleSchema.parse()` method to validate the parsed object. This will throw an error if the object doesn't match the schema. 6. If validation is successful, return the parsed and validated object."
      },
      {
        "step 6": "Create a new test file `src/agents/module_builder_agent.test.ts`. Set up the necessary imports, including the `ModuleBuilderAgent`, a mock `ModuleContract` object for testing, and your testing framework (e.g., Jest/Vitest). Mock the OpenAI client using `jest.mock('openai')` or an equivalent for your test runner. This will allow you to control the LLM's responses in your tests."
      },
      {
        "step 7": "In `module_builder_agent.test.ts`, write a unit test specifically for the prompt construction logic. Instantiate the `ModuleBuilderAgent` (you can pass a `null` or mock client for this test). Call the `_constructPrompt` method (you may need to temporarily make it public or use `// @ts-ignore` for testing). Assert that the returned prompt string contains the system context, critical rules, and the stringified JSON of the mock contract you provided."
      },
      {
        "step 8": "Write a comprehensive suite of tests for the `generateModule` method. Create at least three test cases: 1. **Success Case:** Mock the OpenAI client to return a valid JSON string that conforms to the `GeneratedModuleSchema`. Assert that the method returns the correctly parsed object. 2. **Invalid JSON Case:** Mock the client to return a malformed JSON string (e.g., `{\"code\": ...`). Assert that the method throws a specific `JSON.parse` error. 3. **Schema Mismatch Case:** Mock the client to return a valid JSON string that does *not* conform to the schema (e.g., `{\"implementationCode\": \"...\"}` but missing `testCode`). Assert that the method throws a Zod validation error."
      },
      {
        "step 9": "Perform a final review of the `ModuleBuilderAgent` and its tests. Ensure all type imports are correct, add JSDoc comments to the class and its public methods explaining their purpose, parameters, and return values. Refactor any complex logic for clarity and ensure consistent error handling. Make the `_constructPrompt` method private again if you made it public for testing."
      }
    ],
    "Define the JSON schema for the 'Module Builder' agent's output (`{implementationCode, testCode}`) and implement validation for the LLM response.": [
      {
        "step 1": "Create a new file named `module_code_schema.py` inside a new directory `src/schemas`. This file will store the JSON schema for the output of the 'Module Builder' agent."
      },
      {
        "step 2": "In `src/schemas/module_code_schema.py`, define the JSON schema as a Python dictionary named `MODULE_CODE_SCHEMA`. This schema must enforce that the root is an object with two required string properties: `implementationCode` and `testCode`. Ensure no additional properties are allowed."
      },
      {
        "step 3": "Create a new file named `llm_response_validator.py` inside a new directory `src/validators`. This module will contain the logic to validate LLM responses against our defined schemas."
      },
      {
        "step 4": "In `src/validators/llm_response_validator.py`, implement a function `validate_llm_response(json_string: str, schema: dict) -> tuple[bool, dict | None, str | None]`. This function should parse the input string, validate it against the provided schema, and return a tuple containing: a boolean indicating success, the parsed data if successful (or None), and an error message if it fails (or None). Handle both `json.JSONDecodeError` and `jsonschema.ValidationError`."
      },
      {
        "step 5": "Ensure you have the `jsonschema` library installed. If not, add it to your `requirements.txt` file and install it. The `validate_llm_response` function should use this library for the validation logic."
      },
      {
        "step 6": "Create a new test file `tests/validators/test_llm_response_validator.py` to write unit tests for the validation logic."
      },
      {
        "step 7": "In `tests/validators/test_llm_response_validator.py`, write a comprehensive suite of unit tests for the `validate_llm_response` function using `pytest`. Import the `MODULE_CODE_SCHEMA` for testing. Your tests should cover the following cases: 1. A perfectly valid response. 2. A response that is not valid JSON. 3. A valid JSON response that is missing the required `implementationCode` key. 4. A valid JSON response where `testCode` has the wrong data type (e.g., a boolean instead of a string). 5. A valid JSON response that includes an extraneous, unallowed property."
      }
    ],
    "Implement the logic to collect the generated code and test pairs from all parallel agent runs.": [
      {
        "step 1": "First, let's define the data structures for handling the output from the `Module Builder` agents. In the `src/schemas.py` file, create two new Pydantic models: `ModuleGenerationOutput` which will represent the successful output from a single agent (containing `module_name`, `implementation_code`, and `test_code`), and `Stage2Output` which will be a dictionary mapping module names to either a `ModuleGenerationOutput` object or an `Exception` object for failed generations."
      },
      {
        "step 2": "Navigate to the file responsible for Stage 2 logic, which should be `src/stages/stage_2_code_generation.py`. Create a new asynchronous helper function named `_generate_module_code_async`. This function will take a single `ModuleContract` pydantic object and an `aiohttp.ClientSession` as input. Its purpose is to prepare the prompt for the `Module Builder` agent, make the API call, and parse the response. For now, you can mock the API call to return a sample JSON object like `{\"implementationCode\": \"...\", \"testCode\": \"...\"}`. The function should return an instance of the `ModuleGenerationOutput` model you created in the previous step."
      },
      {
        "step 3": "Now, create the main function for this stage, `run_stage_2_code_generation(contracts: list[ModuleContract]) -> Stage2Output`. This function will be the entry point for Stage 2. Inside this function, initialize an `aiohttp.ClientSession`."
      },
      {
        "step 4": "Inside `run_stage_2_code_generation`, create a list of awaitable tasks. Iterate through the input `contracts` list and for each `contract`, create a task by calling the `_generate_module_code_async` helper function. Store these tasks in a list."
      },
      {
        "step 5": "Use `asyncio.gather(*tasks, return_exceptions=True)` to execute all the module generation tasks concurrently. The `return_exceptions=True` flag is critical as it prevents the entire process from halting on a single agent failure and allows you to collect exceptions along with successful results."
      },
      {
        "step 6": "After `asyncio.gather` completes, process the list of results. Create an empty dictionary that will conform to the `Stage2Output` type. Iterate over the results list and the original `contracts` list simultaneously (e.g., using `zip`). For each result, check if it is an instance of `Exception`. If it is, log the failure and store the exception object in your output dictionary with the corresponding module name as the key. If it's a successful `ModuleGenerationOutput` object, store it in the dictionary."
      },
      {
        "step 7": "Return the final populated dictionary from the `run_stage_2_code_generation` function. Ensure the entire file has proper type hints, logging (e.g., 'Starting code generation for X modules...', 'Successfully generated module Y', 'Failed to generate module Z'), and follows asynchronous programming best practices."
      },
      {
        "step 8": "Finally, create a new test file `tests/stages/test_stage_2_code_generation.py`. Write unit tests for the `run_stage_2_code_generation` function using `pytest` and `pytest-asyncio`. You will need to use `unittest.mock.AsyncMock` to patch `_generate_module_code_async`. Create at least two test cases: one where all modules generate successfully, and another where one or more modules fail, ensuring the output dictionary correctly captures both the successful results and the exceptions."
      }
    ],
    "Structure the collected output into a data format suitable for input into Stage 3 (Verification & Correction).": [
      {
        "step 1": "Define the data structure that will encapsulate all information needed for the verification and correction stage. Create a new file `src/schemas/verification_schemas.py`. In this file, define a Pydantic model named `ModuleVerificationUnit`. This model will represent a single, self-contained package of work for Stage 3."
      },
      {
        "step 2": "Implement the `ModuleVerificationUnit` Pydantic model in `src/schemas/verification_schemas.py`. It must include the following fields: `module_name: str`, `contract: dict` (to hold the full JSON object for the module's contract), `implementation_code: str`, `test_code: str`, `status: str` (default to 'PENDING_VERIFICATION'), `verification_attempts: int` (default to 0), and `test_results: Optional[dict]` (default to None). Also, define a `VerificationStatus` Enum with values like `PENDING_VERIFICATION`, `VERIFICATION_SUCCESS`, `VERIFICATION_FAILURE`, `CORRECTION_ATTEMPT`, `CORRECTION_SUCCESS`, `CORRECTION_FAILURE`, `DEPENDENCY_FAILURE` and use it for the `status` field."
      },
      {
        "step 3": "Create a new script file `src/scripts/prepare_for_verification.py`. This script will be responsible for orchestrating the data transformation. Set up the basic file structure with necessary imports, including the `ModuleVerificationUnit` schema you just created, and a main function placeholder."
      },
      {
        "step 4": "In `src/scripts/prepare_for_verification.py`, implement the logic to load the necessary input data. This involves reading the main contract document from `output/1_contract_document.json` which contains the array of `ModuleContract` objects. Your function should parse this JSON file into a Python list of dictionaries."
      },
      {
        "step 5": "Implement the core transformation logic within the main function of `prepare_for_verification.py`. Iterate through each module contract loaded in the previous step. For each contract, extract the `moduleName` and dynamically construct the file path to its corresponding generated code, which is located at `temp/generated_code/{moduleName}.json`. Load the content of this JSON file, which contains the `implementationCode` and `testCode`."
      },
      {
        "step 6": "Inside the loop in `prepare_for_verification.py`, for each module, instantiate the `ModuleVerificationUnit` Pydantic model. Populate it with the `module_name` from the contract, the full `contract` object itself, the `implementation_code` and `test_code` loaded from the temporary file, and the default status and attempt count. Append each fully populated `ModuleVerificationUnit` object to a list."
      },
      {
        "step 7": "After the loop completes, implement the final step in `prepare_for_verification.py`: serialization. Convert the list of `ModuleVerificationUnit` Pydantic objects into a single JSON string. Ensure the output is pretty-printed for human readability. Write this final JSON string to a new file at `output/2_verification_units.json`. This file will serve as the definitive input for Stage 3."
      },
      {
        "step 8": "Refactor `src/scripts/prepare_for_verification.py` to be a robust, command-line executable script. Use the `typer` library to add command-line arguments for the input contract document path, the generated code directory path, and the output file path. This will make the script more flexible and easier to integrate into the main orchestrator pipeline."
      }
    ],
    "Add error handling and retry logic for individual module generation failures (e.g., LLM API errors, malformed JSON response).": [
      {
        "step 1": "First, locate the function responsible for generating the code for a single module. This function likely takes a `ModuleContract` as input and calls the LLM API. Let's assume this function is named `generateModuleCode` within a file like `src/stages/stage_2_generation.ts`. Isolate this function for modification."
      },
      {
        "step 2": "To handle failures more gracefully, create a new utility function that encapsulates retry logic with exponential backoff. Create a new file `src/utils/retry.ts` and define an async function `withRetry(fn, retries, initialDelay)`. This function should: accept an async function `fn` to execute, a `retries` count, and an `initialDelay` in ms. It should call `fn` and if it fails, wait for `initialDelay`, double the delay, and retry up to `retries` times. If all retries fail, it should re-throw the last error."
      },
      {
        "step 3": "Write comprehensive unit tests for the `withRetry` utility in `src/utils/retry.test.ts`. Use a mocking library like Jest (`jest.fn()`) to create mock async functions. Test the following scenarios: 1. The function succeeds on the first try. 2. The function fails once, then succeeds. 3. The function fails more times than the allowed retries and correctly throws an error. 4. Verify that the delay between retries is being applied correctly."
      },
      {
        "step 4": "Refactor the `generateModuleCode` function in `src/stages/stage_2_generation.ts`. Wrap the core logic (LLM API call and JSON parsing) into its own small, inner async function. Then, call this inner function using your new `withRetry` utility. Configure it with sensible defaults, for example, 3 retries and a 2000ms initial delay."
      },
      {
        "step 5": "Define a custom error class named `ModuleGenerationError` in a new file `src/errors.ts`. This error should accept a message and the original error as constructor arguments. In your `generateModuleCode` function, if the `withRetry` logic ultimately fails, catch the error and throw a new `ModuleGenerationError` that clearly states which module failed to generate and why."
      },
      {
        "step 6": "Now, modify the primary orchestrator function in `stage_2_generation.ts` that generates all modules in parallel. Previously, it might have used `Promise.all`. Change this to use `Promise.allSettled`. This ensures that the process continues even if some module generation promises are rejected."
      },
      {
        "step 7": "After `Promise.allSettled` completes, iterate over the results. Create two arrays: `successfulModules` and `failedModules`. Populate these arrays by checking the `status` of each result object ('fulfilled' or 'rejected'). For rejected promises, log the detailed error (the `reason` property, which should be your `ModuleGenerationError`) to provide clear feedback on which modules failed and why."
      },
      {
        "step 8": "Finally, update the unit tests for the main orchestrator function in `stage_2_generation.ts`. Mock the `generateModuleCode` function to simulate a mix of successful generations and failures (i.e., return a mix of resolved and rejected promises). Assert that the function correctly separates the results into `successfulModules` and `failedModules`."
      }
    ]
  },
  "Phase 4: Sandboxed Verification and Correction Loop Implementation": {
    "Task 4.1: Design and build the sandboxed execution environment (e.g., a Docker container with Node.js, Jest, and TypeScript pre-installed).": [
      {
        "step 1": "Create a new directory named `sandbox` which will contain all the files for our execution environment. All subsequent files in this task should be created within this directory unless specified otherwise."
      },
      {
        "step 2": "Inside the `sandbox` directory, create a `package.json` file. This file will define the necessary dependencies for our test execution environment. It should include `jest`, `typescript`, `ts-jest`, `@types/jest`, and `ts-node` as development dependencies. Define a `test` script with the command `jest --json --outputFile=test-results.json`."
      },
      {
        "step 3": "Inside the `sandbox` directory, create a `tsconfig.json` file. Configure it for a modern Node.js environment. Key settings should include `target: 'ES2020'`, `module: 'commonjs'`, `esModuleInterop: true`, `strict: true`, and `skipLibCheck: true`."
      },
      {
        "step 4": "Inside the `sandbox` directory, create a `jest.config.js` file. This file will configure Jest to work with TypeScript. Export a configuration object that sets the `preset` to `'ts-jest'` and the `testEnvironment` to `'node'`."
      },
      {
        "step 5": "Create a file named `Dockerfile` in the `sandbox` directory. This file will define the containerized sandboxed environment. The Dockerfile should: 1. Start with a `node:18-slim` base image. 2. Set the working directory to `/app`. 3. Copy `package.json`, `package-lock.json` (if it exists), `tsconfig.json`, and `jest.config.js` into the working directory. 4. Run `npm install --omit=dev` to install only production dependencies (as test dependencies will be part of the image layer). Then run `npm install` to get all dependencies for the test runner itself. 5. The container will receive code and tests via a mounted volume, so do not copy any application source code at build time. 6. The `CMD` should be set to execute a script that will run the tests, for example, `CMD [\"node\", \"/app/run_tests.js\"]`."
      },
      {
        "step 6": "Create a Node.js script named `run_tests.js` in the `sandbox` directory. This script will be the main entry point executed inside the Docker container. Its purpose is to run Jest and pipe the structured results to stdout for the orchestrator to capture. The script should: 1. Use Node.js's `child_process.exec` to run the `jest` command. Since the `package.json` test script is already configured, you can simply run `npm test`. 2. Capture `stdout`, `stderr` from the Jest process. 3. Use a `try...catch` or promise `.catch()` block to handle errors. If the Jest process fails (exits with a non-zero code), the script should still attempt to read the `test-results.json` file. 4. After the Jest process completes (successfully or not), read the `test-results.json` file. 5. Print the entire JSON content of the results file to `process.stdout`. 6. If `test-results.json` cannot be read, log the captured `stderr` from Jest to `process.stderr` and exit with a non-zero status code."
      },
      {
        "step 7": "In the project's root directory (outside of `sandbox`), create a shell script named `build_sandbox_image.sh`. This script should contain the Docker command to build the image from the `sandbox/Dockerfile`. Tag the image with a clear name, such as `autonomous-agent/execution-sandbox:latest`. Make the script executable."
      },
      {
        "step 8": "To test the entire setup, create a directory in the project root named `test_module_example`. Inside this directory, create two files: 1. `calculator.ts`: Implement a simple `Calculator` class with `add(a, b)` and `subtract(a, b)` methods. 2. `calculator.test.ts`: Write a Jest test suite for the `Calculator` class. Import the class, and write at least one test that passes and one test that is designed to fail (e.g., `expect(calculator.add(2, 2)).toBe(5);`) to verify the failure reporting mechanism."
      },
      {
        "step 9": "In the project's root directory, create a shell script named `run_sandbox_test.sh`. This script will simulate how the orchestrator uses the sandbox. It should: 1. Use the `docker run` command with the `--rm` flag. 2. Mount the `test_module_example/calculator.ts` file to `/app/implementation.ts` inside the container. 3. Mount the `test_module_example/calculator.test.ts` file to `/app/implementation.test.ts` inside the container. 4. Run the `autonomous-agent/execution-sandbox:latest` image. 5. The script should capture and print the standard output from the container, which should be the JSON test report."
      },
      {
        "step 10": "Finally, create a `README.md` file inside the `sandbox` directory. Document the purpose of this sandboxed environment and provide clear instructions on how to use the scripts you've created. Explain how to build the image with `build_sandbox_image.sh` and how to execute the example test with `run_sandbox_test.sh`. Describe the expected JSON output format from Jest."
      }
    ],
    "Task 4.2: Implement the 'Verification Runner' service that takes a module's code and tests, executes them in the sandbox, and captures the structured test results (e.g., Jest's JSON output).": [
      {
        "step 1": "Create a new directory for the service at `packages/verification-runner`. Inside this directory, initialize a new Node.js/TypeScript project by creating a `package.json` (e.g., `npm init -y`) and a `tsconfig.json` (e.g., `npx tsc --init`). Configure `tsconfig.json` with a `rootDir` of `./src`, an `outDir` of `./dist`, and set `module` to `CommonJS` and `target` to `ES2020` or newer. Install necessary dependencies: `typescript`, `ts-node`, `@types/node`, and `jest`, `ts-jest`, `@types/jest` as dev dependencies. Also install `uuid` and `@types/uuid` for generating unique directory names."
      },
      {
        "step 2": "Create a file `packages/verification-runner/src/types.ts`. Define the data structures for this service. Create a `VerificationInput` interface with `implementationCode: string` and `testCode: string`. Create a `VerificationResult` type which will be a union of a `SuccessResult` interface (containing `success: true` and `testResults: any`) and a `FailureResult` interface (containing `success: false`, `error: string`, and optionally `stdout: string`, `stderr: string`). The `testResults` field should be typed to match the structure of Jest's JSON output format."
      },
      {
        "step 3": "Create a `Dockerfile` in the root of the `packages/verification-runner` directory. This file will define the sandboxed test environment. It should use a `node` base image (e.g., `node:18-slim`), create a working directory (e.g., `/usr/src/app`), copy a placeholder `package.json` into it, run `npm install`, and then copy the rest of the test files. The `CMD` should be `[\"npm\", \"test\"]`. This Docker image will be used to run tests in isolation for each module."
      },
      {
        "step 4": "Create a `jest.config.js` file in `packages/verification-runner`. Configure it to use the `ts-jest` preset for running TypeScript tests. Crucially, add configuration to ensure Jest can output a JSON report. You will later invoke Jest with the `--json` and `--outputFile` flags to capture this structured output. Also, create a placeholder `package.json` file in a new `config` directory (e.g., `packages/verification-runner/config/package.json.template`). This template will be copied into each sandbox environment and should list `jest`, `ts-jest`, and `@types/jest` as dev dependencies."
      },
      {
        "step 5": "Create the main service file at `packages/verification-runner/src/runner.ts`. Begin implementing the primary function `runVerification(input: VerificationInput): Promise<VerificationResult>`. In this step, focus on the setup phase: use the `fs/promises` and `path` modules to create a unique temporary directory for each verification run (hint: use the `uuid` library). Inside this directory, write the `implementationCode` to a file (e.g., `module.ts`), the `testCode` to another file (e.g., `module.test.ts`), and copy the `jest.config.js` and the `package.json.template` (as `package.json`) into it."
      },
      {
        "step 6": "In `packages/verification-runner/src/runner.ts`, implement the sandbox execution logic within the `runVerification` function. Use the `child_process` module (specifically `exec` or `spawn`) to run Docker commands. The command sequence should be: 1. `npm install` inside the temporary directory via a Docker container. 2. `jest --json --outputFile results.json` also via a Docker container. The Docker run command must mount the temporary directory as a volume (e.g., `docker run --rm -v $(pwd)/temp-dir:/usr/src/app ...`). Capture `stdout` and `stderr` from these commands."
      },
      {
        "step 7": "Complete the `runVerification` function in `packages/verification-runner/src/runner.ts`. After the Docker command finishes, implement the results processing and cleanup. Check if `results.json` was created in the temporary directory. If it exists, read and parse it, and return a `SuccessResult`. If it does not exist, or if the Docker command failed, construct a `FailureResult` object containing the error message and any captured `stdout`/`stderr`. Finally, ensure the temporary directory and its contents are deleted in all cases (success or failure) using a `try...finally` block."
      },
      {
        "step 8": "Create an entry point file `packages/verification-runner/src/index.ts`. Expose the `runVerification` function. To facilitate testing, add a simple `main` async function that calls `runVerification` with hard-coded sample `implementationCode` and `testCode`. Include one example that should pass and one that should fail (e.g., a test that expects `2 + 2` to be `5`). Use `console.log` to print the structured `VerificationResult` for both cases. This will serve as a manual integration test for the service."
      }
    ],
    "Task 4.3: Develop the prompt and schema for the 'Corrector' agent, designed to accept the original contract, faulty code, and a formal test report.": [
      {
        "step 1": "Create a new file at `src/schemas/corrector_schemas.ts`. In this file, you will define the Zod schemas that govern the inputs and outputs for the Corrector agent. Start by importing `z` from `zod` and the `MODULE_CONTRACT_SCHEMA` from `./contract_schema.ts`."
      },
      {
        "step 2": "In `src/schemas/corrector_schemas.ts`, define and export a Zod schema named `TEST_REPORT_SCHEMA`. This schema will represent the structured output from a test runner. It should be an object with the following properties: `passed` (a boolean), `summary` (a string containing a high-level summary of the test run), and `failedTestDetails` (an array of objects, where each object contains `testName` (string), `errorMessage` (string), and an optional `stackTrace` (string))."
      },
      {
        "step 3": "In the same file, `src/schemas/corrector_schemas.ts`, define and export the main input schema named `CORRECTOR_AGENT_INPUT_SCHEMA`. This schema will represent the complete package of information sent to the Corrector agent. It must be a Zod object with three properties: `moduleContract` (which uses the imported `MODULE_CONTRACT_SCHEMA`), `faultyCode` (a non-empty string), and `testReport` (which uses the `TEST_REPORT_SCHEMA` you just created)."
      },
      {
        "step 4": "Now, define and export the output schema in `src/schemas/corrector_schemas.ts`. Name it `CORRECTOR_AGENT_OUTPUT_SCHEMA`. This schema ensures the Corrector agent's response is predictable. It should be a Zod object with two required string properties: `correctedCode` (the new, fixed version of the code) and `reasoning` (a detailed explanation of what was wrong and how the new code fixes it)."
      },
      {
        "step 5": "Create a new file at `src/prompts/corrector_prompt.ts`. This file will contain the system prompt template for the Corrector agent. Import the `CORRECTOR_AGENT_INPUT_SCHEMA` and `CORRECTOR_AGENT_OUTPUT_SCHEMA` types from `../schemas/corrector_schemas.ts` to ensure type safety."
      },
      {
        "step 6": "In `src/prompts/corrector_prompt.ts`, define and export a function `createCorrectorPrompt(input: z.infer<typeof CORRECTOR_AGENT_INPUT_SCHEMA>): string`. This function will take the structured input and generate the final, detailed prompt string to be sent to the LLM."
      },
      {
        "step 7": "Implement the body of the `createCorrectorPrompt` function. The returned string must be a meticulously crafted prompt that instructs the AI on its role and task. Structure the prompt with the following sections:\n\n1.  **ROLE AND GOAL**: State clearly: 'You are an expert software developer specializing in debugging. Your task is to analyze faulty code, a formal module contract, and a test report, and then produce a corrected version of the code that passes all tests and adheres to the contract.'\n\n2.  **CONTEXT**: Explain the three pieces of input they will receive: `moduleContract`, `faultyCode`, and `testReport`. Emphasize that the `moduleContract` (specifically its `promptInstructions` and `acceptanceTests` fields) is the ultimate source of truth for the required functionality.\n\n3.  **CRITICAL RULES**: Provide a list of non-negotiable rules:\n    *   'Methodically analyze the `testReport` to understand the exact error messages and stack traces.'\n    *   'Cross-reference the `faultyCode` with the `moduleContract` to identify where the implementation deviates from the specification.'\n    *   'Your primary goal is to fix the bugs. Rewrite the code to be correct, robust, and efficient.'\n    *   'You MUST NOT alter the public interface. The names and signatures of functions/classes defined in the contract's `publicAPI` are immutable.'\n    *   'In your `reasoning`, provide a clear, step-by-step explanation of your diagnosis and the rationale for your fix.'\n\n4.  **INPUT DATA**: Use template literals to inject the stringified JSON of the `moduleContract`, the `faultyCode` as a string, and the stringified JSON of the `testReport` into clearly marked sections within the prompt.\n\n5.  **OUTPUT INSTRUCTIONS**: Conclude with a strict instruction for the output format: 'You MUST reply with a single, valid JSON object that conforms to the `CORRECTOR_AGENT_OUTPUT_SCHEMA`. Do not include markdown formatting, comments, or any other text outside of the JSON object.'"
      }
    ],
    "Task 4.4: Implement the core 'Correction Loop' in the orchestrator, which invokes the Corrector agent on test failure and re-submits the code for verification, respecting a retry limit.": [
      {
        "step 1": "Create a new prompt template specifically for the Corrector Agent. In `src/agents/prompts.ts`, add a new exported constant `CORRECTOR_PROMPT_TEMPLATE`. This prompt should instruct the AI that it is a debugging expert. It will receive a module contract, the faulty implementation code, the original test code, and a detailed test failure report. Its goal is to analyze the error and fix ONLY the `implementationCode` while leaving the `testCode` unchanged. The output must be a JSON object with keys `implementationCode` and `testCode`."
      },
      {
        "step 2": "Create the `CorrectorAgent`. In the `src/agents/` directory, create a new file `CorrectorAgent.ts`. This agent will be responsible for invoking the LLM to fix code. Implement a function `invokeCorrectorAgent(contract: ModuleContract, faultyCode: { implementationCode: string; testCode: string }, testReport: string): Promise<{ implementationCode: string; testCode: string }>`. This function should format the `CORRECTOR_PROMPT_TEMPLATE` with the provided arguments, call the LLM service, parse the JSON response, and return the corrected code. Ensure robust error handling for LLM calls and JSON parsing."
      },
      {
        "step 3": "Introduce configuration for the correction loop. In your main configuration file (e.g., `src/config.ts` or a constants file), add a new constant `MAX_CORRECTION_RETRIES` and set its value to `3`. This will control how many times the system attempts to fix a failing module."
      },
      {
        "step 4": "Update the module status tracking to accommodate the correction loop. Locate the type or enum that defines the status of a module (likely in `src/types.ts` or `src/orchestrator.ts`). Add new statuses to represent the verification and correction lifecycle, such as `VERIFICATION_FAILED`, `CORRECTION_IN_PROGRESS`, and `VERIFIED_AFTER_CORRECTION`. This will provide more granular insight into the build process."
      },
      {
        "step 5": "Refactor the verification logic in the orchestrator to prepare for the loop. In the main orchestrator file (e.g., `src/orchestrator.ts`), locate the function responsible for Stage 3 (Verification). Isolate the logic that runs the tests for a single module into a separate, reusable function, for example `runVerification(module: Module): Promise<{ passed: boolean; report: string }>`. This function should execute the tests in the sandbox and return a structured result without modifying the module's state directly."
      },
      {
        "step 6": "Implement the core correction loop within the Stage 3 orchestration logic. For each module, replace the single verification call with a `for` or `while` loop that iterates up to `MAX_CORRECTION_RETRIES` times. Inside the loop, do the following:\n1. Call the `runVerification` function from the previous step.\n2. If `result.passed` is true, update the module's status to `VERIFIED` (or `VERIFIED_AFTER_CORRECTION` if it's not the first attempt), log the success, and break the loop.\n3. If `result.passed` is false, log the failure and the attempt number.\n4. If the retry limit has been reached, update the module's status to `VERIFICATION_FAILED`, log the final failure, and break the loop.\n5. If retries remain, update the module's status to `CORRECTION_IN_PROGRESS`, call the `invokeCorrectorAgent` with the module's contract, the failing code, and the test report.\n6. Replace the module's `implementationCode` with the newly corrected code from the agent and continue to the next iteration of the loop."
      },
      {
        "step 7": "Integrate the `Dependency Failure Propagation` logic. After the verification/correction loops for all modules have completed, iterate through the modules one last time. For each module, check its final status. If a module's status is `VERIFICATION_FAILED`, find all other modules that list it as a dependency. Mark each of these dependent modules as `FAILED_DEPENDENCY`, as they cannot be integrated correctly."
      },
      {
        "step 8": "Review and refine the logging within the orchestrator. Add detailed log statements at each key step of the correction loop: when verification fails, when a correction attempt is initiated, when corrected code is received, and the final outcome (success or failure) for each module. This is critical for debugging the autonomous system itself."
      }
    ],
    "Task 4.5: Implement the 'Dependency Failure Propagation' logic that marks modules as failed if any of their dependencies failed verification.": [
      {
        "step 1": "Create a new file at `src/verification/dependency-propagator.ts`. Inside this file, define the necessary types for the verification process. Specifically, create a `VerificationStatus` string literal type ('PASSED' | 'FAILED' | 'PENDING') and a `ModuleVerificationResult` interface with properties: `moduleName: string`, `dependencies: string[]`, and `status: VerificationStatus`."
      },
      {
        "step 2": "In `src/verification/dependency-propagator.ts`, implement an exported function `propagateDependencyFailures(results: ModuleVerificationResult[]): ModuleVerificationResult[]`. This function must not mutate the input array. It should perform the following logic:\n1. Create a deep copy of the input `results` array.\n2. Create a `Set<string>` containing the names of all modules that initially have a 'FAILED' status.\n3. Use a loop that continues as long as new failures are propagated in a pass (a `do-while` loop is a good choice here).\n4. In each pass, iterate through all modules in the copied array. For each module that is not already marked 'FAILED', check if any of its `dependencies` exist in the failed modules set.\n5. If a dependency has failed, update the current module's status to 'FAILED', add its name to the failed modules set, and signal that a change occurred in this pass.\n6. The loop terminates when a full pass over all modules results in no new failures being added.\n7. Return the updated array of results."
      },
      {
        "step 3": "Create a corresponding test file at `src/verification/dependency-propagator.test.ts`. Import the `propagateDependencyFailures` function and the `ModuleVerificationResult` type. Use a testing framework like Jest or Vitest to set up the test suite. Prepare mock data for the test cases."
      },
      {
        "step 4": "In `dependency-propagator.test.ts`, write unit tests for the following basic scenarios:\n1. **No Failures:** A test case where all modules have a 'PASSED' status. Assert that the output is identical to the input.\n2. **Direct Dependency Failure:** A test case with three modules (A, B, C). Module A depends on B. Module B's initial status is 'FAILED'. Assert that the final status of Module A is 'FAILED', while C's status remains unchanged."
      },
      {
        "step 5": "In `dependency-propagator.test.ts`, add more complex unit tests to ensure robustness:\n1. **Transitive Dependency Failure:** A test case where Module A depends on B, and B depends on C. C's initial status is 'FAILED'. Assert that both A and B are marked as 'FAILED' in the output.\n2. **Multiple Dependencies Failure:** A test case where Module A depends on B and C. C's initial status is 'FAILED'. Assert that A is marked as 'FAILED'.\n3. **Diamond Dependency Failure:** A test case where D depends on B and C; B and C both depend on A. A's initial status is 'FAILED'. Assert that B, C, and D are all marked as 'FAILED' in the output."
      },
      {
        "step 6": "Finally, integrate the new logic into the main system orchestrator. Locate the code responsible for managing 'Stage 3: Verification, Correction, and Integration'. After the initial parallel verification and correction loops are complete and you have a definitive list of each module's pass/fail status, call the `propagateDependencyFailures` function. Use the result from this function (the list with propagated failures) for all subsequent logic, such as determining which modules to pass to the 'Integrator' agent."
      }
    ],
    "Task 4.6: Implement the 'Integrator' agent, responsible for performing a topological sort on the dependency graph of successful modules.": [
      {
        "step 1": "Create a new file `src/agents/integrator.ts`. In this file, define a class named `Integrator`. This class will be responsible for orchestrating the final assembly of the project. For now, its constructor should accept an array of `ModuleContract` objects. Create a placeholder public method `generateInstantiationOrder()` that takes a list of successful module names (string array) and returns an array of strings."
      },
      {
        "step 2": "Inside the `Integrator` class, implement a private helper method `_buildDependencyGraph(successfulModuleNames: string[])`. This method should parse the `ModuleContract[]` stored in the class instance. It will construct two data structures: an adjacency list (e.g., `Map<string, string[]>`) representing the graph where a key is a module and the value is an array of its dependencies, and an in-degree map (e.g., `Map<string, number>`) tracking how many incoming edges each module has. Only include modules present in the `successfulModuleNames` list in your graph."
      },
      {
        "step 3": "Implement the core topological sort logic (Kahn's algorithm) within the `generateInstantiationOrder` method. Use the `_buildDependencyGraph` helper to get the initial graph and in-degrees. Initialize a queue with all modules that have an in-degree of 0. Then, loop while the queue is not empty: dequeue a module, add it to your sorted list, and for each of its neighbors, decrement their in-degree. If a neighbor's in-degree becomes 0, enqueue it."
      },
      {
        "step 4": "Enhance your topological sort implementation to include robust cycle detection. A cycle exists if the final sorted list of modules does not contain all the modules that were initially in the graph (i.e., the `successfulModuleNames`). If a cycle is detected, throw a specific `Error` with a message that clearly indicates a circular dependency was found and lists the modules that could not be sorted."
      },
      {
        "step 5": "Refactor the `Integrator` class to ensure clear separation of concerns. The `generateInstantiationOrder` method should be the main public entry point that orchestrates the calls to `_buildDependencyGraph` and the sorting loop. Ensure the final output is the correctly ordered list of module names for instantiation, or an error is thrown for cycles."
      },
      {
        "step 6": "Create a new test file `src/agents/integrator.test.ts`. Write unit tests for the `Integrator` class using a testing framework like Jest. Your tests should cover successful sorting scenarios. Use mock `ModuleContract` objects to test at least the following cases: a simple linear dependency (e.g., A depends on B, B depends on C), a more complex directed acyclic graph (e.g., A depends on B and C, B depends on D, C depends on D), and a set of modules with no dependencies."
      },
      {
        "step 7": "Add unit tests to `src/agents/integrator.test.ts` specifically for failure conditions. Test that the `generateInstantiationOrder` method correctly throws an error when the dependency graph contains a cycle. Create mock contracts for a simple two-module cycle (A -> B -> A) and a more complex three-module cycle (A -> B -> C -> A). Verify that the error message is informative."
      },
      {
        "step 8": "Add a final unit test case to handle scenarios where a module lists a dependency that is not in the list of successful modules. The `_buildDependencyGraph` method should gracefully ignore dependencies on modules that are not in the `successfulModuleNames` input array, as those modules are considered failed and out of scope for integration."
      }
    ],
    "Task 4.7: Enhance the 'Integrator' agent to generate the main application entry point (e.g., main.ts) by correctly importing and instantiating all verified modules.": [
      {
        "step 1": "Create a new file `src/agents/integrator.ts`. This file will house the logic for the Integrator agent, specifically the function to generate the main application entry point."
      },
      {
        "step 2": "In `src/agents/integrator.ts`, import the necessary types, especially `ModuleContract` from your schema definitions. Then, define the function signature for the entry point generator: `export function generateMainEntryPoint(contracts: ModuleContract[], verifiedModuleNames: string[]): string`. This function will take the full list of contracts and the names of the modules that have successfully passed verification."
      },
      {
        "step 3": "Implement a helper function within `integrator.ts` to perform a topological sort on the module dependency graph. The function signature should be `function topologicalSort(contracts: ModuleContract[], verifiedModuleNames: string[]): string[]`. It should first build a directed graph (e.g., using an adjacency list `Map<string, string[]>`) where nodes are module names and edges represent dependencies. Crucially, only include modules present in the `verifiedModuleNames` list. The function should return an array of module names in the correct instantiation order or throw an error if a cycle is detected."
      },
      {
        "step 4": "In the `generateMainEntryPoint` function, begin by calling the `topologicalSort` function to get the ordered list of modules. Handle the potential error for cyclic dependencies gracefully. Store the result in a variable, for example, `instantiationOrder`."
      },
      {
        "step 5": "Next, implement the code generation for the import statements. Iterate through the `verifiedModuleNames`. For each module name, find its corresponding contract, and generate a TypeScript import statement. For a module named 'Database', the output should be `import { Database } from './modules/Database';`. Concatenate these import strings into a single block."
      },
      {
        "step 6": "Now, implement the code generation for module instantiation. Iterate through the `instantiationOrder` array. For each module name: \n1. Find its contract in the `contracts` array.\n2. Look up its `constructorParams`. \n3. Generate the instantiation code, e.g., `const database = new Database(config);`. \n4. The arguments for the constructor must be the variable names of the dependencies, which should have already been instantiated due to the topological sort. Use a convention like camelCase for variable names (e.g., 'Database' module becomes 'database' variable). Keep a map of module names to their instance variable names for easy lookup."
      },
      {
        "step 7": "After generating the imports and instantiations, add a simple asynchronous IIFE (Immediately Invoked Function Expression) to the end of the generated code string. This will serve as the main execution block. For example: `\n\n(async () => {\n  console.log('Application starting...');\n  // You can add a call to a method on the final module in the chain here if applicable.\n})();\n`. This makes the generated `main.ts` file runnable."
      },
      {
        "step 8": "Create a new test file `src/agents/integrator.test.ts`. Import the `generateMainEntryPoint` function and the `ModuleContract` type."
      },
      {
        "step 9": "Write unit tests for the `topologicalSort` helper function. Create mock `ModuleContract` data to test several scenarios: \n1. A simple linear dependency (A depends on B, B depends on C). \n2. A more complex graph with multiple dependencies (e.g., a diamond shape). \n3. A graph with a cycle, and assert that your function correctly throws an error. \n4. A case where some modules are not in the `verifiedModuleNames` list and ensure they are excluded from the sort."
      },
      {
        "step 10": "Write a comprehensive test for the `generateMainEntryPoint` function. Create a set of mock `ModuleContract` objects representing a small but non-trivial project. Call `generateMainEntryPoint` with these mocks and a list of verified modules. Assert that the returned string contains: \n1. The correct import statements for all verified modules. \n2. The correct instantiation order as determined by a topological sort. \n3. The correct dependency injection, where constructor arguments are the variable names of previously instantiated modules."
      },
      {
        "step 11": "Finally, review the complete implementation in `src/agents/integrator.ts`. Add JSDoc comments to all functions explaining their purpose, parameters, and return values. Ensure strong typing is used throughout and that any potential edge cases (like an empty list of verified modules) are handled correctly. Refactor the code for clarity and maintainability."
      }
    ],
    "Task 4.8: Task the 'Integrator' agent with generating necessary project configuration files (package.json, tsconfig.json, README.md).": [
      {
        "step 1": "First, you must determine the project's dependencies to create an accurate `package.json`. Scan the `implementationCode` for all successfully verified modules. Analyze the `import` statements to identify all external npm packages. Ignore relative imports (e.g., './module') and built-in Node.js modules (e.g., 'fs', 'path'). Compile these into a list of production dependencies. Your dev dependencies should include `typescript`, `ts-node`, `jest`, `ts-jest`, `@types/node`, and `@types/jest`. Store these two lists for the next step."
      },
      {
        "step 2": "Using the dependency lists from the previous step, generate the `package.json` file. The file should be created in the root of the project directory. Use the original high-level user goal to create a kebab-case `name` and a descriptive `description`. Set the `version` to `1.0.0` and `main` to `dist/main.js`. Populate the `dependencies` and `devDependencies` fields. Define the `scripts` section with the following commands: `\"start\": \"ts-node src/main.ts\"`, `\"build\": \"tsc\"`, and `\"test\": \"jest\"`."
      },
      {
        "step 3": "Generate the TypeScript configuration file, `tsconfig.json`, in the project root. Use the following JSON content, which provides a robust configuration for a modern Node.js project. Ensure the file is created with exactly this content. \n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"CommonJS\",\n    \"rootDir\": \"./src\",\n    \"outDir\": \"./dist\",\n    \"esModuleInterop\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"strict\": true,\n    \"skipLibCheck\": true,\n    \"resolveJsonModule\": true\n  },\n  \"include\": [\"src/**/*.ts\"],\n  \"exclude\": [\"node_modules\", \"**/*.test.ts\"]\n}\n```"
      },
      {
        "step 4": "Generate a comprehensive `README.md` file in the project root. Synthesize information from the original user goal and the `ModuleContract[]` document. The README should have the following structure:\n1.  **Project Title:** A main heading (`#`) derived from the user goal.\n2.  **Description:** A paragraph summarizing the project's purpose.\n3.  **Architecture Overview:** A sub-heading (`##`) followed by a list of all modules defined in the `ModuleContract[]` document. For each module, list its name and `purpose`.\n4.  **Installation:** A sub-heading with instructions: `npm install`.\n5.  **Running the Application:** A sub-heading with instructions: `npm start`.\n6.  **Running Tests:** A sub-heading with instructions: `npm test`."
      }
    ],
    "Task 4.9: Implement the 'Final Assembler' utility that writes all generated source code, tests, and configuration files into a coherent project directory structure.": [
      {
        "step 1": "Create a new file at `src/utils/final_assembler.ts`. Inside this file, define the necessary types for the inputs. Create a `SuccessfulModule` type with `name: string`, `implementationCode: string`, and `testCode: string`. Then, define an `AssemblyData` type that includes `successfulModules: SuccessfulModule[]`, `packageJsonContent: string`, `tsConfigJsonContent: string`, `mainTsContent: string`, and `readmeContent: string`. Finally, define the main function signature: `export async function assembleProject(data: AssemblyData, outputDir: string): Promise<string>`. The function will eventually return the path to the final zip archive."
      },
      {
        "step 2": "In `assembleProject`, implement the project directory creation logic. Use the `fs/promises` module. First, create the main output directory specified by `outputDir`. Then, create the `src` and `tests` subdirectories within it. Use `path.join` for constructing paths to ensure cross-platform compatibility. Hint: Use the `{ recursive: true }` option in `fs.mkdir` to prevent errors if the directory already exists."
      },
      {
        "step 3": "Implement the logic to write the module source code and test files. Iterate over the `data.successfulModules` array. For each module, create a suitable filename from its `name` (e.g., 'MyApiModule' -> 'myApiModule.ts'). Write the `implementationCode` to a file in the `src` directory (e.g., `[outputDir]/src/myApiModule.ts`) and the `testCode` to a file in the `tests` directory (e.g., `[outputDir]/tests/myApiModule.test.ts`). Use `fs/promises.writeFile` for all file operations."
      },
      {
        "step 4": "Next, write the static configuration and entry point files to the root of the project directory. Use `fs/promises.writeFile` to create the following files with their corresponding content from the `data` object: `[outputDir]/package.json`, `[outputDir]/tsconfig.json`, `[outputDir]/README.md`, and `[outputDir]/main.ts`."
      },
      {
        "step 5": "Now, implement the zipping functionality. First, add the `archiver` library to the project dependencies by running `npm install archiver` and `npm install @types/archiver --save-dev`. Create a new private helper function `async function zipDirectory(sourceDir: string, outPath: string): Promise<void>`. Inside this function, use `archiver` to create a zip archive of the `sourceDir` and save it to `outPath`. Ensure you handle the stream events correctly (`'close'`, `'error'`) within a Promise."
      },
      {
        "step 6": "Integrate the zipping functionality into the `assembleProject` function. After all files and directories have been written successfully, define the output path for the zip file (e.g., `${outputDir}.zip`). Call your `zipDirectory` helper function with the `outputDir` and the new zip file path. The `assembleProject` function should return the path to this generated zip file upon successful completion. Add comprehensive `try...catch` blocks around all I/O operations and include `console.log` statements to provide feedback on the assembly process (e.g., 'Creating directories...', 'Writing module file X...', 'Zipping project...')."
      },
      {
        "step 7": "Create a new test file at `src/utils/final_assembler.test.ts`. To test the file system operations without writing to disk, install the `mock-fs` library: `npm install mock-fs --save-dev` and `npm install @types/mock-fs --save-dev`. Set up a basic test suite using your preferred test runner (e.g., Jest) and import `assembleProject` and `mock-fs`."
      },
      {
        "step 8": "Write a comprehensive unit test for `assembleProject`. In the test, define mock `AssemblyData`. Use `mock-fs` to set up a virtual file system before the test runs and restore it afterward (`mock.restore()`). Call `assembleProject` with the mock data. After it executes, use `fs.readFileSync` and `fs.existsSync` (which will operate on the mocked file system) to assert that: 1. The `src` and `tests` directories were created. 2. All module source files and test files exist in the correct locations with the correct content. 3. All configuration files (`package.json`, etc.) exist in the root directory with the correct content."
      }
    ],
    "Task 4.10: Implement the final project archiving utility to package the assembled project directory into a single downloadable .zip file.": [
      {
        "step 1": "Create a new file at `src/services/archiver.service.ts`. This file will contain the utility function for creating zip archives."
      },
      {
        "step 2": "To handle zip file creation efficiently, you will use the `archiver` library. Add it and its corresponding type definitions to the project's development dependencies. Execute the following command: `npm install archiver --save` and `npm install @types/archiver --save-dev`."
      },
      {
        "step 3": "In `src/services/archiver.service.ts`, define and export an asynchronous function named `createProjectZip`. This function should accept two string parameters: `sourceDir` (the path to the directory to be zipped) and `outPath` (the file path for the output zip archive). It should return a `Promise<void>` that resolves upon successful creation of the archive and rejects on error."
      },
      {
        "step 4": "Implement the logic for the `createProjectZip` function. Use the `fs` module to create a write stream to `outPath`. Initialize an `archiver` instance with the 'zip' format. Your implementation must handle the full lifecycle: pipe the archiver to the output stream, listen for the 'close' event on the stream to resolve the promise, and listen for 'error' events on both the stream and the archiver to reject the promise. Use `archiver.directory(sourceDir, false)` to add the contents of the source directory to the root of the archive. Finally, call `archiver.finalize()` to start the process."
      },
      {
        "step 5": "Create a corresponding test file at `src/services/archiver.service.test.ts` to ensure the archiving utility is robust and reliable."
      },
      {
        "step 6": "In `src/services/archiver.service.test.ts`, write a test case for the successful creation of a zip archive. Use the `fs/promises` and `path` modules to create a temporary directory structure for testing (e.g., `tmp/test-project/`). Populate it with a few dummy files (e.g., `index.js`, `data/file.txt`). Call `createProjectZip` and assert that the output zip file exists after the function completes. Ensure you implement proper setup and teardown logic to create and remove the temporary files and directories for each test."
      },
      {
        "step 7": "To verify the integrity of the created archive, add a library for reading zip files to your dev dependencies. `adm-zip` is a good choice. Run `npm install adm-zip @types/adm-zip --save-dev`. Enhance your primary success test case: after creating the zip, use `adm-zip` to read the archive and assert that the list of file entries and their contents match the temporary source directory you created."
      },
      {
        "step 8": "Add a test case to verify the error handling. Call `createProjectZip` with a path to a non-existent source directory. Assert that the returned promise is rejected, and that the error message is appropriate. This ensures the function behaves predictably when given invalid input."
      }
    ]
  },
  "Phase 5: Final Integration, Assembly, and Packaging": {
    "Task 5.1: Determine the final set of successful modules by propagating dependency failures.": [
      {
        "step 1": "Create a new script file named `src/integration/propagate_failures.ts` to house the logic for determining the final set of successful and failed modules."
      },
      {
        "step 2": "In `propagate_failures.ts`, define the necessary type definitions to represent the module contracts and verification results. You will need to read `build/contracts.json` and `build/verification_results.json`. Assume `verification_results.json` is an array of objects, each with `moduleName: string` and `status: 'success' | 'failure'`."
      },
      {
        "step 3": "Implement a function to load the module contracts from `build/contracts.json` and the verification results from `build/verification_results.json`. This function should return both data structures for further processing."
      },
      {
        "step 4": "Implement a function `buildDependencyGraphs(contracts: ModuleContract[])`. This function should create and return two data structures: a `dependencies` graph and a `dependents` graph. A `Map<string, string[]>` is a suitable choice for both. The `dependencies` graph maps a module to the modules it depends on. The `dependents` graph maps a module to the modules that depend on it (the reverse mapping)."
      },
      {
        "step 5": "Implement the core failure propagation logic in a function `calculateFinalStatuses`. This function should take the list of all module names, the verification results, and the `dependents` graph as input. Follow this algorithm: \n1. Initialize a `Set<string>` called `failedModules` with the names of all modules that have a 'failure' status in the verification results. \n2. Initialize a queue (e.g., an array) with the initial list of failed modules. \n3. While the queue is not empty, dequeue a module name. \n4. Find all modules that depend on this failed module using your `dependents` graph. \n5. For each dependent module, if it's not already in the `failedModules` set, add it to the set and enqueue it. \n6. After the loop terminates, the `failedModules` set will contain the complete list of all failed modules (initial and propagated). \n7. Determine the list of `successfulModules` by taking the set difference between all module names and the final `failedModules` set."
      },
      {
        "step 6": "Create a main execution function that orchestrates the entire process: \n1. Call the function to load contracts and verification results. \n2. Call `buildDependencyGraphs` to get the dependency mappings. \n3. Call `calculateFinalStatuses` to get the final lists of successful and failed modules. \n4. Write the results to a new file at `build/final_module_statuses.json`. The JSON structure should be: `{ \"successfulModules\": string[], \"failedModules\": string[] }`."
      },
      {
        "step 7": "Add a command to your `package.json` scripts section, e.g., `\"propagate:failures\": \"ts-node src/integration/propagate_failures.ts\"`, to allow for easy execution of this script."
      }
    ],
    "Task 5.2: Perform a topological sort on the dependency graph of successful modules.": [
      {
        "step 1": "Create a new file at `src/integrator/dependency_sorter.ts`. This utility file will be responsible for performing a topological sort on the dependency graph of the verified modules."
      },
      {
        "step 2": "In `src/integrator/dependency_sorter.ts`, define and export a function named `topologicalSort`. This function should accept two arguments: `allModules: ModuleContract[]` (the full list of module contracts from the contract document) and `successfulModuleNames: Set<string>` (a set of names for the modules that have passed verification). The function should return a `string[]` representing the correctly ordered list of module names for instantiation. Import the `ModuleContract` type from `src/types/contract.ts`."
      },
      {
        "step 3": "Implement the `topologicalSort` function using Kahn's algorithm. Start by filtering the `allModules` array to only include the modules present in the `successfulModuleNames` set. Then, build the necessary data structures for the algorithm: an in-degree map (`Map<string, number>`) and an adjacency list (`Map<string, string[]>`).\n\n**Hint:**\n1. Initialize the in-degree for every successful module based on the length of its `dependencies` array.\n2. The adjacency list should map a dependency to the modules that depend on it. For a module `A` that depends on `B`, the entry should be `adj.get('B').push('A')`."
      },
      {
        "step 4": "Continue the implementation of `topologicalSort`. Initialize a queue with all successful modules that have an in-degree of 0 (i.e., no dependencies within the successful set). Then, begin processing the queue: dequeue a module, add its name to the final sorted list, and for each module that depends on it (using your adjacency list), decrement its in-degree. If a dependent's in-degree becomes 0, add it to the queue. Continue this process until the queue is empty."
      },
      {
        "step 5": "Finalize the `topologicalSort` function by adding robust error handling for circular dependencies. After the main processing loop is complete, compare the length of your resulting sorted list with the total number of successful modules. If they do not match, it indicates a cycle in the dependency graph. In this case, throw a `new Error('Circular dependency detected among successful modules. Cannot complete integration.')`."
      },
      {
        "step 6": "Create a corresponding test file `src/integrator/dependency_sorter.test.ts`. Write a comprehensive suite of unit tests for the `topologicalSort` function using Jest. Mock the `ModuleContract` data to test the following scenarios:\n1. A simple linear dependency chain.\n2. A complex Directed Acyclic Graph (DAG) with multiple branches (e.g., a diamond dependency).\n3. A graph containing modules with no dependencies.\n4. A graph with a direct circular dependency (A -> B, B -> A) to verify that an error is thrown.\n5. A graph with an indirect circular dependency (A -> B -> C -> A) to verify error handling.\n6. A scenario where `successfulModuleNames` correctly filters out a module that would otherwise be part of a cycle."
      }
    ],
    "Task 5.3: Generate the main application entry point (e.g., main.ts) to integrate all successful modules.": [
      {
        "step 1": "Analyze the dependency graph of all successfully verified modules. You will be provided with the full `module_contracts.json` document and a list of module names that have passed verification. Filter the contracts to include only the successful modules. From these, construct a directed graph where an edge from module A to module B means A depends on B. Perform a topological sort on this graph to determine the precise order in which modules must be instantiated. Output the topologically sorted list of module names."
      },
      {
        "step 2": "Using the topologically sorted list of module names from the previous step, generate the TypeScript import statements for the main entry point file. For each module (e.g., 'Database'), assume its class is exported from a file with the same name within a 'src/modules' directory (e.g., `import { Database } from './modules/Database';`)."
      },
      {
        "step 3": "Generate the TypeScript instantiation code for all successful modules. Iterate through the topologically sorted list of module names. For each module, create a `const` variable (e.g., `const database = new Database(...)`). Use the `module_contracts.json` to determine the correct constructor parameters (`constructorParams`). The arguments passed to the constructor must be the variable names of the dependencies, which will have already been instantiated due to the topological sort. Ensure variable names are a camelCase version of the module class name (e.g., 'ApiServer' -> 'apiServer')."
      },
      {
        "step 4": "Assemble the final `main.ts` file. Combine the import statements from step 2 and the instantiation code from step 3. Wrap the instantiation logic within a self-executing `async function main()`. Include `try/catch` error handling for the initialization process. Add console logs to indicate the start of initialization and its successful completion. Add a placeholder comment for where application startup logic (e.g., `server.listen()`) would be called after all modules are instantiated."
      }
    ],
    "Task 5.4: Generate the project's package.json file, including all necessary dependencies and scripts.": [
      {
        "step 1": "Analyze all verified module code to identify external dependencies. Read the `implementationCode` and `testCode` for every successful module. Scan for all `import` or `require` statements that point to external npm packages. Ignore Node.js built-in modules (e.g., 'fs', 'path') and local relative imports (e.g., './', '../'). Create two distinct lists: one for production dependencies found in `implementationCode` and one for development dependencies found in `testCode`."
      },
      {
        "step 2": "Augment the development dependencies list with standard TypeScript project tooling. To the list of development dependencies you identified in the previous step, add the following essential packages if they are not already present: 'typescript', 'ts-node', 'jest', 'ts-jest', '@types/jest', and '@types/node'. Ensure the final list of development dependencies is unique."
      },
      {
        "step 3": "Define the project metadata and scripts. Using the original high-level user goal as a reference, create a suitable 'name' (e.g., 'real-time-chat-app') and 'description' for the project. Prepare a JSON object with the following standard fields and values: 'version': '1.0.0', 'main': 'dist/main.js', 'author': 'Autonomous AI Agent', 'license': 'MIT'."
      },
      {
        "step 4": "Define the standard npm scripts for the project. Create a 'scripts' object with the following key-value pairs: 'start': 'ts-node src/main.ts', 'test': 'jest', and 'build': 'tsc'."
      },
      {
        "step 5": "Assemble and create the final package.json file. Combine the production dependency list, the augmented development dependency list, the project metadata, and the scripts into a single, well-formatted JSON object. For all packages in the dependency lists, use 'latest' as the version string. Write this final JSON object to a new file named 'package.json' in the project's root directory."
      }
    ],
    "Task 5.5: Generate the tsconfig.json configuration file.": [
      {
        "step 1": "Analyze the project's file structure, specifically noting the location of source files (e.g., in a `src` directory) and test files. Also, review the `package.json` file to identify TypeScript-related dependencies (`typescript`, `ts-node`, `ts-jest`, etc.) and scripts. This context will inform the creation of the `tsconfig.json` file."
      },
      {
        "step 2": "Generate the `compilerOptions` for a modern, robust Node.js TypeScript project. Create a JSON object with the following recommended settings:\n- `target`: \"ES2022\"\n- `module`: \"NodeNext\"\n- `moduleResolution`: \"NodeNext\"\n- `outDir`: \"./dist\"\n- `rootDir`: \"./src\"\n- `strict`: true\n- `esModuleInterop`: true\n- `skipLibCheck`: true\n- `resolveJsonModule`: true\n- `sourceMap`: true\n- `declaration`: true\n- `baseUrl`: \".\"\nEnsure these options are appropriate for a backend application that will be compiled to JavaScript for production."
      },
      {
        "step 3": "Now, add the `include` and `exclude` properties to your configuration object. Set `include` to an array containing `[\"src/**/*\"]` to ensure all files within the source directory are processed. Set `exclude` to an array containing `[\"node_modules\", \"dist\", \"**/*.test.ts\", \"**/*.spec.ts\"]` to prevent the compiler from processing dependencies, the output directory, and test files during the main build."
      },
      {
        "step 4": "Combine the `compilerOptions`, `include`, and `exclude` properties into a single, valid JSON object. Write this final configuration to a new file named `tsconfig.json` in the root directory of the project. Ensure the file is well-formatted with standard indentation."
      }
    ],
    "Task 5.6: Generate a comprehensive README.md file for the project.": [
      {
        "step 1": "Analyze all available project artifacts to gather context for the README.md file. Load and parse the original user goal, the `reasoning_tree.json`, the final `ModuleContract[]` document, and the `package.json` file. Extract key information such as the project's main objective, the high-level architecture (module names and their purposes), and the commands required for installation and execution."
      },
      {
        "step 2": "Draft the content for the README.md in Markdown format. Based on the artifacts analyzed in the previous step, generate text for the following sections: \n1. **Project Title:** A concise and descriptive title based on the user goal. \n2. **Description:** A one or two-paragraph overview of what the project does. \n3. **Architecture Overview:** A section that lists each module from the `ModuleContract[]` document, briefly explaining its purpose and its primary dependencies. This explains *how* the project is built. \n4. **Installation:** A 'Getting Started' section that provides the exact command (e.g., `npm install`) from `package.json` to install dependencies. \n5. **Usage:** A section with sub-headings for 'Running the Application' and 'Running Tests', providing the specific commands defined in the `package.json` scripts. \n6. **Built With:** A brief concluding note stating that the project was autonomously generated by this framework, highlighting the principles of Hierarchical Planning and Contract-First Design."
      },
      {
        "step 3": "Assemble the drafted sections into a single, cohesive Markdown document. Ensure proper formatting with appropriate headings (e.g., `#`, `##`), code fences for commands (```bash...```), and lists. Pay attention to clarity and readability."
      },
      {
        "step 4": "Write the final, formatted Markdown string to a new file named `README.md` in the root directory of the project."
      }
    ],
    "Task 5.7: Assemble all generated source code, tests, and configuration files into the final project directory structure.": [
      {
        "step 1": "Create the root directory for the final project named `output_project`. Inside `output_project`, create two subdirectories: `src` and `tests`."
      },
      {
        "step 2": "Inside the `output_project/src` directory, create a new subdirectory named `modules`. Similarly, inside the `output_project/tests` directory, create a subdirectory named `modules`. This will keep the module source code and their corresponding tests organized."
      },
      {
        "step 3": "Access the collection of all verified modules from Stage 3. For each module, retrieve its `implementationCode` and `moduleName`. Write the `implementationCode` to a new file inside the `output_project/src/modules/` directory. The filename must be `{moduleName}.ts`, using the exact name from the module contract (e.g., `Logger.ts`, `APIClient.ts`)."
      },
      {
        "step 4": "Using the same collection of verified modules, now write the corresponding test files. For each module, retrieve its `testCode` and `moduleName`. Write the `testCode` to a new file inside the `output_project/tests/modules/` directory. The filename must be `{moduleName}.test.ts` (e.g., `Logger.test.ts`, `APIClient.test.ts`)."
      },
      {
        "step 5": "Retrieve the `main.ts` content that was generated by the Integrator agent in Task 5.3. Write this content to the file path `output_project/src/main.ts`."
      },
      {
        "step 6": "Retrieve the contents for `package.json` (from Task 5.4), `tsconfig.json` (from Task 5.5), and `README.md` (from Task 5.6). Write each of these files to the root of the `output_project` directory."
      },
      {
        "step 7": "As a final assembly step, create a standard `.gitignore` file in the `output_project` root directory to ensure version control hygiene. Populate it with common patterns for a Node.js/TypeScript project. Hint: Include entries such as `node_modules/`, `dist/`, `build/`, `.env`, `*.log`, `coverage/`, and IDE-specific folders like `.vscode/`."
      }
    ],
    "Task 5.8: Create a final, downloadable .zip archive of the complete project.": [
      {
        "step 1": "Create a new JavaScript file named `create_archive.js` in the project's root directory. This script will be responsible for packaging the final project."
      },
      {
        "step 2": "To handle the creation of the zip archive, you will need the `archiver` library. Add this as a development dependency to the project by running `npm install archiver --save-dev` in the terminal. Ensure the `package.json` and `package-lock.json` files are updated."
      },
      {
        "step 3": "Implement the logic in `create_archive.js` to compress the contents of the `./dist` directory into a zip file. The final archive should be named `project_archive.zip` and placed in the project's root directory. Hint: Use the `fs` and `archiver` modules. Your script should: 1. Define the output file path and the source directory. 2. Create a write stream for the output file. 3. Initialize the archiver with the 'zip' format. 4. Pipe the archive data to the file stream. 5. Add the entire `./dist` directory to the root of the archive using `archive.directory('dist/', false)`. 6. Finalize the archive. 7. Include console logs to indicate when the process is complete or if an error occurs."
      },
      {
        "step 4": "Execute the script you just created to generate the zip archive. Run the command `node create_archive.js` from the project's root directory in your terminal."
      },
      {
        "step 5": "Verify that the task was successful. List the files in the root directory and confirm that a file named `project_archive.zip` exists. You can use the command `ls -l project_archive.zip` to check for its presence and size."
      }
    ]
  }
}