{
  "timestamp": 1750914695.9005687,
  "goal": "# Architecture Document: Ironclad Code Generation System\n\n## 1. Introduction and Goals\n\nThe Ironclad Code Generation System is designed to automate and standardize the process of generating software module implementations based on formal architectural specifications and interface contracts. At its core, the system acts as a sophisticated build tool that leverages the capabilities of Large Language Models (LLMs) for code generation, while ensuring the output adheres strictly to predefined structural, contractual, and validation rules.\n\nThe product idea addresses a critical need in modern software development: maintaining consistency, correctness, and security across numerous microservices or modules, particularly when dealing with complex domain logic and inter-module dependencies. Manually writing boilerplate code, implementing intricate interfaces, and ensuring adherence to coding standards and validation rules is time-consuming, error-prone, and difficult to scale across large teams or projects.\n\nThe target audience for the Ironclad system includes software architects, development teams, and CI/CD engineers working on projects that require a high degree of architectural discipline, code quality consistency, and accelerated module development.\n\nThe primary problem solved is the translation of formal module specifications and interface contracts into high-quality, validated, and test-covered code implementations in a deterministic and automatable manner. This reduces manual effort, minimizes human error in implementing contracts and rules, and ensures that generated code components are inherently aligned with the system's architecture from inception.\n\nThe key objectives this architecture aims to achieve are:\n\n*   **Determinism:** Given a specific set of inputs (ARCHITECTURE_SPEC, contracts, context, validation rules, prompt templates, and a specific version/state of the IMG LLM), the system should produce the same output code and failure reports.\n*   **Correctness and Adherence:** Ensure that generated code precisely implements the defined module contracts (interfaces) and adheres to all specified validation rules (DSL checks, type safety, linter rules).\n*   **Efficiency:** Enable parallel generation of independent modules to reduce overall build time.\n*   **Resilience:** Implement retry mechanisms for transient LLM generation failures and isolate module generation tasks to prevent single-module issues from halting the entire process.\n*   **Auditability and Integrity:** Provide mechanisms (like BlueprintLock) to verify the integrity of the generated system blueprint and maintain a clear record of generation failures.\n*   **Extensibility:** Design the system such that adding new validation rules, prompt templates, or supporting different output languages is feasible.\n*   **Maintainability:** Structure the system's components logically with clear responsibilities and interfaces to facilitate understanding, modification, and debugging.\n\nBy achieving these goals, the Ironclad system serves as a robust blueprint-to-code pipeline, significantly enhancing developer productivity, code quality, and architectural compliance.\n\n## 2. High-Level Architecture Overview\n\nThe Ironclad Code Generation System operates primarily as a pipeline orchestrated by distinct processes interacting with a shared file-based repository. It is not a continuously running service but rather an on-demand tool executed within a build environment (developer machine or CI/CD pipeline).\n\nThe architecture consists of two main phases, executed sequentially:\n1.  **Blueprint Initialization Phase:** Executed by the **Ironclad Blueprint Architect (IBA)**.\n2.  **Code Generation and Validation Phase:** Exechestraed by the **Task Runner**, which spawns **Job** processes that interact with the **Ironclad Module Generator (IMG)** and **Validation Engines**.\n\nHere's a textual description of the main components and their interactions:\n\n```mermaid\ngraph TD\n    A[ARCHITECTURE_SPEC] --> B(Ironclad Blueprint Architect - IBA);\n    B --> C(Repository Skeleton);\n    C --> D(contracts/*.json);\n    C --> E(idl/*.ts);\n    C --> F(system_context.json);\n    C --> G(validation_dsl_spec.md);\n    C --> H(prompt_templates/*.tmpl);\n    C --> I(src/modules/I*.ts - Interface Stubs);\n    C --> J(Repository - Initialized State);\n\n    J --> K(Task Runner);\n    K -- Discovers modules --> D;\n    K -- Manages concurrency --> L(Concurrency Limit IRONCLAD_MAX_PARALLEL);\n    K -- Creates isolated workspaces --> M(Task Workspaces .tmp/ironclad_tasks/);\n    M -- Workspace Prep --> D, E, F, G, I;\n    M -- Construct Prompt --> N(Prompt File prompt.txt);\n    N --> O(Job - Spawned Process);\n    O -- Calls --> P(Ironclad Module Generator - IMG LLM);\n    P -- Returns JSON {code, test} --> O;\n    O -- Executes Validation --> Q(Validation Engines: tsc, DSL, Test Runner, Edge Case Check);\n    Q -- Reports failures --> R(failures.json / final_failure.json - in workspace);\n    O -- Writes generated code (if successful) --> S(Generated Code/Tests - in workspace);\n\n    K -- Awaits Job Completion --> O;\n    K -- Aggregates Results --> R, S;\n    K -- Merges successful outputs --> T(Repository - Final State with src/*.ts);\n    K -- Collates failures --> U(.ironclad_failures.json - top-level);\n    K -- Executes Global Validation --> V(Global Validation: Full tsc, Global DSL, Integration Tests);\n    V -- Reports success/failure --> W(Exit Code);\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style B fill:#ccf,stroke:#333,stroke-width:2px\n    style K fill:#ccf,stroke:#333,stroke-width:2px\n    style O fill:#ccf,stroke:#333,stroke-width:2px\n    style P fill:#ff9,stroke:#333,stroke-width:2px\n    style Q fill:#9cf,stroke:#333,stroke-width:2px\n    style T fill:#cfc,stroke:#333,stroke-width:2px\n```\n\n**Component Descriptions:**\n\n*   **ARCHITECTURE_SPEC:** The primary input file defining the overall system structure, including module names, potentially high-level descriptions, and references to contracts.\n*   **Ironclad Blueprint Architect (IBA):** A command-line tool that reads the ARCHITECTURE_SPEC and associated files (contracts, IDLs, etc.), performs initial validations (like DAG integrity), and sets up the initial repository structure with all necessary non-implementation files and interface stubs. It ensures the foundational elements are correct before code generation begins. It embeds integrity checks (BlueprintLock) in emitted files.\n*   **Repository:** The central file-based storage where all architectural definitions (contracts, IDLs), context files, templates, generated code, and reports reside. It transitions from an \"Initialized State\" (post-IBA) to a \"Final State\" (post-Task Runner with merged code).\n*   **Task Runner:** The orchestrator of the code generation phase. It runs as a main process, reads the initialized repository, identifies modules to generate, prepares isolated environments for each module, manages the spawning and monitoring of Job processes, and performs final aggregation and global validation.\n*   **Job:** A short-lived, isolated process spawned by the Task Runner for a single module. It works within its dedicated workspace, interacts with the IMG, runs the validation sequence on the generated code, handles retries, and reports its final status.\n*   **Ironclad Module Generator (IMG):** An external Large Language Model (LLM) or an API wrapper around one. Its sole function is to take a structured prompt as input and return a JSON object containing the generated implementation code and test code for a single module. The architecture views this as a black box service invoked over an API.\n*   **Task Workspaces:** Isolated directories created by the Task Runner, containing only the necessary files for a single module's generation task (its contract, dependencies' contracts/interfaces, IDLs, context files, prompt template). This isolation is crucial for preventing cross-task interference and ensuring deterministic generation per module.\n*   **Validation Engines:** A set of tools and scripts (TypeScript compiler `tsc`, custom DSL validators, test runner like Jest/Mocha) invoked by the Job process to verify the correctness and adherence of the generated code.\n*   **Generated Code/Tests:** The `*.ts` and `*.test.ts` files produced by the IMG and validated by the Job. Successfully validated files are eventually merged into the main repository.\n*   **Failure Reports:** JSON files (`failures.json`, `final_failure.json`) detailing validation errors or generation issues within a Job's workspace. These are collated by the Task Runner into a top-level `.ironclad_failures.json`.\n*   **Global Validation:** Final checks performed by the Task Runner on the complete, merged codebase (e.g., full project type-check, integration tests).\n\n**Technology Stack Choices and Justifications:**\n\n*   **Primary Language (Task Runner, IBA, Validation Logic):** TypeScript/Node.js.\n    *   *Justification:* Provides type safety for building robust tooling, leverages the vast npm ecosystem (file system operations, process management, JSON parsing, hashing, templating), excellent for CLI tools, and aligns well with generating TypeScript code. Node.js's asynchronous nature is suitable for managing concurrent Jobs.\n*   **Repository Management:** Git and File System.\n    *   *Justification:* Standard, widely adopted tools for source control and project structure. The system's state is inherently file-based (specifications, contracts, generated code). Leveraging Git simplifies versioning, collaboration, and integration into existing CI/CD workflows.\n*   **Configuration/Specification Format:** JSON, Markdown, Plain Text files (for templates).\n    *   *Justification:* JSON is standard, machine-readable, and ideal for structured data like contracts and failure reports. Markdown is suitable for human-readable specs (ARCHITECTURE_SPEC, validation DSL spec). Plain text templates are standard for prompt generation.\n*   **Code Generation Model:** Large Language Model (IMG).\n    *   *Justification:* LLMs are powerful for translating natural language instructions and structured data (from prompts) into code. This is the core innovation allowing automated code generation. Specific LLM choice is abstracted behind the IMG interface.\n*   **Validation Tools:**\n    *   `tsc` (TypeScript Compiler): Essential for static type checking, ensuring the generated code conforms to the defined interfaces (`I<ModuleName>.ts`).\n    *   Jest/Mocha (Test Runner): Standard JavaScript/TypeScript test frameworks for executing generated unit tests (`*.test.ts`).\n    *   Custom Scripts/Libraries: For DSL rule validation, edge case string checks, JSON schema validation. Implementations will depend on the specific DSL format chosen.\n    *   Linters (e.g., ESLint, Prettier): Can be integrated into validation steps to enforce coding style and detect common errors.\n    *   Security Linters (e.g., SonarQube scanner, Semgrep): Can be added for basic static analysis security checks on generated code.\n*   **Process Management:** Node.js `child_process` module or similar.\n    *   *Justification:* Native capabilities in Node.js for spawning independent processes (`fork` or `spawn`) are required to implement the Job isolation and concurrency model.\n*   **Hashing:** Standard cryptographic library (e.g., Node.js `crypto`).\n    *   *Justification:* Required for generating SHA-256 hashes for the BlueprintLock integrity check and prompt hash verification.\n\nThis architecture emphasizes using standard tools and file-based workflows to keep the system itself relatively simple, robust, and integrable, while abstracting the complex, non-deterministic part (LLM generation) behind a well-defined interface and surrounding it with deterministic validation and retry logic.\n\n## 3. Detailed Component Design\n\nThis section details the design and responsibilities of the primary components within the Ironclad system.\n\n### 3.1. Ironclad Blueprint Architect (IBA)\n\n*   **Responsibilities:**\n    *   Read the master `ARCHITECTURE_SPEC` file.\n    *   Parse and validate all individual module contract files (`contracts/*.json`).\n    *   Construct and validate the module dependency graph (from `contracts/*.json`) to ensure it is a Directed Acyclic Graph (DAG). Reject input if cycles are detected.\n    *   Parse and validate the `system_context.json` and `validation_dsl_spec.md` files.\n    *   Validate prompt template files (`prompt_templates/*.tmpl`).\n    *   Scaffold the initial repository directory structure (e.g., `contracts/`, `idl/`, `src/modules/`, `prompt_templates/`, `.tmp/`, etc.).\n    *   Generate Interface Definition Language (IDL) files (`idl/*.ts`) based on specifications (not explicitly detailed in spec but a logical precursor to `I*.ts`). *Self-correction: The spec only mentions `idl/` directory and `src/modules/I*.ts` stubs. I'll focus on generating the `I*.ts` stubs based on contracts, assuming the contract JSON implicitly or explicitly defines the interface surface.*\n    *   Copy validated contract, context, DSL spec, and template files to their canonical locations in the repository.\n    *   Generate module interface stub files (`src/modules/I<ModuleName>.ts`) based on the module contracts.\n    *   Embed a `BlueprintLock: sha256:<hash>` header in *every* file it writes or copies into the repository (contracts, IDLs, interface stubs, context, DSL spec, templates). This hash is calculated over the file's content *after* adding the header line itself (a standard practice requiring careful handling of the header calculation).\n    *   Ensure the initial state of `src/modules/` contains only the generated interface stubs (`I*.ts`) and no implementation files (`*.ts`).\n*   **APIs (Internal Contracts):**\n    *   `validateContracts(contractFiles: string[]): Map<string, ModuleContract>`: Reads and validates JSON schema/content of contract files, returns a map of module name to parsed contract object.\n    *   `buildDependencyGraph(contracts: Map<string, ModuleContract>): DependencyGraph`: Constructs graph data structure from contract dependencies.\n    *   `validateDAG(graph: DependencyGraph): boolean`: Checks for cycles in the dependency graph. Throws error if cycle detected.\n    *   `generateInterfaceStubs(contracts: Map<string, ModuleContract>, outputPath: string): Map<string, string>`: Generates TypeScript interface code strings based on contracts.\n    *   `writeFilesWithBlueprintLock(files: Map<string, string>, baseDir: string): void`: Writes file content to disk, adding the BlueprintLock header and calculating the correct hash.\n    *   `scaffoldDirectories(baseDir: string): void`: Creates necessary empty directories.\n*   **Data Flows:** Reads source files (ARCHITECTURE_SPEC, initial contracts, context, DSL, templates) from an input directory. Writes structured files (contracts, IDLs, interfaces, context, DSL, templates) into the designated repository directory.\n*   **Key Algorithms:**\n    *   **DAG Cycle Detection:** Standard graph traversal algorithms like Depth First Search (DFS) with state tracking (visiting, visited) can detect cycles efficiently.\n    *   **BlueprintLock Hashing:** Requires a specific procedure: read file content *without* the header, calculate hash, format header line, prepend header to content, calculate final hash of *total* content, replace header's placeholder hash with the final hash, write file.\n*   **Design Patterns:**\n    *   **Builder:** The IBA acts as a builder, constructing the initial repository structure step-by-step.\n    *   **Validator:** Encapsulates validation logic for contracts, DAG, etc.\n\n### 3.2. Task Runner (Main Process)\n\n*   **Responsibilities:**\n    *   Discover all module names by enumerating `contracts/*.json` files in the initialized repository.\n    *   Read the concurrency limit from the `IRONCLAD_MAX_PARALLEL` environment variable (defaulting to 1).\n    *   Manage a pool of active Job processes, respecting the concurrency limit.\n    *   For each module:\n        *   Create an isolated workspace directory (`.tmp/ironclad_tasks/<ModuleName>/`).\n        *   Copy or symlink required files into the workspace (module's contract, dependency contracts, IDLs, context, DSL spec, module's interface stub, dependency interface stubs). Use symlinks where possible to save space and time, ensuring the Job reads the canonical source files (besides its own temporary outputs).\n        *   Construct the `prompt.txt` file within the workspace by populating `prompt_templates/module_prompt.tmpl` with data from the workspace files.\n        *   Calculate and save the SHA-256 hash of the generated `prompt.txt` to `prompt.hash` in the workspace.\n        *   Spawn a new Job process within this isolated workspace.\n    *   Monitor spawned Job processes, waiting for their termination.\n    *   Aggregate results upon Job completion:\n        *   If a Job succeeds (no `final_failure.json` and generated files exist), copy the validated `src/modules/<ModuleName>.ts` and `src/modules/__tests__/<ModuleName>.test.ts` from the workspace to their canonical paths in the main repository.\n        *   If a Job fails (`final_failure.json` exists), record the path to this file.\n    *   After all Jobs complete, collate all individual `final_failure.json` reports into a single top-level `.ironclad_failures.json` file.\n    *   Execute global validation steps on the fully merged repository (full `tsc --noEmit`, global DSL validators, run integration tests).\n    *   Determine the final exit status: non-zero if `.ironclad_failures.json` exists or global validation fails; zero otherwise.\n*   **APIs (Internal Contracts):**\n    *   `discoverModules(contractsDir: string): string[]`: Finds all module names.\n    *   `prepareWorkspace(moduleName: string, repoDir: string, workspaceBaseDir: string, contracts: Map<string, ModuleContract>): string`: Creates and populates the workspace directory, returns its path.\n    *   `constructPrompt(workspaceDir: string, templatePath: string): void`: Generates and writes `prompt.txt` and `prompt.hash`.\n    *   `spawnJob(workspaceDir: string, jobScriptPath: string): ChildProcess`: Creates and starts a new process for a Job.\n    *   `awaitJobs(jobs: ChildProcess[]): Promise<JobResult[]>`: Waits for all job processes to exit and collects their status.\n    *   `mergeOutputs(jobResults: JobResult[], repoDir: string, workspaceBaseDir: string): void`: Copies successful outputs and collates failure report paths.\n    *   `collateFailureReports(failureReportPaths: string[], repoDir: string): void`: Combines individual failure reports.\n    *   `runGlobalValidation(repoDir: string): boolean`: Executes final validation checks.\n*   **Data Flows:** Reads contract and support files from the main repository. Writes task-specific files into workspace directories. Reads results from workspace directories. Writes generated code and consolidated failure reports back to the main repository.\n*   **Key Algorithms:**\n    *   **Concurrency Management:** Using a queue and tracking active job count to limit parallel execution based on `IRONCLAD_MAX_PARALLEL`.\n    *   **File/Directory Management:** Efficiently copying or symlinking files into workspaces.\n*   **Design Patterns:**\n    *   **Orchestrator:** The Task Runner coordinates the execution of multiple, independent Job tasks.\n    *   **Worker Pool:** Manages a pool of Job processes to control concurrency.\n\n### 3.3. Job (Worker Process)\n\n*   **Responsibilities:**\n    *   Operate entirely within its dedicated workspace directory (`.tmp/ironclad_tasks/<ModuleName>/`).\n    *   Read the `prompt.txt` and `prompt.hash` files in the workspace. Verify the hash matches the content.\n    *   Manage the retry loop for IMG invocation and validation (maximum 3 attempts).\n    *   Invoke the IMG (external LLM API call) with the current `prompt.txt`.\n    *   Receive the JSON response from the IMG (`{ implementationCode: string, testCode: string }`).\n    *   Execute the detailed validation sequence on the generated code:\n        *   **JSON Schema Validation:** Check if the IMG response is valid JSON matching the expected structure.\n        *   **TypeScript Compilation:** Write `implementationCode` to `<ModuleName>.ts` and run `tsc --noEmit` targeting this file and the corresponding `I<ModuleName>.ts` (present in the workspace), ensuring type compatibility.\n        *   **DSL Rule Validation:** Run configured validation engines (`validation_dsl_spec.md`) against the generated code (`<ModuleName>.ts`). This might involve parsing the code (AST) and applying rules defined in the DSL spec.\n        *   **Edge Case String Check:** If the module's contract (`contracts/<ModuleName>.json`) has an `instructions.edgeCases` array, check if the generated `testCode` string contains *literal strings* matching each entry in this array. This is a simple heuristic to encourage the LLM to generate tests covering specific scenarios requested in the contract.\n        *   **Test Execution:** Write `testCode` to `__tests__/<ModuleName>.test.ts` and run the test runner (e.g., `jest __tests__/<ModuleName>.test.ts`) in isolation.\n    *   If any validation step fails:\n        *   Append details of the failure (validation type, errors) to a `failures.json` file within the workspace.\n        *   Increment the attempt counter.\n        *   Amend the `prompt.txt` file by appending a deterministic error message section detailing the *specific* validation failures encountered.\n        *   If attempts < 3, loop back to invoke IMG with the amended prompt.\n        *   If attempts = 3 and validation still fails, write a `final_failure.json` file containing the accumulated failures and terminate.\n    *   If all validation steps pass on any attempt:\n        *   Write the validated `implementationCode` to `<ModuleName>.ts` and `testCode` to `__tests__/<ModuleName>.test.ts` within the workspace.\n        *   Terminate successfully (exit code 0).\n*   **APIs (Internal/External Interactions):**\n    *   `callIMG(prompt: string): Promise<{ implementationCode: string, testCode: string }>`: Makes the API call to the IMG. Handles API key management securely (e.g., reading from environment).\n    *   `runValidation(code: string, testCode: string, moduleName: string, workspaceDir: string, contract: ModuleContract): Promise<ValidationResult[]>`: Orchestrates the sequence of validation checks.\n*   **Data Flows:** Reads configuration and prompt data from its workspace. Sends prompt data to the IMG API. Receives code/test data from the IMG API. Writes temporary code/test files and failure reports within its workspace.\n*   **Key Algorithms:**\n    *   **Retry Logic:** Implementing the fixed 3-attempt loop with prompt amendment.\n    *   **Validation Orchestration:** Sequencing and interpreting results from multiple external validation tools/scripts.\n    *   **Prompt Amendment:** Deterministically adding failure context to the prompt for retries.\n*   **Design Patterns:**\n    *   **State Machine:** The Job follows a state machine: Start -> Call IMG -> Validate -> (Success -> Exit) or (Failure -> Append Failures -> Amend Prompt -> (Attempts < 3 -> Call IMG) or (Attempts = 3 -> Write Final Failure -> Exit)).\n    *   **Strategy:** The validation sequence can be seen as applying multiple validation strategies.\n\n### 3.4. Ironclad Module Generator (IMG)\n\n*   **Responsibilities:**\n    *   Accept a text prompt as input.\n    *   Utilize an underlying LLM to generate two distinct code strings based on the prompt: one for the module implementation and one for its unit tests.\n    *   Return the generated code strings packaged in a specific JSON format: `{ \"implementationCode\": \"...\", \"testCode\": \"...\" }`.\n*   **APIs (External Interface):**\n    *   An HTTP POST endpoint (or SDK call) expecting a JSON payload like `{ \"prompt\": \"...\" }`.\n    *   Returns a JSON payload like `{ \"implementationCode\": \"string\", \"testCode\": \"string\" }` upon success (HTTP 200).\n    *   Returns appropriate HTTP error codes (e.g., 400 for bad request, 401/403 for auth, 429 for rate limit, 500 for internal errors).\n*   **Data Flows:** Receives text prompt. Returns JSON object containing two string fields.\n*   **Key Algorithms:** Relies on the underlying LLM's internal architecture and training data. The system *calling* the IMG does not control this, only the prompt input.\n*   **Design Patterns:**\n    *   **Facade:** The IMG API acts as a facade over the potentially complex interaction with the raw LLM.\n\n### 3.5. Validation Engines\n\n*   **Responsibilities:**\n    *   Execute specific checks on generated code/tests.\n    *   Each engine focuses on a single type of validation (type checking, DSL rules, test execution, string matching).\n    *   Report success or detailed failure information (line numbers, error messages) in a machine-readable format.\n*   **APIs (Internal Interface - invoked by Job):**\n    *   Scripts or library functions callable from the Job process.\n    *   E.g., `runTsc(filePath: string, interfacePath: string): Promise<ValidationResult>`\n    *   E.g., `runDslValidation(code: string, rulesSpec: string): Promise<ValidationResult[]>`\n    *   E.g., `runTests(testFilePath: string): Promise<ValidationResult>`\n    *   E.g., `checkEdgeCases(testCode: string, edgeCases: string[]): Promise<ValidationResult[]>`\n*   **Data Flows:** Read generated code/test files and relevant specification files (interfaces, DSL spec, contract). Produce validation result objects.\n*   **Key Algorithms:** Depends on the specific engine (e.g., AST parsing for DSL, compiler logic for tsc, test runner logic for Jest).\n*   **Design Patterns:**\n    *   **Strategy:** Each validation engine implements a specific validation strategy.\n\n## 4. Data Model and Database Design\n\nThe Ironclad Code Generation System is designed to be largely stateless between runs, with its entire \"state\" residing within the file system of the repository and temporary workspaces. It does *not* rely on a traditional relational database (like PostgreSQL, MySQL) or a NoSQL database (like MongoDB, Cassandra) for its core operation or to persist long-term state about generation tasks or modules.\n\nThe file system itself serves as the de facto \"database\" or data store for the system's inputs, intermediate artifacts, and outputs.\n\n**Core Data Entities (File Types):**\n\n1.  **ARCHITECTURE_SPEC:**\n    *   **Purpose:** Defines the overall system, lists modules, provides high-level context.\n    *   **Format:** Markdown (`.md`).\n    *   **Attributes:** System name, description, list of modules, potentially high-level relationships.\n    *   **Relationships:** Implicitly links to all module contracts listed within it.\n2.  **Module Contracts:**\n    *   **Purpose:** Formally define the interface, dependencies, and generation instructions for a single module.\n    *   **Format:** JSON (`contracts/<ModuleName>.json`).\n    *   **Attributes:**\n        *   `moduleName` (string): Unique identifier.\n        *   `description` (string): Human-readable purpose.\n        *   `interfaceDefinition` (object): Structure defining input/output types (can reference types in `idl/`).\n        *   `dependencies` (string[]): List of other `ModuleName`s this module depends on (used for DAG and workspace prep).\n        *   `instructions` (object): Specific guidance for the IMG.\n            *   `functionalDescription` (string): Detailed behavior description.\n            *   `edgeCases` (string[]): List of specific scenarios the tests *must* mention as string literals.\n            *   `generationHints` (string): Any specific coding style, library preference hints for the LLM.\n    *   **Relationships:** Directed graph structure defined by the `dependencies` array.\n3.  **IDL Files:**\n    *   **Purpose:** Define shared data structures (types, enums, interfaces) used across multiple module contracts and implementations.\n    *   **Format:** TypeScript (`idl/*.ts`).\n    *   **Attributes:** Type definitions using TypeScript syntax.\n    *   **Relationships:** Referenced by `interfaceDefinition` within Module Contracts and used by generated code.\n4.  **System Context:**\n    *   **Purpose:** Provides global context to the IMG, such as common utility libraries, architectural principles, or domain-specific jargon definitions.\n    *   **Format:** JSON (`system_context.json`).\n    *   **Attributes:** Arbitrary key-value pairs or structured data relevant to all modules.\n5.  **Validation DSL Specification:**\n    *   **Purpose:** Defines the rules that generated code must satisfy beyond type-checking and test execution.\n    *   **Format:** Markdown (`validation_dsl_spec.md`).\n    *   **Attributes:** Description of the DSL syntax and the specific rules to be enforced (e.g., \"No 'any' types allowed\", \"All public functions must have JSDoc\", \"Specific forbidden library calls\").\n6.  **Prompt Templates:**\n    *   **Purpose:** Define the structure and phrasing used to construct the prompt sent to the IMG.\n    *   **Format:** Plain Text (`prompt_templates/*.tmpl`).\n    *   **Attributes:** Template syntax (e.g., Handlebars, Jinja) referencing variables populated from contracts, IDLs, context, and DSL spec.\n7.  **Interface Stubs:**\n    *   **Purpose:** TypeScript interface files (`src/modules/I<ModuleName>.ts`) generated by the IBA, representing the contract surface of a module. Used by `tsc` validation to ensure generated implementation code adheres to the interface.\n    *   **Format:** TypeScript (`src/modules/I*.ts`).\n    *   **Attributes:** TypeScript interface definitions.\n    *   **Relationships:** Corresponds to a Module Contract; referenced by the generated implementation code and its dependents.\n8.  **Generated Implementation Code & Tests:**\n    *   **Purpose:** The actual source code and unit tests produced by the IMG and validated by the Job.\n    *   **Format:** TypeScript (`src/modules/<ModuleName>.ts`, `src/modules/__tests__/<ModuleName>.test.ts`).\n    *   **Attributes:** Valid TypeScript code.\n    *   **Relationships:** The implementation file must implement its corresponding interface stub (`I<ModuleName>.ts`); tests exercise the implementation.\n9.  **Failure Reports:**\n    *   **Purpose:** Record detailed information about validation failures for individual Jobs and the final aggregated status.\n    *   **Format:** JSON (`.tmp/ironclad_tasks/<ModuleName>/failures.json`, `.tmp/ironclad_tasks/<ModuleName>/final_failure.json`, `.ironclad_failures.json`).\n    *   **Attributes:** Array of failure entries, each including:\n        *   `attempt` (number)\n        *   `timestamp` (string)\n        *   `validator` (string: \"json-schema\", \"tsc\", \"dsl\", \"edge-case\", \"test-runner\")\n        *   `message` (string): Error description.\n        *   `details` (object): Specific error details (e.g., compiler errors with line/column, test runner output).\n    *   **Relationships:** `final_failure.json` aggregates entries from `failures.json` for a single job. `.ironclad_failures.json` aggregates `final_failure.json` from all failed jobs.\n\n**File System Structure as the \"Schema\":**\n\nThe directory structure imposes a logical schema:\n*   `contracts/`: Module contracts.\n*   `idl/`: Shared interface definitions.\n*   `system_context.json`: Global context.\n*   `validation_dsl_spec.md`: Validation rules.\n*   `prompt_templates/`: Templates for IMG prompts.\n*   `src/modules/`: Contains `I*.ts` stubs (generated by IBA) and ultimately `*.ts` implementation files (generated by IMG/Task Runner).\n*   `src/modules/__tests__/`: Contains `*.test.ts` files (generated by IMG/Task Runner).\n*   `.tmp/ironclad_tasks/`: Temporary workspaces for Jobs. Each subdirectory `<ModuleName>/` contains task-specific files:\n    *   `contracts/<ModuleName>.json` (link/copy)\n    *   `contracts/<DependencyName>.json` (links/copies)\n    *   `idl/` (link/copy)\n    *   `system_context.json` (link/copy)\n    *   `validation_dsl_spec.md` (link/copy)\n    *   `src/modules/I<ModuleName>.ts` (link/copy)\n    *   `src/modules/I<DependencyName>.ts` (links/copies)\n    *   `prompt_templates/module_prompt.tmpl` (link/copy)\n    *   `prompt.txt` (generated by Task Runner)\n    *   `prompt.hash` (generated by Task Runner)\n    *   `task.pid` (written by Job)\n    *   `failures.json` (written by Job during retries)\n    *   `final_failure.json` (written by Job on permanent failure)\n    *   `<ModuleName>.ts` (written by Job, temp)\n    *   `__tests__/<ModuleName>.test.ts` (written by Job, temp)\n*   `.ironclad_failures.json`: Aggregated failure report (written by Task Runner).\n\n**Database Technology Rationale:**\n\nGiven that the system's data is entirely configuration, specification, code, and logs/reports, all managed within the file system and version control (Git), a dedicated database system (SQL or NoSQL) is unnecessary and would introduce undue complexity, overhead, and external dependencies. The file system provides sufficient structure, persistence, and access control for this use case. The requirements for the system's internal state (primarily the collection of files at different stages) are well-suited to file-based storage. CAP theorem considerations are not relevant here, as the system operates on local files within a single execution context, not a distributed data store. Consistency is ensured by the deterministic pipeline and validation steps operating on immutable file inputs for each stage.\n\n**Data Management Strategies:**\n\n*   **Versioning:** Handled entirely by Git for all source files (contracts, IDLs, specs, templates) and generated code.\n*   **Immutability:** Input files for the Task Runner (generated by IBA) are treated as immutable during the generation process. The `BlueprintLock` header reinforces this for IBA outputs.\n*   **Isolation:** Workspaces provide data isolation for individual generation Jobs, preventing interference.\n*   **Cleanup:** Temporary workspace directories (`.tmp/ironclad_tasks/`) should be cleaned up after the Task Runner completes, regardless of success or failure, to conserve disk space.\n\n## 5. API Design and Endpoints (Public/External)\n\nThe Ironclad Code Generation System is primarily a command-line interface (CLI) tool designed to be integrated into development workflows and CI/CD pipelines. It does not expose a traditional web-based REST API for its core generation process. Its public interface is the command-line entry point and the structure/content of the files it reads and writes.\n\nHowever, it *interacts* with an external API for the Ironclad Module Generator (IMG). This interaction is an internal detail of the Job process, but the specification of the IMG API endpoint is crucial for understanding the system's external dependencies.\n\n**Public Interface (CLI):**\n\nThe system is invoked via a command-line command, potentially with subcommands for the different phases.\n\n*   **Command Structure:**\n    ```bash\n    ironclad <command> [options]\n    ```\n*   **IBA Command:**\n    ```bash\n    ironclad blueprint build <architecture-spec-path> --output <repository-path>\n    ```\n    *   **Purpose:** Executes the Ironclad Blueprint Architect phase.\n    *   **Inputs:**\n        *   `<architecture-spec-path>` (string, required): Path to the main `ARCHITECTURE_SPEC.md` file.\n        *   `--output <repository-path>` (string, required): Directory where the repository skeleton will be created/initialized.\n        *   `--contracts-dir <path>` (string, optional): Directory containing initial module contracts (default: relative to spec path).\n        *   `--idls-dir <path>` (string, optional): Directory containing initial IDL files (default: relative to spec path).\n        *   `--context-file <path>` (string, optional): Path to initial `system_context.json` (default: relative to spec path).\n        *   `--dsl-file <path>` (string, optional): Path to initial `validation_dsl_spec.md` (default: relative to spec path).\n        *   `--template-dir <path>` (string, optional): Directory containing initial prompt templates (default: relative to spec path).\n    *   **Outputs:**\n        *   Creates/populates the directory structure at `<repository-path>` with all non-implementation files including BlueprintLock headers.\n        *   Standard output/error stream for progress, validation errors (especially DAG cycles).\n        *   Exit code 0 on success, non-zero on failure.\n*   **Task Runner Command:**\n    ```bash\n    ironclad generate run <repository-path>\n    ```\n    *   **Purpose:** Executes the Task Runner code generation and validation phase.\n    *   **Inputs:**\n        *   `<repository-path>` (string, required): Path to the repository directory initialized by the IBA.\n        *   Environment Variable: `IRONCLAD_MAX_PARALLEL` (integer, optional): Maximum number of concurrent Jobs (default: 1).\n        *   Environment Variable: `IMG_API_KEY` (string, required for IMG interaction): API key for authentication with the IMG endpoint.\n        *   Environment Variable: `IMG_API_URL` (string, required for IMG interaction): URL of the IMG endpoint.\n    *   **Outputs:**\n        *   Modifies files within `<repository-path>` by adding successfully generated `src/modules/*.ts` and `src/modules/__tests__/*.test.ts`.\n        *   Writes `.ironclad_failures.json` at the top level of `<repository-path>` if any Job or global validation failed.\n        *   Standard output/error stream for Task Runner progress, Job status updates, and global validation results.\n        *   Exit code 0 on success (all modules generated/validated, global validation passed), non-zero on failure.\n\n**External API (IMG Interaction - Internal to System):**\n\nThe Job process interacts with the IMG, which is treated as an external service. The specific API contract is as follows:\n\n*   **Endpoint:** Configurable via `IMG_API_URL` environment variable. Assumed to be a single POST endpoint, e.g., `POST /generate`.\n*   **HTTP Method:** `POST`.\n*   **Request Format (JSON):**\n    ```json\n    {\n      \"prompt\": \"string\"\n    }\n    ```\n    *   `prompt`: The detailed text prompt generated by the Task Runner for a specific module, containing instructions, context, interfaces, etc.\n*   **Request Headers:**\n    *   `Content-Type: application/json`\n    *   `Authorization: Bearer <IMG_API_KEY>` (Example using bearer token, actual scheme depends on IMG provider, could be custom header `X-IMG-API-Key`). The API key is read from `IMG_API_KEY` environment variable by the Job.\n*   **Response Format (JSON):**\n    ```json\n    {\n      \"implementationCode\": \"string\",\n      \"testCode\": \"string\"\n    }\n    ```\n    *   `implementationCode`: The generated TypeScript source code for the module implementation.\n    *   `testCode`: The generated TypeScript source code for the module's unit tests.\n*   **HTTP Status Codes:**\n    *   `200 OK`: Successfully generated code. Response body contains the JSON payload.\n    *   `400 Bad Request`: Prompt was invalid or malformed.\n    *   `401 Unauthorized`: Invalid or missing API key.\n    *   `403 Forbidden`: API key lacks necessary permissions.\n    *   `429 Too Many Requests`: Rate limit exceeded.\n    *   `500 Internal Server Error`: An error occurred within the IMG service (e.g., LLM failure).\n    *   Other 4xx/5xx codes for specific errors.\n*   **Authentication:** Token-based authentication using an API key passed via an HTTP header.\n*   **Versioning:** Assumed to be handled by the IMG service itself (e.g., endpoint path `/v1/generate`). The Task Runner would be configured with the specific versioned URL via `IMG_API_URL`. The system's reliance on prompt structure implies a strong coupling to the specific IMG model/version it was developed against. Changes to the IMG might require updates to prompt templates and validation logic.\n*   **Rate Limiting:** Expected to be enforced by the IMG provider. The Task Runner manages *local* concurrency (`IRONCLAD_MAX_PARALLEL`), but the IMG provider might have its own limits. The Job process should ideally implement exponential backoff and retry for 429 errors if the IMG API supports it, though the current spec only mentions retries for generation/validation failures (which *might* stem from IMG errors). The Job's 3-attempt limit implicitly handles persistent IMG failures.\n\nThis design clearly separates the public CLI interface for system users from the internal API interaction with the external IMG service.\n\n## 6. User Interface (UI) and User Experience (UX) Design\n\nThe Ironclad Code Generation System operates primarily as a backend tool without a dedicated graphical user interface (GUI). Its \"UI\" is the command line interface (CLI) and the file system structure of the repository. The \"UX\" is centered around ease of command execution, clarity of output (console and files), predictability of file locations, and the overall developer workflow integration.\n\n**User Personas:**\n\n1.  **Architect/System Designer:**\n    *   **Goals:** Define the system structure, module boundaries, contracts, and validation rules using ARCHITECTURE_SPEC, contracts, IDLs, system context, and DSL spec files. Ensure architectural compliance and consistency across modules.\n    *   **Interaction:** Primarily interacts with the system by creating and modifying input files in the repository, running the `ironclad blueprint build` command, and reviewing the initial repository structure generated by the IBA.\n2.  **Module Developer:**\n    *   **Goals:** Focus on implementing specific domain logic, potentially writing manual code where LLM generation is insufficient. Review generated code for correctness, efficiency, and adherence to requirements. Debug validation failures reported by the system.\n    *   **Interaction:** Creates/modifies module contracts (`contracts/*.json`) and related IDLs (`idl/*.ts`). Runs the `ironclad generate run` command (often implicitly via CI). Reviews the generated `src/*.ts` and `src/__tests__/*.test.ts` files. Reads the `.ironclad_failures.json` report to understand why a module failed generation/validation and potentially debug/amend the prompt or contracts.\n3.  **CI/CD Engineer:**\n    *   **Goals:** Integrate the Ironclad system into the automated build pipeline. Ensure successful execution, monitor for failures, and report status. Manage environment configuration (like `IRONCLAD_MAX_PARALLEL`, `IMG_API_KEY`).\n    *   **Interaction:** Configures build scripts to run `ironclad blueprint build` followed by `ironclad generate run` on relevant commits. Monitors standard output/error, checks the process exit code, and makes the `.ironclad_failures.json` report available in the CI dashboard.\n4.  **Validation Rule Author:**\n    *   **Goals:** Define and refine validation rules in the `validation_dsl_spec.md` to enforce coding standards, security policies, or architectural constraints.\n    *   **Interaction:** Creates/modifies `validation_dsl_spec.md`. Relies on the system's validation engines to correctly interpret and apply these rules during the `ironclad generate run` phase.\n\n**User Flows/Journeys:**\n\n1.  **Initial System Setup:**\n    *   Architect defines initial ARCHITECTURE_SPEC, core contracts, IDLs, context, DSL spec, prompt templates.\n    *   Architect runs `ironclad blueprint build <spec> --output <repo>`.\n    *   System validates inputs (e.g., DAG).\n    *   System scaffolds repository, generates interface stubs, adds BlueprintLocks.\n    *   Architect reviews generated repository structure and interface stubs.\n2.  **Generating Initial Modules:**\n    *   Architect/Developer ensures contracts are complete.\n    *   CI/CD pipeline or Developer runs `ironclad generate run <repo>` (with `IRONCLAD_MAX_PARALLEL` set).\n    *   Task Runner discovers modules, prepares workspaces, spawns Jobs.\n    *   Jobs invoke IMG, run validation loop.\n    *   Task Runner aggregates results.\n    *   If successful, generated code is merged. CI/CD reports success.\n    *   If failures, Task Runner writes `.ironclad_failures.json`. CI/CD reports failure.\n    *   Developer reviews `.ironclad_failures.json` to identify failed modules and reasons.\n3.  **Iterating on Failed Modules:**\n    *   Developer reads `final_failure.json` (collated in `.ironclad_failures.json`).\n    *   Developer modifies the contract (`contracts/<ModuleName>.json`) or potentially the `system_context.json` or `validation_dsl_spec.md` to guide the LLM or adjust rules. *Note: Directly editing the prompt.txt or generated code is against the system's philosophy; inputs (contracts, context, rules, templates) should be modified to influence output.*\n    *   Developer commits changes.\n    *   CI/CD pipeline runs `ironclad blueprint build` (to regenerate interface stubs if contracts changed, and update BlueprintLocks) and then `ironclad generate run`.\n    *   Repeat until module generation and validation pass.\n4.  **Adding New Modules or Features:**\n    *   Architect defines new module contract(s) and updates ARCHITECTURE_SPEC. Adds/updates IDLs if needed.\n    *   Developer commits changes.\n    *   CI/CD runs the full pipeline.\n\n**Conceptual Wireframes/Mockups (Described):**\n\n*   **CLI Output:**\n    *   Progress indicators (e.g., `[IBA] Validating contracts... OK`, `[IBA] Scaffolding repository... OK`).\n    *   Task Runner startup: `Discovering modules... Found 15 modules.`\n    *   Job spawning: `[Task Runner] Spawning job for UserService (PID 12345)`\n    *   Job status updates (if implemented, e.g., via logging): `[Job UserService] Attempt 1: Calling IMG...`, `[Job UserService] Attempt 1: Validation failed (tsc error). Retrying...`, `[Job ProductService] Attempt 1: Validation successful. Merging output.`\n    *   Task Runner aggregation: `[Task Runner] All jobs completed. 12 successful, 3 failed.`\n    *   Global validation: `[Task Runner] Running global validation (full tsc)... OK.`, `[Task Runner] Running global validation (integration tests)... FAILED.`\n    *   Final summary: `BUILD FAILED. See .ironclad_failures.json for details.` or `BUILD SUCCESSFUL.`.\n*   **`.ironclad_failures.json` Structure:** A clear, well-formatted JSON array, easy for machines to parse and humans to read, listing failures grouped by module, attempt, and validation step, including error messages and potentially code snippets or line numbers.\n\n**UI Technology Choices:**\n\n*   **CLI Framework:** A Node.js CLI framework (e.g., Commander, Oclif, Yargs) to handle command parsing, options, help messages, and colored output.\n*   **Output Formatting:** Standard console logging (`console.log`, `console.error`) for basic progress and status. Libraries for structured logging and potentially richer console output (e.g., spinners, progress bars for Job execution) could enhance UX. JSON formatting libraries for `.ironclad_failures.json`.\n\n**State Management:**\n\nThe system is largely stateless between command invocations. The \"state\" is the file system contents. Within a single `ironclad generate run` execution, the Task Runner manages the state of the generation process:\n*   List of modules to process.\n*   Queue of modules awaiting generation.\n*   List of active Jobs.\n*   Tracking of completed Jobs (success/failure).\n*   Accumulation of failure report paths.\n\nThis state is managed in memory by the Task Runner process and discarded upon completion. Job processes manage their own state within their retry loop (attempt counter, accumulated failures) using in-memory variables and the `failures.json` file in their workspace.\n\n**Usability, Accessibility, Responsive Design:**\n\n*   **Usability:** Focused on clarity of CLI commands, predictable file inputs/outputs, informative console messages, and a well-structured failure report file that facilitates debugging. The isolation of jobs makes debugging individual module failures easier (inspecting the workspace).\n*   **Accessibility:** As a CLI tool, traditional accessibility concerns (screen reader compatibility with graphical elements) are not applicable. Output is text-based, suitable for standard terminal access. Ensuring clear, concise language in messages and documentation is key for cognitive accessibility.\n*   **Responsive Design:** Not applicable to a CLI tool. Performance considerations relate to execution time and resource usage, not adapting to different screen sizes.\n\n## 7. Authentication and Authorization Strategy\n\nThe Ironclad Code Generation System's authentication and authorization needs are relatively simple compared to a multi-user, web-facing application. The system primarily operates within a controlled environment (developer machine or CI/CD runner) and its security model is built around file system permissions and secure handling of credentials for external services.\n\n**Authentication:**\n\n*   **System Users (Developers, CI/CD):** The system itself does not implement user authentication. Access to run the `ironclad` commands and modify the input/output files is controlled by the operating system's file permissions and the version control system (Git) access controls. Users are implicitly authenticated by their ability to log in to the machine or CI environment and interact with the repository files.\n*   **IMG API:** The *most critical* authentication requirement is for the Job processes to securely authenticate with the external Ironclad Module Generator (IMG) API.\n    *   **Mechanism:** The IMG API is expected to use an API key or token-based authentication. The specific scheme (e.g., Bearer token in `Authorization` header, custom header like `X-IMG-API-Key`) depends on the IMG provider.\n    *   **Credential Management:** The IMG API key (`IMG_API_KEY`) MUST NOT be hardcoded in source code or configuration files checked into version control. It MUST be provided to the `ironclad generate run` process securely, typically via environment variables or a secret management system integrated into the CI/CD environment (e.g., HashiCorp Vault, AWS Secrets Manager, Kubernetes Secrets, GitHub Actions Secrets, GitLab CI/CD Variables).\n    *   **Implementation:** The Job process, when making the `callIMG` API request, will read the `IMG_API_KEY` environment variable (or fetch it from a configured secret source) and include it in the appropriate request header.\n\n**Authorization:**\n\n*   **System Users:** Authorization is managed by the underlying OS and Git permissions. Users who have write access to the repository can run the tools and modify files. Users with read access can run the tools and view inputs/outputs but not modify them in the repository.\n*   **IMG API:** The IMG API key/token should ideally have the *least privilege* necessary. Its *only* authorized action should be to invoke the code generation endpoint (`/generate`). It should not have permissions to access other services or data within the IMG provider's infrastructure. This principle of least privilege limits the blast radius if a key is compromised. The IMG service itself is responsible for enforcing this authorization based on the provided key.\n*   **Internal System Authorization:** Within the Ironclad system processes (IBA, Task Runner, Job), authorization is implicit based on the process's identity and the file system permissions of the user running the command.\n    *   The Task Runner and Jobs need read/write access to the `.tmp/ironclad_tasks/` directory and read/write access to the main repository directory (`<repository-path>`) to read inputs, write outputs, and merge results.\n    *   The IBA needs write access to create/initialize the repository directory.\n\n**Identity Provider Integration:**\n\nNo external identity provider (like Okta, Auth0, Active Directory Federation Services) is required for the Ironclad system itself, as it doesn't manage user identities or logins. The system relies on the identity context provided by the execution environment (the user running the command, the CI/CD runner's identity).\n\n**Permissions Enforcement:**\n\n*   **File System:** The operating system enforces read/write permissions on files and directories.\n*   **Git:** Git hosting platforms enforce push/pull permissions on the repository.\n*   **IMG API:** The IMG provider enforces authorization based on the API key presented with each request.\n*   **Within Ironclad:** The system's components rely on each other's outputs being available in the file system and trust that they are operating under sufficient permissions granted by the execution environment. There is no internal permission layer between the IBA, Task Runner, and Job processes beyond the file system.\n\n**Secure Credential Handling:**\n\n*   API keys MUST NOT be stored directly in configuration files or the repository.\n*   Environment variables are the minimal acceptable mechanism for passing secrets in automated environments.\n*   Integration with dedicated secret management systems is the recommended best practice for production CI/CD pipelines.\n*   The Job process should load the key from the environment variable *just before* making the API call and avoid logging the key value.\n\nIn summary, the security model is focused on securing access to the repository files via standard OS/Git controls and, crucially, handling the external IMG API key with robust secret management practices.\n\n## 8. Security Considerations and Threat Model\n\nDeveloping an automated code generation system that interacts with potentially untrusted inputs (developer-provided specifications, external LLMs) and produces executable code necessitates a strong focus on security. The threat model considers potential vulnerabilities throughout the pipeline.\n\n**Threat Model (STRIDE/DREAD applied to key components):**\n\n| Threat Category    | Description                                                                                                | Affected Components     | Potential Impact                                                                   |\n| :----------------- | :--------------------------------------------------------------------------------------------------------- | :---------------------- | :--------------------------------------------------------------------------------- |\n| **Spoofing**       | Forging BlueprintLock hashes to disguise tampering.                                                        | IBA, Task Runner (Validation) | Compromised integrity verification, allowing malicious changes to inputs or outputs. |\n| **Tampering**      | Modifying input files (specs, contracts, templates) after IBA processing but before/during Task Runner execution. | Repository Files        | Injecting malicious instructions, altering contracts, breaking validation rules.   |\n|                    | Modifying generated code/tests within a workspace before validation.                                     | Task Workspace          | Bypassing validation, injecting vulnerabilities into final code.                   |\n|                    | Modifying the IMG API response before it reaches the Job.                                                | IMG API Call            | Injecting malicious code, invalid code.                                            |\n| **Repudiation**    | Denying that a specific generation run produced certain code.                                              | Entire System           | Difficulty auditing code origin and changes.                                       |\n| **Information Disclosure** | Leaking sensitive information (e.g., IMG API keys, internal system details) via logs or files.            | Job, Task Runner, Logs  | Compromise of external accounts, exposure of internal architecture.                |\n| **Denial of Service** | Providing overly complex inputs (large contracts, deep dependency graphs) consuming excessive resources.   | IBA, Task Runner        | Slowing down or crashing the system.                                               |\n|                    | Triggering excessive IMG API calls (rate limits, costs).                                                   | Job (via Retries)       | Financial costs, service disruption.                                               |\n|                    | Generating enormous code files causing storage/memory issues.                                              | IMG, Job, Task Runner   | System instability, disk filling.                                                  |\n| **Elevation of Privilege** | Generating code that, when run later, exploits vulnerabilities in its execution environment.               | IMG (Code Output)       | Depends on deployment environment; standard code execution risks.                  |\n|                    | Exploiting vulnerabilities in the Ironclad tool itself to gain unauthorized access.                      | IBA, Task Runner, Job   | Compromise of the build environment.                                               |\n\n**OWASP Top 10 (Applied to the concept):**\n\nWhile not a web application, certain principles apply:\n*   **A01: Broken Access Control:** Relevant to file system permissions and IMG API key scope.\n*   **A03: Injection:** Prompt injection into the LLM is a major risk \u2013 crafting inputs (contracts, context, templates) that manipulate the LLM into generating unintended or malicious code.\n*   **A05: Security Misconfiguration:** Improperly configured validation rules, lax file permissions, insecure handling of IMG API key.\n*   **A08: Software and Data Integrity Failures:** Lack of integrity checks on inputs/outputs, reliance on untrusted components (LLM). BlueprintLock directly addresses data integrity.\n\n**Countermeasures:**\n\n1.  **Input Validation and Sanitization:**\n    *   Rigorously validate the structure and content of all input files (ARCHITECTURE_SPEC, contracts, context, DSL spec, templates). Use JSON schema validation for contracts and context.\n    *   Sanitize any input data incorporated into the IMG prompt to minimize prompt injection risks. While complete sanitization for LLMs is hard, filter out potentially harmful control characters or excessively long/complex sequences.\n    *   Validate the dependency graph for cycles in the IBA.\n2.  **Output Validation and Static Analysis:**\n    *   The multi-step validation process within the Job is the primary defense against malicious or incorrect LLM output.\n    *   **Static Analysis:** `tsc` provides strong type safety. Implement robust DSL validation that can parse code (e.g., using Abstract Syntax Trees - ASTs) and enforce structural/syntactic rules. Integrate standard linters (ESLint, Prettier) and security linters (Semgrep, SonarQube scanner) into the validation sequence.\n    *   **Edge Case Checks:** While basic, the string literal check for edge cases encourages the LLM to include specific functional coverage in tests.\n    *   **Test Execution:** Running generated tests provides dynamic validation of functional correctness.\n    *   **Global Validation:** Full project type check and integration tests catch issues arising from inter-module dependencies or the merging process.\n3.  **Execution Isolation:**\n    *   Run each Job in a strictly isolated environment (separate process, potentially a container or sandbox). The temporary workspace directory should ideally be mounted with restricted permissions.\n    *   Limit the resources (CPU, memory, network access) available to Job processes to mitigate DoS threats from resource-hungry generations or infinite loops in tests.\n4.  **Secure Credential Management (Secrets Management):**\n    *   Use environment variables *at minimum* for non-sensitive configuration.\n    *   For sensitive credentials like the `IMG_API_KEY`, use a dedicated secret management system (HashiCorp Vault, cloud provider secrets managers) integrated into the CI/CD pipeline.\n    *   Fetch secrets just-in-time within the Job process and do not write them to disk or logs.\n5.  **Integrity Verification (BlueprintLock):**\n    *   The `BlueprintLock: sha256:<hash>` header implemented by the IBA provides a mechanism to verify that the core architectural blueprint files (contracts, IDLs, specs, templates, interface stubs) have not been tampered with after the initial build phase. The Task Runner or a separate verification tool should ideally re-check these hashes before starting generation and after merging.\n6.  **Secure Coding Practices:**\n    *   Develop the Ironclad tooling itself (IBA, Task Runner, Job logic, Validation Engines) using secure coding principles.\n    *   Perform static analysis and dependency scanning on the Ironclad tool's codebase to prevent vulnerabilities in the tooling itself.\n7.  **Logging and Monitoring:**\n    *   Implement comprehensive logging of the generation process, including prompts sent to the IMG (sanitized if necessary), IMG responses, validation results, and failure details.\n    *   Monitor system resource usage and execution times to detect potential DoS attempts or inefficient generations.\n8.  **Principle of Least Privilege:**\n    *   The user/service account running the Ironclad tools should have only the necessary file system permissions.\n    *   The IMG API key should have minimal permissions on the IMG provider's side.\n9.  **Dependency Scanning and Vulnerability Management:**\n    *   Regularly scan the Ironclad tool's dependencies for known vulnerabilities.\n    *   Implement a process for patching or updating dependencies promptly.\n10. **Deterministic Behavior:** The emphasis on determinism (given the same inputs, produce the same outputs) is a security benefit as it makes unexpected changes easier to spot and verify.\n\nBy combining rigorous input/output validation, process isolation, secure credential handling, integrity checks, and secure development practices for the tool itself, the Ironclad system can mitigate many of the inherent risks associated with automated code generation using external AI models.\n\n## 9. Scalability, Performance, and Resilience\n\nThe Ironclad Code Generation System's architecture is designed with scalability, performance, and resilience in mind, primarily within the context of a build pipeline execution.\n\n**Scalability:**\n\n*   **User Growth:** The system is not a multi-user online service, so user concurrency isn't a direct scaling concern in the traditional sense. Scalability relates to the number of modules being managed and generated by a development team or organization.\n*   **Module Growth:** The architecture scales horizontally with the number of modules by processing them in parallel. The `IRONCLAD_MAX_PARALLEL` environment variable is the primary knob for controlling this concurrency.\n*   **Data Volume:** The data volume primarily consists of source files (contracts, specs) and generated code. While this grows with the project size, it's managed by the file system and version control, which are assumed to handle large volumes. Large individual files (e.g., huge generated code files) could pose issues, but typical module sizes should be manageable.\n*   **Transaction Rates:** The \"transactions\" are module generation tasks. The system scales by increasing the rate at which these tasks are processed concurrently.\n*   **Bottlenecks:**\n    *   **LLM API Throughput/Rate Limits:** The IMG API is an external dependency and a likely bottleneck. Batching requests (if the API supports it) or simply increasing `IRONCLAD_MAX_PARALLEL` up to the API's limit are factors. High LLM latency directly impacts Job duration.\n    *   **Validation Engine Performance:** Running `tsc`, linters, test runners for each job can be CPU-intensive. Slow validation engines will bottleneck the Job completion rate.\n    *   **File I/O:** Creating/populating workspaces and merging outputs involves significant file operations, which can be a bottleneck on slow storage.\n    *   **CPU/Memory Resources:** The Task Runner and multiple concurrent Jobs consume CPU and memory. The available resources on the build machine/container limit `IRONCLAD_MAX_PARALLEL`.\n    *   **DAG Structure:** While DAG ensures no circular dependencies, a very deep or wide DAG might imply complex interdependencies that, while not blocking *generation*, could complicate *global validation* or make individual module testing/validation slower due to loading many dependencies.\n\n**Scaling Approaches:**\n\n*   **Horizontal Scaling (within a single run):** The `IRONCLAD_MAX_PARALLEL` setting enables running multiple Jobs concurrently on a single machine/runner. This is the primary built-in scaling mechanism.\n*   **Horizontal Scaling (across build agents):** For very large projects, the module set could theoretically be partitioned and assigned to different CI build agents, each running a subset of the modules using the Task Runner. The current architecture assumes a single runner, but the isolation of jobs makes this distributed approach conceptually possible with an external orchestration layer.\n*   **Vertical Scaling:** Running the Task Runner and Jobs on a more powerful machine with more CPU cores and RAM will allow increasing `IRONCLAD_MAX_PARALLEL` and potentially speed up validation steps.\n*   **Caching:**\n    *   **Dependency Caching (CI):** Standard CI practices (e.g., caching `node_modules`) apply to the Ironclad tool's dependencies and the dependencies needed by the generated code/tests.\n    *   **LLM Response Caching:** This is complex and must be done carefully. Caching LLM outputs for a given prompt hash (`prompt.hash`) could drastically speed up regeneration if inputs haven't changed. However, this breaks the direct interaction with the LLM and must be bypassable. It also requires a cache invalidation strategy if the underlying LLM model changes or is updated. *This is a potential future enhancement, not in the core design.*\n    *   **Validation Results Caching:** If a module's inputs (contract, dependencies, context, templates) and the IMG response haven't changed, and the validation engines are versioned, validation results could potentially be cached. Again, complex invalidation rules apply. *Future enhancement.*\n\n**Performance Targets:**\n\nQuantitative performance targets would be set based on project needs, e.g.:\n*   **Overall Pipeline Latency:** The time from starting `ironclad generate run` to getting a success/failure exit code. Target could be, e.g., < 10 minutes for a project of N modules.\n*   **Module Generation Throughput:** Number of modules successfully generated and validated per minute, given `IRONCLAD_MAX_PARALLEL`.\n*   **Job Latency:** Average time taken for a single Job (including retries). Target could be, e.g., < 2 minutes per module.\n*   **Validation Step Latency:** Performance targets for individual validation steps (tsc, dsl, test runner).\n\nAchieving these targets relies heavily on tuning `IRONCLAD_MAX_PARALLEL`, the performance of the chosen LLM and its API, and the efficiency of the validation engines.\n\n**Resilience:**\n\n*   **Job Isolation:** Failure in one Job (due to bad LLM output, validation error, or transient issue) does NOT affect other concurrent Jobs or the main Task Runner process. This is a core resilience mechanism.\n*   **IMG Call Retries:** The 3-attempt retry loop within each Job handles transient network issues or temporary LLM glitches. Prompt amendment helps the LLM potentially correct previous mistakes.\n*   **Failure Reporting:** The system doesn't crash silently. Failures are explicitly captured in `final_failure.json` and aggregated into `.ironclad_failures.json`, providing transparency and enabling debugging.\n*   **Deterministic Behavior:** Aids resilience by making failures reproducible and easier to diagnose.\n*   **Idempotency (Partial):** While not fully idempotent (re-running changes the repository), the Job processing *within a workspace* aims for idempotency per attempt - the same prompt to the same IMG version should ideally yield the same result, leading to deterministic validation results for that attempt. The overall process isn't fully idempotent because retries modify the prompt, and successful runs modify the repository. However, given the *same* input repository state, `ironclad generate run` should produce the *same* output repository state (including `.ironclad_failures.json`).\n*   **Task Runner Robustness:** The Task Runner needs to gracefully handle Job process crashes (non-zero exit codes, signals) and report them correctly.\n*   **BlueprintLock:** While not directly related to runtime resilience, it ensures the integrity of the *blueprint*, which is foundational for reliable generation.\n\n**High Availability / Fault Tolerance:**\n\nHA/FT concepts like failover and redundancy are not directly applicable to the Ironclad tool itself, as it's an on-demand process, not a continuously running service. Its \"availability\" is tied to the availability of the build environment (developer machine, CI runner) and the external IMG service. The resilience features ensure that a single *run* of the tool is as likely to succeed as possible, and failures are contained and reported. The CI/CD environment provides the overall fault tolerance by allowing failed builds to be retried.\n\n## 10. Deployment Strategy and Infrastructure\n\nThe Ironclad Code Generation System is designed to be deployed and executed within standard software development build environments. It is not a service requiring dedicated infrastructure like a production cluster.\n\n**CI/CD Pipeline:**\n\nThe system is intended to be a key step in the build pipeline.\n\n*   **Tools:** Compatible with common CI/CD platforms (Jenkins, GitHub Actions, GitLab CI, Azure DevOps Pipelines, CircleCI, etc.). Relies on standard build runners capable of executing shell commands, Node.js, and having access to necessary tools like `tsc`, test runners (Jest/Mocha), linters.\n*   **Stages:** A typical pipeline involving Ironclad would include:\n    1.  **Checkout:** Get the source code from the Git repository.\n    2.  **Environment Setup:** Install Node.js, Ironclad tool dependencies, validation engine dependencies (tsc, test runner, linters). Set environment variables (`IRONCLAD_MAX_PARALLEL`, `IMG_API_KEY`, `IMG_API_URL`).\n    3.  **IBA Build:** Run `ironclad blueprint build <spec> --output <repo>`. Fail the build if this step fails (e.g., DAG cycle).\n    4.  **Task Runner Generation:** Run `ironclad generate run <repo>`. This is the core generation and validation step.\n    5.  **Check Results:** Check the exit code of the Task Runner. If non-zero, fail the build.\n    6.  **(Optional) Artifacts:** Publish the `.ironclad_failures.json` file as a build artifact.\n    7.  **(Optional) Further Checks:** Run additional steps on the generated code, like security scanning, complexity analysis, deployment manifest generation (potentially using Ironclad principles for *generating* those manifests too).\n    8.  **Commit/Merge:** If the build is successful (both IBA and Task Runner passed), the generated code can be committed back to a specific branch (e.g., `generated/main`) or used directly for deployment downstream in the pipeline. *Self-correction: Automatically committing generated code back to the source branch can be problematic for Git history and merge conflicts. A common pattern is to generate code on a feature branch and include it in the PR, or generate it on a dedicated branch, or generate it as a build artifact used only for deployment.* The architecture supports generating the code *into* the working directory, making any of these post-generation steps feasible.\n\n**Hosting Environment:**\n\n*   The system runs within the build environment provided by the CI/CD platform. This can be:\n    *   **Managed Cloud CI Runners:** (e.g., GitHub Actions runners, GitLab shared runners, AWS CodeBuild).\n    *   **Self-Hosted CI Runners:** Virtual machines or physical servers managed by the organization.\n    *   **Developer Machines:** For local development and testing.\n*   Requires a compatible operating system (Linux, macOS, Windows) and sufficient resources (CPU, RAM, Disk) to support the Ironclad tool and its spawned Jobs.\n\n**Containerization (Docker):**\n\n*   **Recommendation:** Containerizing the Ironclad tool and its dependencies within a Docker image is highly recommended.\n*   **Benefits:**\n    *   **Consistency:** Ensures the same environment (Node.js version, tool versions, libraries) is used across all build agents, eliminating \"it works on my machine\" issues.\n    *   **Isolation:** Provides a clean environment for each build run, avoiding conflicts with other software on the runner.\n    *   **Simplified Setup:** Dependencies are pre-packaged in the image.\n    *   **Portability:** The Docker image can be run easily on any machine or CI platform that supports Docker.\n*   **Implementation:** A `Dockerfile` would define the base image (e.g., Node.js), install the Ironclad tool's dependencies, install validation tools (tsc, test runner, linters), and set up the entry point. The CI pipeline would pull and run this container image.\n\n**Infrastructure as Code (IaC):**\n\n*   IaC (Terraform, CloudFormation, Pulumi) is not used to deploy the Ironclad *tool* itself, as it's not a long-running service.\n*   IaC *is* relevant for provisioning the *build environment* where Ironclad runs (e.g., EC2 instances for self-hosted runners, configuring cloud CI services).\n*   Furthermore, the *output* of the Ironclad system could *include* IaC definitions or deployment manifests (e.g., Kubernetes YAML, CloudFormation templates) for the generated modules, enabling automated infrastructure provisioning for the built services. This is a potential future enhancement leveraging the generation capabilities.\n\n**Deployment Strategies for Generated Code:**\n\nThe Ironclad system focuses on generating code. The deployment of the *generated services* would follow standard practices for the chosen technology stack (e.g., building Docker images for each service, deploying to Kubernetes, Serverless functions, etc.). The Ironclad output provides the source code artifact for this process.\n\n**Environment Configuration Management:**\n\n*   **Build-time Configuration:** Configuration specific to the build process (like `IRONCLAD_MAX_PARALLEL`, paths) is managed via environment variables or command-line flags passed to the `ironclad` command.\n*   **Runtime Configuration (of generated code):** Configuration needed by the *generated services* at runtime should be handled separately, likely defined in configuration files within the repository (potentially also generated/managed by Ironclad principles) and injected into the service environment using standard practices (ConfigMaps in Kubernetes, Parameter Store, environment variables).\n\n**Blue/Green or Canary Deployment:**\n\nThese strategies are deployment patterns for the *generated services*, not the Ironclad build tool itself. The Ironclad system contributes to these strategies by providing potentially higher-quality, more consistent code artifacts.\n\nOverall, the deployment strategy for Ironclad is tightly coupled with the organization's CI/CD practices, treating the tool as a standard build dependency executed within a containerized and securely configured environment.\n\n## 11. Monitoring, Logging, and Observability\n\nEffective monitoring, logging, and observability are crucial for understanding the behavior of the Ironclad code generation pipeline, diagnosing failures, identifying performance bottlenecks, and ensuring the system is operating correctly within the CI/CD environment.\n\n**Logging Strategy:**\n\n*   **Structured Logging:** All components (IBA, Task Runner, Jobs, Validation Engines) should emit structured logs, preferably in JSON format. This makes logs easy to parse, filter, and analyze by automated systems.\n*   **Log Levels:** Use standard log levels (DEBUG, INFO, WARN, ERROR, FATAL) to control verbosity.\n*   **Contextual Information:** Logs MUST include contextual information to identify the source and related task:\n    *   `timestamp`\n    *   `level`\n    *   `component` (e.g., \"IBA\", \"TaskRunner\", \"Job\")\n    *   `message` (human-readable summary)\n    *   `taskId` (unique ID for the Task Runner run)\n    *   `module` (for Job-specific logs, the module name)\n    *   `attempt` (for Job retry loop logs)\n    *   `validationStep` (for validation engine logs)\n    *   `details` (structured object for error messages, validation output, etc.)\n*   **Log Destination:**\n    *   During local development, logs are printed to the console (stdout/stderr).\n    *   In CI/CD, logs should be captured by the CI platform's logging mechanism. Ideally, these logs are shipped to a centralized logging system (e.g., ELK stack, Splunk, Datadog Logs, CloudWatch Logs) for aggregation, search, and analysis.\n*   **Sensitive Data:** Logs MUST NOT contain sensitive information, especially the `IMG_API_KEY` or the full content of sensitive inputs/prompts unless absolutely necessary for debugging in a secure environment and with appropriate redaction. Generated code/tests might be logged at DEBUG level in workspaces but should not typically be in aggregated remote logs due to verbosity and potential IP concerns. Failure reports (`.ironclad_failures.json`) are distinct from logs and are structured artifacts.\n\n**Metrics Collection Strategy:**\n\n*   Collect key metrics to track system performance and health.\n*   **Tools:** Use a metrics library within the Node.js application (e.g., Prometheus client library) to expose metrics, or simply emit metrics as structured log events that a logging system can parse. In CI/CD, these metrics can be collected by the CI platform or pushed to a time-series database (e.g., Prometheus, InfluxDB, CloudWatch Metrics, Datadog Metrics).\n*   **Key Metrics to Track:**\n    *   `ironclad_build_total_runs_count`: Counter for total `generate run` executions.\n    *   `ironclad_build_success_count`: Counter for successful runs.\n    *   `ironclad_build_failure_count`: Counter for failed runs.\n    *   `ironclad_build_duration_seconds`: Histogram/Summary of total `generate run` duration.\n    *   `ironclad_module_total_generation_attempts_count`: Counter for total IMG/validation attempts across all jobs.\n    *   `ironclad_module_successful_generation_count`: Counter for modules successfully generated (passed validation).\n    *   `ironclad_module_permanent_failure_count`: Counter for modules that failed after all retries.\n    *   `ironclad_job_duration_seconds`: Histogram/Summary of individual Job durations (from spawn to exit).\n    *   `ironclad_validation_step_duration_seconds`: Histogram/Summary per validation step type (tsc, dsl, test runner, etc.), tagged by `validation_step` and `module`.\n    *   `ironclad_validation_step_failure_count`: Counter for failures at each validation step, tagged by `validation_step` and `module`.\n    *   `ironclad_img_api_call_count`: Counter for total calls to the IMG API.\n    *   `ironclad_img_api_error_count`: Counter for errors received from the IMG API (tagged by status code).\n    *   Resource utilization metrics for the Task Runner process and Job processes (CPU, memory, disk I/O) collected by the execution environment.\n\n**Observability:**\n\n*   **Distributed Tracing:** Not directly applicable to this pipeline architecture as it's a sequence of distinct process invocations rather than requests flowing through interconnected services.\n*   **Dashboards:** Create dashboards using metrics and logs in a centralized observability platform.\n    *   Overall build success/failure rates.\n    *   Average build duration and identification of slow trends.\n    *   Breakdown of failures by validation step and module.\n    *   Distribution of Job attempt counts (see how often retries are needed).\n    *   Resource usage trends over time.\n*   **Alerting:** Set up alerts based on critical metrics:\n    *   High `ironclad_build_failure_count` rate.\n    *   Significant increase in `ironclad_module_permanent_failure_count`.\n    *   Increase in IMG API error rate.\n    *   Build duration exceeding a threshold.\n    *   Excessive resource utilization on build agents.\n*   **Incident Response:** When a build fails, the CI/CD platform should provide immediate access to the logs and the `.ironclad_failures.json` artifact. Developers/Engineers use these to diagnose the root cause, identifying whether the failure was due to:\n    *   An issue with the Ironclad tool itself.\n    *   An invalid input specification/contract.\n    *   A limitation or error in the IMG output.\n    *   A problem with the validation rules or engines.\n    *   An environmental issue (resource limits, network).\n\nThis comprehensive approach to logging, metrics, and dashboards provides deep insight into the health and performance of the Ironclad system, enabling proactive identification and resolution of issues during the automated code generation process.\n\n## 12. Testing Strategy\n\nA comprehensive testing strategy is essential to ensure the reliability, correctness, and security of the Ironclad Code Generation System itself and to verify that it consistently produces high-quality, valid code according to the specifications.\n\n**Testing the Ironclad System (IBA, Task Runner, Jobs, Validation Orchestration):**\n\n1.  **Unit Tests:**\n    *   **Scope:** Individual functions or small groups of functions within the IBA, Task Runner, and Job logic.\n    *   **Focus:** Testing business logic independent of external dependencies or file system interactions (where possible). E.g., DAG cycle detection algorithm, prompt construction logic, retry loop state transitions, failure report parsing/collation.\n    *   **Tools:** Standard unit testing frameworks (Jest, Mocha, Vitest). Mocking frameworks for isolating components.\n    *   **Code Coverage:** Aim for high code coverage (e.g., >80%) for critical logic paths.\n2.  **Integration Tests:**\n    *   **Scope:** Testing the interaction between different components of the Ironclad system and with external dependencies (mocked).\n    *   **Focus:**\n        *   IBA interacting with the file system (reading inputs, writing outputs).\n        *   Task Runner discovering modules, preparing workspaces, spawning *mocked* Jobs.\n        *   Task Runner aggregating results from *mocked* Jobs.\n        *   Job orchestrating *mocked* IMG calls and *mocked* Validation Engine calls.\n        *   Testing the logic of collating `final_failure.json` into `.ironclad_failures.json`.\n    *   **Tools:** Jest, Mocha, test runners capable of mocking file system operations and child process spawning.\n3.  **Component Tests:**\n    *   **Scope:** Testing the Validation Engines in isolation or integrated with the code they execute against.\n    *   **Focus:**\n        *   Testing the TypeScript compiler (`tsc`) invocation logic and error parsing.\n        *   Testing the DSL validator implementation against various valid and invalid code snippets based on the `validation_dsl_spec`.\n        *   Testing the test runner invocation logic and result parsing.\n        *   Testing the edge case string check logic.\n    *   **Tools:** Dedicated test suites for each validation engine, potentially involving running the actual external tools (`tsc`, `jest`, custom scripts) in a controlled environment and verifying their output.\n4.  **End-to-End Tests:**\n    *   **Scope:** Running the complete Ironclad pipeline (`ironclad blueprint build` followed by `ironclad generate run`) with realistic sample inputs.\n    *   **Focus:** Verifying that the system correctly processes inputs, interacts with a *mocked* IMG, runs validations, produces correct outputs (generated code, failure reports), and exits with the correct status code. Test both success paths (all modules generate correctly) and failure paths (some modules fail, `.ironclad_failures.json` is generated).\n    *   **Tools:** Scripting languages, shell scripts, or a testing framework capable of executing external processes and asserting on file system state and process exit codes. Use a *mock* IMG service to control the LLM output predictably for testing purposes.\n5.  **Performance/Load Tests:**\n    *   **Scope:** Testing the system's performance under varying load conditions.\n    *   **Focus:**\n        *   Measure execution time with increasing numbers of modules.\n        *   Measure execution time with varying `IRONCLAD_MAX_PARALLEL` settings.\n        *   Monitor CPU, memory, and disk usage during runs.\n        *   Test the system's behavior with large input files or code outputs.\n    *   **Tools:** Benchmarking tools, system monitoring tools.\n6.  **Security Penetration Tests:**\n    *   **Scope:** Probing the Ironclad system for vulnerabilities.\n    *   **Focus:**\n        *   Attempting prompt injection via crafted input files.\n        *   Testing file system access controls and workspace isolation.\n        *   Verifying secure handling of the `IMG_API_KEY`.\n        *   Scanning the Ironclad tool's code and dependencies for known vulnerabilities.\n    *   **Tools:** Standard security scanning tools (SAST, DAST if applicable), manual security review.\n\n**Testing the Generated Code:**\n\nWhile the Ironclad system is responsible for *generating* tests (`*.test.ts`), it's crucial to have a strategy for ensuring the *quality* and *sufficiency* of these generated tests and the generated code itself.\n\n1.  **Generated Unit Test Execution (by Job):** The Job process MUST execute the generated unit tests as part of its validation loop. This verifies that the generated code passes the tests *the LLM itself created*.\n2.  **Global Integration Tests (by Task Runner):** The Task Runner MUST run integration tests defined separately (not generated by the LLM) on the *entire* merged codebase. These tests verify the interaction between modules and the system's overall functionality, acting as a higher-level check that the sum of the parts works correctly.\n3.  **Manual Code Review:** Developers MUST review the generated code, especially in the initial phases, to assess its quality, efficiency, readability, and adherence to implicit standards not captured by automated validation.\n4.  **Manual Test Review:** Developers MUST review the generated tests to ensure they provide adequate coverage, are meaningful, and test the module's behavior effectively. The `edgeCases` check in contracts is a basic way to guide LLM test generation quality.\n5.  **Acceptance Testing (UAT):** For critical modules, manual or automated acceptance tests may be performed on the integrated system using the generated code.\n\n**Tools and Frameworks:**\n\n*   **Testing Frameworks:** Jest, Mocha, Vitest for unit/integration tests.\n*   **Mocking:** Libraries compatible with the chosen test framework. `mock-fs` or similar for file system mocking.\n*   **Validation Tools:** `tsc`, specific DSL validator executables/libraries, test runners (`jest`, `mocha`), linters (`eslint`, `prettier`), security linters (`semgrep`).\n*   **CI/CD Platforms:** To automate test execution on every commit.\n*   **Code Coverage Tools:** `nyc`, built into Jest/Vitest.\n*   **Static Analysis:** SonarQube, Semgrep, ESLint.\n\n**Code Coverage Targets:**\n\n*   **Ironclad Tool Codebase:** Aim for high unit test code coverage (>80%) for core logic.\n*   **Generated Code:** Code coverage targets for the *generated* code should be defined based on project standards. While the LLM generates tests, manually ensuring critical paths are covered and setting coverage gates is good practice. The generated tests executed by the Job should ideally meet a minimum coverage target defined in the DSL spec or configuration.\n\nThis multi-layered testing strategy, combining tests *of* the tool with tests *of* the generated code, provides confidence in both the generation process and its outputs.\n\n## 13. Future Enhancements and Technical Debt Roadmap\n\nThe current architecture provides a solid foundation for automated code generation. As the system matures and requirements evolve, several areas are ripe for enhancement. Additionally, certain design choices might incur technical debt that requires future refactoring.\n\n**Potential Future Enhancements:**\n\n1.  **Support for Multiple Languages:** Extend the system to generate code in languages other than TypeScript (e.g., Python, Java, Go, Rust). This would require:\n    *   Language-specific IDL formats or transformations.\n    *   Language-specific interface stub generation.\n    *   Language-specific prompt templates.\n    *   Integration of language-specific validation tools (compilers, linters, test runners, static analyzers).\n    *   Updating the Job logic to handle different tool invocations.\n2.  **Enhanced DSL Validation Engine:** Develop a more sophisticated and flexible DSL validation engine.\n    *   Support for richer rule definitions (AST-based analysis, control flow analysis).\n    *   A dedicated language or configuration format for the DSL rules, possibly compiled for performance.\n    *   Easier integration of custom or third-party validation checks.\n3.  **LLM Response Caching:** Implement a caching layer for IMG responses based on the prompt hash (`prompt.hash`).\n    *   Requires a persistent cache store (e.g., file system, Redis).\n    *   Careful invalidation strategy (e.g., cache expires, cache invalidated if IMG_API_URL/version changes, manual invalidation).\n    *   Mechanism to bypass the cache for specific runs or modules.\n4.  **Interactive Failure Debugging:** Develop tooling to assist developers in debugging generation failures.\n    *   A CLI command to recreate a Job's workspace for local inspection and re-running validation steps manually.\n    *   Providing more context in the failure reports (e.g., relevant snippet of the prompt, specific lines of generated code causing validation errors).\n5.  **Generation of Other Artifacts:** Extend the system to generate documentation (e.g., JSDoc, OpenAPI specs from contracts/code), API gateway configurations, deployment manifests (Kubernetes YAML, Dockerfiles), or configuration files based on the architectural specifications.\n6.  **Improved Prompt Templating:** Move to a more powerful and maintainable templating engine with features like partials, helpers, and better error handling.\n7.  **Dependency Management Generation:** Explore generating dependency declarations (e.g., `package.json` entries) based on the module dependency graph, although this is complex due to versioning conflicts.\n8.  **Optimized Workspace Preparation:** For very large repositories, optimize workspace creation using advanced file system techniques (copy-on-write snapshots if available) or more sophisticated symlinking strategies.\n\n**Technical Debt Roadmap:**\n\n1.  **Initial DSL Validator Implementation:** The initial DSL validator might be a simple script performing regex checks or basic string matching.\n    *   **Debt:** Limited rule complexity, prone to false positives/negatives, hard to maintain.\n    *   **Resolution:** Refactor into a robust AST-based validator (Phase 2/3 Enhancement).\n2.  **Prompt Template Management:** Using simple text files with a basic templating syntax.\n    *   **Debt:** Can become unwieldy for complex prompts, limited logic in templates.\n    *   **Resolution:** Migrate to a more powerful templating engine (Phase 1/2 Enhancement).\n3.  **Workspace Cleanup Robustness:** Ensuring temporary workspaces are *always* cleaned up, even after unexpected process termination.\n    *   **Debt:** Could lead to disk space exhaustion over time.\n    *   **Resolution:** Implement cleanup using `try...finally` blocks, process exit handlers, or an external cleanup utility that runs periodically.\n4.  **Error Reporting Consistency:** Ensuring all validation engines and internal components report errors in a uniform, machine-readable format for easy aggregation.\n    *   **Debt:** Inconsistent formats make parsing `failures.json` difficult.\n    *   **Resolution:** Define a strict error reporting schema and ensure all components adhere to it.\n\n**High-Level Phased Roadmap (Conceptual):**\n\n*   **Phase 1 (Core Functionality - Current):** Implement the core IBA and Task Runner protocol as specified, focusing on TypeScript generation, basic validation (tsc, test execution, simple DSL/edge case), and robust failure reporting.\n*   **Phase 2 (Validation & Usability):** Enhance validation capabilities (more powerful DSL, better linters, security checks). Improve CLI output and failure reporting readability. Implement workspace recreation for debugging.\n*   **Phase 3 (Extensibility & Performance):** Explore support for a second language. Implement LLM response caching. Optimize file operations. Refactor core validation engine.\n*   **Phase 4 (Broader Generation):** Add capabilities to generate documentation, deployment manifests, or other project artifacts.\n\nThis roadmap allows for incremental development, delivering core value first while planning for future growth, robustness, and broader applicability.\n\n## 14. Non-Functional Requirements (NFRs) Summary\n\nThis section summarizes the key non-functional requirements that govern the quality attributes of the Ironclad Code Generation System.\n\n*   **Reliability:**\n    *   **Deterministic Output:** Given the same inputs (including the specific IMG version/state), the system SHALL produce byte-for-byte identical outputs (generated code, failure reports, exit code). This is a cornerstone requirement.\n    *   **Consistent Failure Reporting:** When failures occur, the system SHALL produce a structured, machine-readable report (`.ironclad_failures.json`) that accurately reflects the validation errors encountered.\n    *   **Fault Isolation:** Failure during the generation/validation of one module SHALL NOT prevent the successful generation/validation of other independent modules or crash the entire Task Runner process.\n    *   **Retry Mechanism:** The system SHALL attempt LLM invocation and validation up to 3 times per module Job to mitigate transient failures.\n    *   **MTBF (Mean Time Between Failures):** While not a continuous service, the tooling itself should be stable. Aim for a high MTBF for the core Task Runner process during a run, minimizing crashes of the tool itself.\n    *   **MTTR (Mean Time To Recover):** Failures in individual module Jobs are reported immediately. Recovery involves developer action based on the failure report. The tool's design supports rapid diagnosis via detailed logging and the `.ironclad_failures.json` artifact.\n\n*   **Availability:**\n    *   The system's availability is tied to the availability of the build environment (CI/CD runner) and the external IMG API. It is not a 24/7 service.\n    *   The system SHALL gracefully handle unavailable or slow IMG APIs (via retries and timeouts), although persistent unavailability will cause the build to fail.\n\n*   **Performance:**\n    *   **Throughput:** The system SHALL be able to process N modules within T time, where T is reasonably linear with N for N > `IRONCLAD_MAX_PARALLEL`, assuming sufficient resources and a responsive IMG API. Target: Generate and validate 100 modules (average complexity) within 15 minutes on a standard build agent with `IRONCLAD_MAX_PARALLEL` appropriately configured.\n    *   **Latency (per Job):** The average time for a single module Job (including retries if needed) SHALL be minimized. Target: Average Job completion time < 2 minutes.\n    *   **Resource Utilization:** The system SHALL operate within reasonable CPU, memory, and disk limits for a build process on the target CI environment. Memory usage per Job SHALL be constrained.\n\n*   **Scalability:**\n    *   The system SHALL support scaling generation throughput by increasing the `IRONCLAD_MAX_PARALLEL` setting, up to the limits of the execution environment and external IMG API.\n    *   The architecture SHALL remain viable for projects with hundreds or thousands of modules, although execution time will increase proportionally to the number of modules and the `IRONCLAD_MAX_PARALLEL` setting.\n\n*   **Maintainability:**\n    *   The codebase for the Ironclad tooling SHALL adhere to standard coding practices (e.g., ESLint, Prettier) and include comprehensive documentation (code comments, architectural diagrams).\n    *   Components SHALL have clear responsibilities and well-defined interfaces to minimize coupling.\n    *   The system's configuration and input files SHALL be well-structured and documented.\n\n*   **Extensibility:**\n    *   It SHALL be possible to integrate new validation rules into the DSL validation process without modifying core Task Runner logic.\n    *   It SHALL be possible to modify or add prompt templates to guide the LLM for different module types or requirements.\n    *   The architecture SHOULD allow for the addition of support for generating code in new programming languages in future phases with isolated component changes.\n\n*   **Auditability:**\n    *   The system SHALL produce logs detailing the execution flow, including Job spawning, attempts, and validation results.\n    *   The `.ironclad_failures.json` file provides a record of all generation failures for a specific run.\n    *   The BlueprintLock headers provide cryptographic verification of the integrity of core architectural input files.\n    *   The generated code itself, when checked into version control, provides an audit trail of the output.\n\n*   **Security:**\n    *   The system SHALL prevent prompt injection attempts from compromising the integrity of the generation process or output code, primarily through rigorous input validation and output validation.\n    *   The system SHALL handle sensitive credentials (IMG API key) securely, relying on environment variables or dedicated secret management.\n    *   Generated code SHALL be subjected to static analysis and security linting as part of the validation process.\n    *   Jobs SHALL be executed in isolated environments to limit the impact of malicious or faulty generated code during the validation phase.\n\n*   **Compliance:**\n    *   The Ironclad tool itself doesn't typically require compliance like GDPR or HIPAA unless the *inputs* or *generated code* contain sensitive data, which is outside the scope of the tool's core function but must be considered in its *usage*. The tool's secure handling of input data and secrets contributes to overall system compliance where applicable.\n    *   The generated code should pass checks defined in the `validation_dsl_spec.md`, which can include compliance-related rules (e.g., logging formats required for audit trails, specific data handling patterns).\n\nThese NFRs define the necessary qualities the Ironclad system must possess to be a reliable, performant, and trustworthy part of the software development lifecycle.\n\n## 15. Glossary and Definitions\n\nThis glossary defines key terms, acronyms, and architectural concepts used throughout this document to ensure a shared understanding.\n\n*   **ARCHITECTURE_SPEC:** The primary Markdown file that defines the overall structure and modules of the system to be generated.\n*   **BlueprintLock:** A header (e.g., `BlueprintLock: sha256:<hash>`) added by the IBA to files it generates or copies into the repository, used for verifying file integrity. The hash is a SHA-256 checksum of the file's content, including the final header line itself.\n*   **Contracts (Module Contracts):** JSON files (`contracts/*.json`) formally defining the interface, dependencies, and generation instructions for individual modules.\n*   **DAG (Directed Acyclic Graph):** A graph data structure where nodes are connected by directed edges (dependencies) and contain no cycles. The module dependency graph must be a DAG.\n*   **Deterministic:** A process or system is deterministic if, given the same input, it always produces the same output. A core goal of the Ironclad system.\n*   **DSL (Domain Specific Language):** A small language or format used to define specific rules or configurations. `validation_dsl_spec.md` uses a DSL to define code validation rules.\n*   **IBA (Ironclad Blueprint Architect):** The initial process that reads the `ARCHITECTURE_SPEC` and related files, validates the input, and scaffolds the initial repository structure including interface stubs and BlueprintLocks.\n*   **IDL (Interface Definition Language):** Files (`idl/*.ts`) defining shared data types and interfaces used across modules and contracts.\n*   **IMG (Ironclad Module Generator):** The external Large Language Model (LLM) invocation responsible for generating the implementation and test code for a single module based on a prompt. Treated as an external service via an API.\n*   **Independently:** Describes a Job operating with no read or write dependency on any files generated by another concurrent Job, except for the initial shared input files in the workspace.\n*   **Interface Stub:** A TypeScript interface file (`src/modules/I<ModuleName>.ts`) generated by the IBA, representing the public contract of a module for type checking.\n*   **Job:** A single, isolated operating-system process spawned by the Task Runner to handle the full lifecycle (IMG invocation, validation, retries) for exactly one Module-Generation Task.\n*   **LLM (Large Language Model):** The type of AI model used by the IMG to generate code.\n*   **Module-Generation Task:** A single, isolated unit of work performed by a Job, involving invoking the IMG and validating the output for one specific module.\n*   **NFRs (Non-Functional Requirements):** Quality attributes of the system such as performance, scalability, reliability, security, and maintainability, as opposed to functional behavior.\n*   **Parallel:** Multiple Jobs executing concurrently in time, with their start and end times overlapping, limited by `IRONCLAD_MAX_PARALLEL`.\n*   **Prompt Amendment:** The process within a Job's retry loop where details of previous validation failures are added to the prompt before re-invoking the IMG.\n*   **Prompt Template:** A file (`prompt_templates/*.tmpl`) containing a template used by the Task Runner to construct the `prompt.txt` for the IMG.\n*   **Repository:** The file system directory containing all input specifications, generated code, and output reports for a specific project using the Ironclad system. Managed via Git.\n*   **Spawn:** To create a completely separate operating-system process, container, or cloud function invocation for executing a Job.\n*   **System Context:** A JSON file (`system_context.json`) providing global information and context relevant to the generation of all modules.\n*   **Task Runner:** The main orchestration process that discovers modules, prepares workspaces, manages concurrency, spawns Jobs, awaits their completion, aggregates results, and performs global validation.\n*   **Task Workspace:** A temporary, isolated directory (`.tmp/ironclad_tasks/<ModuleName>/`) created by the Task Runner for a single Job to operate within. Contains only the files necessary for that specific module's task.\n*   **Validation:** The deterministic, automated sequence of checks performed by a Job on generated code, including static type-checking, DSL-rule validation, linter checks, edge case string checks, and execution of the generated test file.\n\nThis glossary serves as a quick reference for understanding the terminology used within this architectural document and the Ironclad system specification.\n\nA.1 Component & Sequence Diagrams (non-normative)\nmermaid\nCopy\nEdit\nflowchart TD\n    subgraph Control-Plane\n        IBA[Ironclad Blueprint\u2006Architect]\n        TR[Task Runner]\n    end\n    subgraph N(x) Module-Jobs\n        Job1[Job::<Module>]\n        JobN[\u2026]\n    end\n\n    IBA -->|Blueprint skeleton| TR\n    TR -->|spawn| Job1\n    TR -->|spawn| JobN\n    Job1 -->|impl+tests| TR\n    JobN --> TR\nmermaid\nCopy\nEdit\nsequenceDiagram\n    autonumber\n    participant TR as Task-Runner\n    participant Job as Module-Job\n    participant IMG as LLM (IMG)\n\n    TR->>Job: spawn()\n    Job->>IMG: prompt.txt (attempt 1)\n    IMG-->>Job: JSON {impl, tests}\n    Job->>Job: validate \u279c \u274c\n    loop up to 3 attempts\n        Job->>IMG: amended prompt (includes LAST_ERROR)\n        IMG-->>Job: new JSON\n        Job->>Job: validate\n    end\n    Job-->>TR: success \u2713 or final_failure.json\nA.2 Canonical JSON Schemas & Examples\n<details><summary>contracts/<ModuleName>.json</summary>\njsonc\nCopy\nEdit\n{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"title\": \"ModuleContract\",\n  \"type\": \"object\",\n  \"required\": [\"name\",\"purpose\",\"publicAPI\",\"dependencies\",\"constructorParams\"],\n  \"properties\": {\n    \"name\": { \"type\": \"string\", \"pattern\": \"^[A-Z][A-Za-z0-9]+$\" },\n    \"purpose\": { \"type\": \"string\", \"minLength\": 10 },\n    \"functionSignatures\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"required\": [\"name\",\"parameters\",\"returnType\"],\n        \"properties\": {\n          \"name\": { \"type\": \"string\" },\n          \"parameters\": { \"type\": \"array\", \"items\": {\"$ref\":\"#/definitions/param\"} },\n          \"returnType\": { \"type\": \"string\" },\n          \"description\": { \"type\": \"string\" }\n        }\n      }\n    },\n    \"dataStructures\":   { \"type\": \"array\", \"items\": {\"$ref\":\"#/definitions/dataStruct\"} },\n    \"publicAPI\":        { \"type\": \"array\", \"items\": { \"type\": \"string\" } },\n    \"dependencies\":     { \"type\": \"array\", \"items\": { \"type\": \"string\" } },\n    \"constructorParams\":{ \"type\": \"array\", \"items\": { \"type\": \"string\" } },\n    \"instructions\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"overview\":   { \"type\": \"string\" },\n        \"steps\":      { \"type\": \"array\", \"items\": { \"type\": \"string\" } },\n        \"edgeCases\":  { \"type\": \"array\", \"items\": { \"type\": \"string\" } }\n      }\n    }\n  },\n  \"definitions\": {\n    \"param\":      { \"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"},\"type\":{\"type\":\"string\"}}},\n    \"dataStruct\": { \"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"},\"properties\":{\"type\":\"array\",\"items\":{\"$ref\":\"#/definitions/param\"}}}}\n  }\n}\nMinimal example:\n\njson\nCopy\nEdit\n{\n  \"name\": \"UserService\",\n  \"purpose\": \"Business-logic fa\u00e7ade for user CRUD and profile enrichment.\",\n  \"publicAPI\": [\"createUser\",\"getById\"],\n  \"dependencies\": [\"EmailAdapter\"],\n  \"constructorParams\": [\"EmailAdapter\"],\n  \"functionSignatures\": [\n    { \"name\":\"createUser\", \"parameters\":[{\"name\":\"dto\",\"type\":\"CreateUserDTO\"}], \"returnType\":\"User\" },\n    { \"name\":\"getById\",    \"parameters\":[{\"name\":\"id\",\"type\":\"string\"}],      \"returnType\":\"User\" }\n  ],\n  \"instructions\": {\n    \"overview\": \"Send welcome-mail after persisting user.\",\n    \"steps\": [\n      \"Validate DTO\",\n      \"Hash password with bcrypt-12\",\n      \"Persist record\",\n      \"Invoke EmailAdapter.sendWelcome\"\n    ],\n    \"edgeCases\": [\"email already exists\",\"db timeout > 1s\"]\n  }\n}\n</details> <details><summary>final_failure.json</summary>\njson\nCopy\nEdit\n{\n  \"module\": \"UserService\",\n  \"attempts\": 3,\n  \"lastPromptHash\": \"sha256:ab12\u2026\",\n  \"errorSummary\": \"Edge-case 'db timeout > 1s' not covered in tests.\"\n}\n</details>\nA.3 Prompt-Template Reference (prompt_templates/module_prompt.tmpl)\ntext\nCopy\nEdit\n### DO NOT EDIT ABOVE THIS LINE \u2013 auto-generated ###\nModule: {{ contract.name }}\n\n<CONTRACT_JSON>\n{{ contract | toJsonPretty }}\n</CONTRACT_JSON>\n\n<IDL_SNIPPETS>\n{{ idlSnippets }}\n</IDL_SNIPPETS>\n\n<SYSTEM_CONTEXT>\n{{ systemContext | toJsonPretty }}\n</SYSTEM_CONTEXT>\n\n{{#if previousFailure}}\n===LAST_ERROR===\n{{ previousFailure | toJsonPretty }}\n{{/if}}\n\n### REQUIRED_OUTPUT\nRespond with EXACT JSON:\n{\n  \"implementationCode\": \"string\",\n  \"testCode\": \"string\"\n}\nA.4 Environment-Variable Contract\nVar\tDefault\tSecret\tDescription\nIMG_API_KEY\t\u2014\t\u2705\tAPI key for the LLM provider.\nIRONCLAD_MAX_PARALLEL\t4\t\u274c\tMax concurrent Jobs.\nTS_NODE_TRANSPILE_ONLY\t1\t\u274c\tSpeeds up jest runs during validation.\n\nA.5 Concurrency Sizing Formula\nbash\nCopy\nEdit\nmaxJobs = min(\n  cpuCores / 1,                 # ~1 core per Job\n  availMemGB / 2,               # ~2 GB per Job incl. LLM call\n  providerQPS / callsPerJob,    # respect API quotas\n  IRONCLAD_MAX_PARALLEL env\n)\nA.6 Exit-Code Taxonomy\nCode\tMeaning\n0\tAll modules built & global checks passed.\n65\tAttempt to overwrite existing src file (EX_DATAERR).\n100\tAt least one final_failure.json present.\n101\tGlobal type-check failed.\n102\tDSL or lint validation failed.\n130\tInterrupted via SIGINT/SIGTERM.\n\nA.7 Sample Interface Stub & IDL\nIDL (idl/user_service.idl)\n\npgsql\nCopy\nEdit\nservice UserService {\n  rpc Create(CreateUserDTO) returns User;\n  rpc GetById(string id)    returns User;\n}\nGenerated stub (src/modules/IUserService.ts)\n\nts\nCopy\nEdit\nexport interface IUserService {\n  createUser(dto: CreateUserDTO): Promise<User>;\n  getById(id: string): Promise<User>;\n}\nA.8 BlueprintLock Verification Snippet\nbash\nCopy\nEdit\n#!/usr/bin/env bash\nset -euo pipefail\nfile=\"$1\"\nexpected=$(grep -Eo 'BlueprintLock: sha256:[a-f0-9]+' \"$file\" | cut -d: -f3)\nactual=$(sha256sum \"$file\" | awk '{print $1}')\nif [[ \"$expected\" != \"$actual\" ]]; then\n  echo \"Lock mismatch for $file\" >&2\n  exit 1\nfi\n\n\n---\n\n## \ud83d\udd27 Appendix A \u2014 Remaining Specification Gaps\n\n### A.1 Component & Sequence Diagrams  *(non-normative)*\n\n```mermaid\nflowchart TD\n    subgraph Control-Plane\n        IBA[Ironclad Blueprint\u2006Architect]\n        TR[Task Runner]\n    end\n    subgraph N(x) Module-Jobs\n        Job1[Job::<Module>]\n        JobN[\u2026]\n    end\n\n    IBA -->|Blueprint skeleton| TR\n    TR -->|spawn| Job1\n    TR -->|spawn| JobN\n    Job1 -->|impl+tests| TR\n    JobN --> TR\n```\n\n```mermaid\nsequenceDiagram\n    autonumber\n    participant TR as Task-Runner\n    participant Job as Module-Job\n    participant IMG as LLM (IMG)\n\n    TR->>Job: spawn()\n    Job->>IMG: prompt.txt (attempt 1)\n    IMG-->>Job: JSON {impl, tests}\n    Job->>Job: validate \u279c \u274c\n    loop up to 3 attempts\n        Job->>IMG: amended prompt (includes LAST_ERROR)\n        IMG-->>Job: new JSON\n        Job->>Job: validate\n    end\n    Job-->>TR: success \u2713 or final_failure.json\n```\n\n---\n\n### A.2 Canonical JSON Schemas & Examples\n\n<details><summary>contracts/<ModuleName>.json</summary>\n\n```jsonc\n{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"title\": \"ModuleContract\",\n  \"type\": \"object\",\n  \"required\": [\"name\",\"purpose\",\"publicAPI\",\"dependencies\",\"constructorParams\"],\n  \"properties\": {\n    \"name\": { \"type\": \"string\", \"pattern\": \"^[A-Z][A-Za-z0-9]+$\" },\n    \"purpose\": { \"type\": \"string\", \"minLength\": 10 },\n    \"functionSignatures\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"required\": [\"name\",\"parameters\",\"returnType\"],\n        \"properties\": {\n          \"name\": { \"type\": \"string\" },\n          \"parameters\": { \"type\": \"array\", \"items\": {\"$ref\":\"#/definitions/param\"} },\n          \"returnType\": { \"type\": \"string\" },\n          \"description\": { \"type\": \"string\" }\n        }\n      }\n    },\n    \"dataStructures\":   { \"type\": \"array\", \"items\": {\"$ref\":\"#/definitions/dataStruct\"} },\n    \"publicAPI\":        { \"type\": \"array\", \"items\": { \"type\": \"string\" } },\n    \"dependencies\":     { \"type\": \"array\", \"items\": { \"type\": \"string\" } },\n    \"constructorParams\":{ \"type\": \"array\", \"items\": { \"type\": \"string\" } },\n    \"instructions\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"overview\":   { \"type\": \"string\" },\n        \"steps\":      { \"type\": \"array\", \"items\": { \"type\": \"string\" } },\n        \"edgeCases\":  { \"type\": \"array\", \"items\": { \"type\": \"string\" } }\n      }\n    }\n  },\n  \"definitions\": {\n    \"param\":      { \"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"},\"type\":{\"type\":\"string\"}}},\n    \"dataStruct\": { \"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"},\"properties\":{\"type\":\"array\",\"items\":{\"$ref\":\"#/definitions/param\"}}}}\n  }\n}\n```\n\nMinimal example:\n\n```json\n{\n  \"name\": \"UserService\",\n  \"purpose\": \"Business-logic fa\u00e7ade for user CRUD and profile enrichment.\",\n  \"publicAPI\": [\"createUser\",\"getById\"],\n  \"dependencies\": [\"EmailAdapter\"],\n  \"constructorParams\": [\"EmailAdapter\"],\n  \"functionSignatures\": [\n    { \"name\":\"createUser\", \"parameters\":[{\"name\":\"dto\",\"type\":\"CreateUserDTO\"}], \"returnType\":\"User\" },\n    { \"name\":\"getById\",    \"parameters\":[{\"name\":\"id\",\"type\":\"string\"}],      \"returnType\":\"User\" }\n  ],\n  \"instructions\": {\n    \"overview\": \"Send welcome-mail after persisting user.\",\n    \"steps\": [\n      \"Validate DTO\",\n      \"Hash password with bcrypt-12\",\n      \"Persist record\",\n      \"Invoke EmailAdapter.sendWelcome\"\n    ],\n    \"edgeCases\": [\"email already exists\",\"db timeout > 1s\"]\n  }\n}\n```\n\n</details>\n\n<details><summary>final_failure.json</summary>\n\n```json\n{\n  \"module\": \"UserService\",\n  \"attempts\": 3,\n  \"lastPromptHash\": \"sha256:ab12\u2026\",\n  \"errorSummary\": \"Edge-case 'db timeout > 1s' not covered in tests.\"\n}\n```\n\n</details>\n\n---\n\n### A.3 Prompt-Template Reference (`prompt_templates/module_prompt.tmpl`)\n\n```text\n### DO NOT EDIT ABOVE THIS LINE \u2013 auto-generated ###\nModule: {{ contract.name }}\n\n<CONTRACT_JSON>\n{{ contract | toJsonPretty }}\n</CONTRACT_JSON>\n\n<IDL_SNIPPETS>\n{{ idlSnippets }}\n</IDL_SNIPPETS>\n\n<SYSTEM_CONTEXT>\n{{ systemContext | toJsonPretty }}\n</SYSTEM_CONTEXT>\n\n{{#if previousFailure}}\n===LAST_ERROR===\n{{ previousFailure | toJsonPretty }}\n{{/if}}\n\n### REQUIRED_OUTPUT\nRespond with EXACT JSON:\n{\n  \"implementationCode\": \"string\",\n  \"testCode\": \"string\"\n}\n```\n\n---\n\n### A.4 Environment-Variable Contract\n\n| Var                      | Default | Secret | Description                            |\n| ------------------------ | ------- | ------ | -------------------------------------- |\n| `IMG_API_KEY`            | \u2014       | \u2705      | API key for the LLM provider.          |\n| `IRONCLAD_MAX_PARALLEL`  | `4`     | \u274c      | Max concurrent Jobs.                   |\n| `TS_NODE_TRANSPILE_ONLY` | `1`     | \u274c      | Speeds up jest runs during validation. |\n\n---\n\n### A.5 Concurrency Sizing Formula\n\n```\nmaxJobs = min(\n  cpuCores / 1,                 # ~1 core per Job\n  availMemGB / 2,               # ~2 GB per Job incl. LLM call\n  providerQPS / callsPerJob,    # respect API quotas\n  IRONCLAD_MAX_PARALLEL env\n)\n```\n\n---\n\n### A.6 Exit-Code Taxonomy\n\n| Code  | Meaning                                               |\n| ----- | ----------------------------------------------------- |\n| `0`   | All modules built & global checks passed.             |\n| `65`  | Attempt to overwrite existing src file (EX\\_DATAERR). |\n| `100` | At least one `final_failure.json` present.            |\n| `101` | Global type-check failed.                             |\n| `102` | DSL or lint validation failed.                        |\n| `130` | Interrupted via SIGINT/SIGTERM.                       |\n\n---\n\n### A.7 Sample Interface Stub & IDL\n\n*IDL (`idl/user_service.idl`)*\n\n```\nservice UserService {\n  rpc Create(CreateUserDTO) returns User;\n  rpc GetById(string id)    returns User;\n}\n```\n\n*Generated stub (`src/modules/IUserService.ts`)*\n\n```ts\nexport interface IUserService {\n  createUser(dto: CreateUserDTO): Promise<User>;\n  getById(id: string): Promise<User>;\n}\n```\n\n---\n\n### A.8 BlueprintLock Verification Snippet\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\nfile=\"$1\"\nexpected=$(grep -Eo 'BlueprintLock: sha256:[a-f0-9]+' \"$file\" | cut -d: -f3)\nactual=$(sha256sum \"$file\" | awk '{print $1}')\nif [[ \"$expected\" != \"$actual\" ]]; then\n  echo \"Lock mismatch for $file\" >&2\n  exit 1\nfi\n```",
  "reasoning_tree": {
    "Phase 1: Blueprint Architect (IBA) Implementation and Core Scaffolding": {
      "Task 1.1: Initialize TypeScript/Node.js project and configure a CLI framework (e.g., Commander, Yargs).": [
        {
          "id": "step_1",
          "description": "Initialize a new Node.js project. Create a directory named 'ironclad-cg-system', navigate into it, and run `npm init -y`. Immediately after, initialize a Git repository using `git init`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "run `npm init -y`",
                "initialize a Git repository using `git init`"
              ],
              "key_entities_dependencies": [
                "ironclad-cg-system"
              ],
              "technology_hints": [
                "Node.js",
                "npm",
                "Git"
              ]
            },
            "step_critique": {
              "alignment_critique": "Steps are perfectly aligned. They establish the Node.js project foundation and introduce Git versioning, both of which are core requirements defined in the Project Constitution's tech stack and global dependencies.",
              "sequence_critique": "The sequence of creating a directory, initializing the npm project, and then initializing the Git repository is logical and follows standard development practice. No issues found.",
              "clarity_critique": "The instructions are explicit, using specific commands and a defined directory name, making them clear and directly actionable for an AI coding agent."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a `.gitignore` file in the project root. Add entries to ignore common Node.js artifacts and project-specific directories mentioned in the constitution. Include at least: `node_modules/`, `dist/`, `.env`, `*.log`, `.tmp/`, and `.ironclad_failures.json`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                ".gitignore",
                "node_modules/",
                "dist/",
                ".env",
                "*.log",
                ".tmp/",
                ".ironclad_failures.json"
              ],
              "technology_hints": [
                "Node.js",
                "Git"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. It includes standard Node.js artifacts and, crucially, incorporates project-specific transient directories (`.tmp/`) and output files (`.ironclad_failures.json`) defined in the Project Constitution's file map. Ignoring `.env` also directly supports the security requirement for handling API keys.",
              "sequence_critique": "The step is logically sequenced. Creating a `.gitignore` file is a fundamental part of initializing a new project and should be done at the beginning of the setup process.",
              "clarity_critique": "The step is clear and actionable. It specifies the exact file to create, its location, and a minimum set of required entries, making it unambiguous for an AI coding agent."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Install TypeScript and the official Node.js type definitions as development dependencies. Use the command: `npm install --save-dev typescript @types/node`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install 'typescript' as a development dependency",
                "Install '@types/node' as a development dependency"
              ],
              "key_entities_dependencies": [],
              "technology_hints": [
                "TypeScript",
                "Node.js",
                "npm",
                "typescript",
                "@types/node"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Installing TypeScript and its associated Node.js type definitions is a mandatory and foundational action for the task of initializing a project whose tech stack is explicitly defined as 'TypeScript/Node.js' in the Project Constitution.",
              "sequence_critique": "The step is logically sequenced. These packages are prerequisites for any subsequent TypeScript configuration (e.g., creating tsconfig.json) or writing code. No issues found.",
              "clarity_critique": "The step is exceptionally clear and actionable. It provides the exact, correct command, including the appropriate `--save-dev` flag, leaving no ambiguity for an AI coding agent."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Generate a `tsconfig.json` file by running `npx tsc --init`. Then, modify the file to configure it for a modern Node.js CLI application. Set the following options: `target: \"es2022\"`, `module: \"commonjs\"`, `rootDir: \"./src\"`, `outDir: \"./dist\"`, `esModuleInterop: true`, `forceConsistentCasingInFileNames: true`, `strict: true`, `skipLibCheck: true`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "run `npx tsc --init`"
              ],
              "key_entities_dependencies": [
                "tsconfig.json",
                "src",
                "dist"
              ],
              "technology_hints": [
                "TypeScript",
                "tsc",
                "npx",
                "Node.js",
                "es2022",
                "commonjs"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Configuring `tsconfig.json` is a foundational task for a TypeScript/Node.js project, which is the specified tech stack. The chosen settings, especially `strict: true`, directly support the project's core mission of ensuring high-quality, validated code.",
              "sequence_critique": "The sequence is logical. Initializing the configuration file before modifying it is the standard and correct workflow for this task.",
              "clarity_critique": "The instructions are clear, specific, and directly actionable. They provide the exact command to run and the precise key-value pairs to set, leaving no ambiguity for an AI agent."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Create the main source directory `src/` and within it, create the primary CLI entry point file named `index.ts`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/",
                "index.ts"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is a fundamental and necessary action for creating the project's source code structure. It directly supports the task of initializing a TypeScript/Node.js project and aligns perfectly with the overall goal of building the CLI tool specified in the Project Constitution.",
              "sequence_critique": "This is the correct initial step for creating the source code structure. Creating the source directory and entry point file logically precedes adding any application logic or framework configuration. The sequence is sound.",
              "clarity_critique": "The instruction is clear, specific, and unambiguous. It explicitly names the directory (`src/`) and the entry point file (`index.ts`), making it highly actionable for a coding agent."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Install the `commander` library, which will be used for building the CLI interface as specified in the architecture document. Run `npm install commander`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install the `commander` library",
                "Run `npm install commander`"
              ],
              "key_entities_dependencies": [],
              "technology_hints": [
                "commander",
                "npm"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Installing a CLI framework like `commander` is a direct and necessary action for the task of configuring the CLI. The architecture document (Section 6: UI/UX Design) explicitly lists `Commander` as a suitable choice for the project's CLI framework.",
              "sequence_critique": "A prerequisite step is missing. Before installing an npm package, a Node.js project must be initialized (e.g., via `npm init -y`) to create the `package.json` file. This installation step should logically follow the project initialization.",
              "clarity_critique": "The step is clear and directly actionable. It states the purpose of the library and provides the exact command to be executed by a coding agent."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Implement the basic CLI structure in `src/index.ts`. Add the shebang `#!/usr/bin/env node` to the first line. Use `commander` to define the main `ironclad` program, setting its version (e.g., '0.1.0') and a brief description. As per Section 5 of the architecture, add two commands: `blueprint` and `generate`. For now, their action should simply log a confirmation message to the console (e.g., 'Blueprint command called.'). Finally, call `program.parse(process.argv)` to execute the CLI logic.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/index.ts",
                "#!/usr/bin/env node",
                "ironclad",
                "program",
                "blueprint",
                "generate",
                "program.parse",
                "process.argv",
                "Section 5 of the architecture"
              ],
              "technology_hints": [
                "node",
                "commander",
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the task of initializing the CLI framework. They directly address the requirements outlined in Section 5 of the Project Constitution by establishing the main `ironclad` program and its top-level commands, `blueprint` and `generate`.",
              "sequence_critique": "The sequence of actions is logical. It correctly starts with the file-level shebang, moves to the program definition, adds the command stubs, and concludes with the necessary call to parse arguments and execute the CLI. No prerequisite steps are missing.",
              "clarity_critique": "The instructions are clear, specific, and highly actionable. They specify the file, library, command names, and placeholder logic, making the task unambiguous for an AI coding agent."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Configure `package.json` to define the project as an executable. Add a `bin` field to map the `ironclad` command to its compiled output: `\"bin\": { \"ironclad\": \"./dist/index.js\" }`. Also, ensure the `main` field points to the same file: `\"main\": \"./dist/index.js\"`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "package.json",
                "bin",
                "ironclad",
                "./dist/index.js",
                "main"
              ],
              "technology_hints": [
                "package.json"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. The Project Constitution (Section 5) explicitly defines the system as a CLI tool invoked via an `ironclad` command. Configuring the `bin` field in `package.json` is the standard and necessary mechanism to achieve this.",
              "sequence_critique": "The sequence is logical. Defining the project's executable entry point is a foundational step that should occur during the initial project setup and configuration.",
              "clarity_critique": "The instruction is clear and directly actionable. Specifying that the `main` field should also point to the executable's entry point is acceptable and consistent with the Project Constitution's focus on 'CLI Tooling', as there is no stated requirement for the project to be used as a library."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Add development scripts to `package.json` to streamline the development workflow. Add the following to the `scripts` object: `\"build\": \"tsc\"`, `\"start\": \"npm run build && node dist/index.js\"`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "package.json",
                "scripts",
                "dist/index.js"
              ],
              "technology_hints": [
                "tsc",
                "npm",
                "node"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are well-aligned with the task. Adding `build` and `start` scripts is a fundamental and necessary action for creating a runnable TypeScript/Node.js CLI tool, which is the core of the Ironclad system.",
              "sequence_critique": "The sequence is logical. These scripts are typically added during the initial project setup, following the creation of `package.json` and the configuration of TypeScript (`tsconfig.json`).",
              "clarity_critique": "The instruction is clear and directly actionable. However, to better 'streamline the development workflow', consider adding a 'dev' script utilizing a tool like `ts-node` (e.g., `\"dev\": \"ts-node src/index.ts\"`), which is referenced in Appendix A.4 of the constitution. This would allow for faster iteration during development by avoiding a full re-compilation step on every change."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Verify the complete setup. First, compile the project by running `npm run build`. Then, use `npm link` to install the command globally for local testing. Finally, execute `ironclad --help`, `ironclad blueprint`, and `ironclad generate` in your terminal to confirm that the help output and the placeholder messages for each command are displayed correctly.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "compile the project by running `npm run build`",
                "use `npm link` to install the command globally for local testing",
                "execute `ironclad --help`... in your terminal",
                "execute `ironclad blueprint`... in your terminal",
                "execute `ironclad generate`... in your terminal"
              ],
              "key_entities_dependencies": [
                "ironclad",
                "blueprint",
                "generate"
              ],
              "technology_hints": [
                "npm",
                "terminal"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. It serves as a crucial verification that the foundational CLI structure, as specified in the Project Constitution's 'API Design and Endpoints' section, has been correctly established with placeholders for the 'blueprint' and 'generate' commands.",
              "sequence_critique": "The sequence is logical and correct. Compiling the project (`npm run build`) is a necessary prerequisite for making the command available system-wide for testing (`npm link`), which in turn is required before executing the `ironclad` command itself.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. The specific commands to run are provided, and the expected outcome (confirming help and placeholder messages are displayed) is unambiguous, making it easy for an AI agent to execute and verify."
            }
          }
        }
      ],
      "Task 1.2: Define core data structures as TypeScript types, including `ModuleContract`.": [
        {
          "id": "step_1",
          "description": "Create a new directory `src/core` and within it, create a new file named `types.ts`. This file will contain the core TypeScript type definitions for the Ironclad system's data structures.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/core",
                "types.ts",
                "Ironclad system's data structures"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is consistent with the project's file map, which places the tool's entry point at `src/index.ts`, thus implying other tool source files belong in `src`. This action does, however, highlight a minor conflict with the `src` directory's description ('Contains all generated source code') in the constitution. The plan correctly follows the more specific file path definition.",
              "sequence_critique": "The sequence is logical. Creating the file is the correct first action for this task.",
              "clarity_critique": "The instruction is clear, specific, and directly actionable."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `src/core/types.ts`, define the hierarchy of interfaces for the `ModuleContract`. Refer to the JSON schema and example in Appendix A.2 of the architecture document. Create and export the following interfaces:\n1. `Parameter`: `{ name: string; type: string; }`\n2. `FunctionSignature`: `{ name: string; parameters: Parameter[]; returnType: string; description?: string; }`\n3. `DataStructure`: `{ name: string; properties: Parameter[]; }`\n4. `ModuleInstructions`: `{ overview?: string; steps?: string[]; edgeCases?: string[]; }`\n5. `ModuleContract`: `{ name: string; purpose: string; publicAPI: string[]; dependencies: string[]; constructorParams: string[]; functionSignatures: FunctionSignature[]; dataStructures?: DataStructure[]; instructions?: ModuleInstructions; }`\nAdd JSDoc comments to each interface explaining its purpose.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/core/types.ts",
                "ModuleContract",
                "Appendix A.2 of the architecture document",
                "Parameter",
                "FunctionSignature",
                "DataStructure",
                "ModuleInstructions"
              ],
              "technology_hints": [
                "TypeScript",
                "JSON schema",
                "JSDoc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. It directly translates the `ModuleContract` JSON schema from the architecture document's Appendix A.2 into TypeScript interfaces, which is a foundational requirement for the IBA to parse and process contract files as defined in the Project Constitution.",
              "sequence_critique": "The sequence is logical. It defines the smaller, constituent interfaces (`Parameter`, `FunctionSignature`) before composing them into the main `ModuleContract` interface. This bottom-up approach is correct and standard practice.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify the exact file path, interface names, and property definitions, including optionality, and correctly reference the source of truth in the architecture document. An AI agent can execute this step without ambiguity."
            }
          }
        },
        {
          "id": "step_3",
          "description": "In `src/core/types.ts`, define the types related to failure reporting. Refer to Section 4 ('Failure Reports') and Appendix A.2 ('final_failure.json') of the architecture document. Create and export the following:\n1. A string literal union type `ValidatorType`: `'json-schema' | 'tsc' | 'dsl' | 'edge-case' | 'test-runner'`. This represents the different validation stages.\n2. An interface `FailureEntry`: `{ attempt: number; timestamp: string; validator: ValidatorType; message: string; details: object; }`. This represents a single validation failure within a Job's retry loop.\n3. An interface `FinalFailureReport`: `{ module: string; attempts: number; lastPromptHash: string; errorSummary: string; }`. This is the summary report for a single job that permanently failed.\n4. A type alias `AggregatedFailureReport`: `FinalFailureReport[]`. This represents the top-level `.ironclad_failures.json` file content.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/core/types.ts",
                "Section 4 ('Failure Reports')",
                "Appendix A.2 ('final_failure.json')",
                "ValidatorType",
                "FailureEntry",
                "FinalFailureReport",
                "AggregatedFailureReport",
                ".ironclad_failures.json"
              ],
              "technology_hints": [
                "TypeScript",
                "tsc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the task and the Project Constitution. They directly translate the specifications for `FailureReport` from Section 4 and Appendix A.2 of the architecture document into the required TypeScript types.",
              "sequence_critique": "The sequence is logical. It defines the primitive string literal union `ValidatorType` first, which is then used by the `FailureEntry` interface. The progression from individual failure entries to final and aggregated reports is sound.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify the exact file, type names, and property definitions, referencing the source sections in the architecture document, which removes ambiguity for the coding agent."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Review the `src/core/types.ts` file for completeness and accuracy. Cross-reference every field in your defined interfaces with the architecture document, especially Section 4 and Appendix A.2. Ensure all types and interfaces are exported and have clear JSDoc comments. If a `tsconfig.json` file is present, run a type check (e.g., `npx tsc --noEmit --project tsconfig.json`) to validate the syntax and correctness of the new type definitions.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "run a type check (e.g., `npx tsc --noEmit --project tsconfig.json`)"
              ],
              "key_entities_dependencies": [
                "src/core/types.ts",
                "architecture document",
                "Section 4",
                "Appendix A.2",
                "ModuleContract",
                "tsconfig.json"
              ],
              "technology_hints": [
                "TypeScript",
                "JSDoc",
                "npx",
                "tsc"
              ]
            },
            "step_critique": {
              "alignment_critique": "Steps are perfectly aligned. They directly enforce the data models (`ModuleContract`, `FailureReport`) defined in the Project Constitution's `key_data_structures`, Section 4, and Appendix A.2. The use of `tsc` and the requirement for JSDoc also align with the `primary_language_and_tech_stack` and `Maintainability` NFRs.",
              "sequence_critique": "The sequence is logical for a code review task: first, a manual/semantic check against the specification, followed by an automated syntactic and type correctness check. No issues found.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify which file to review, precisely which sections of the architecture document to use as a reference, and provide an exact command for validation. No ambiguity is present."
            }
          }
        }
      ],
      "Task 1.3: Implement a JSON schema validator for `contracts/<ModuleName>.json` files.": [
        {
          "id": "step_1",
          "description": "Install the `ajv` library, which is a high-performance JSON schema validator. This will be the core dependency for validating the module contracts. Execute `npm install ajv`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install the `ajv` library",
                "Execute `npm install ajv`"
              ],
              "key_entities_dependencies": [
                "module contracts"
              ],
              "technology_hints": [
                "ajv",
                "npm",
                "JSON schema"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. The Project Constitution explicitly lists a 'JSON Schema Validator (e.g., ajv)' as a required dependency to validate `ModuleContract` files, which is a core responsibility of the IBA component (Section 3.1).",
              "sequence_critique": "The sequence is logical. Installing the necessary library is the correct prerequisite step before writing any code that uses it.",
              "clarity_critique": "The step is clear and unambiguous. It specifies the exact library to install and the precise command to execute, leaving no room for misinterpretation."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a new file at `src/schemas/moduleContract.schema.ts`. In this file, define and export a constant named `moduleContractSchema`. The value of this constant should be the JSON schema object for a `ModuleContract`, as specified in 'Appendix A.2' of the architecture document. Ensure the schema is a valid JSON Schema draft 2020-12 object.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/schemas/moduleContract.schema.ts",
                "moduleContractSchema",
                "ModuleContract",
                "Appendix A.2"
              ],
              "technology_hints": [
                "JSON Schema draft 2020-12"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Creating the schema object is a foundational and necessary action to implement a validator for `ModuleContract` files, directly supporting the IBA's responsibility to validate contracts as defined in the Project Constitution (Section 3.1).",
              "sequence_critique": "The sequence is logical. Defining the schema is the necessary first step before creating a validator function that consumes it.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify the exact file path, constant name, and an unambiguous reference ('Appendix A.2') to the source content, leaving no room for misinterpretation."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Create a new file for the validator logic at `src/iba/validators/contractValidator.ts`. Import `Ajv` from the `ajv` library and the `moduleContractSchema` constant from `src/schemas/moduleContract.schema.ts`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/validators/contractValidator.ts",
                "Ajv",
                "moduleContractSchema",
                "src/schemas/moduleContract.schema.ts"
              ],
              "technology_hints": [
                "ajv"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It directly supports Task 1.3 by setting up the file and dependencies for validating `ModuleContract` files. This is a core responsibility of the Ironclad Blueprint Architect (IBA) as defined in the constitution (Section 3.1) and utilizes the prescribed `ajv` dependency.",
              "sequence_critique": "The sequence is logical. Creating a file and importing its dependencies is the correct foundational step for implementing the validator logic. The step correctly assumes the `moduleContractSchema` has been defined in a prerequisite step.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify exact file paths, library names, and the names of constants to import, leaving no ambiguity for an AI coding agent."
            }
          }
        },
        {
          "id": "step_4",
          "description": "In `src/iba/validators/contractValidator.ts`, define and export an asynchronous function `validateModuleContract(contractData: unknown): Promise<{ isValid: boolean; errors: any[] | null }>`. This function should initialize an `Ajv` instance, compile the `moduleContractSchema`, and use it to validate the `contractData`. It should return an object indicating validity and providing detailed errors from `ajv.errors` if validation fails.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/validators/contractValidator.ts",
                "validateModuleContract",
                "contractData",
                "moduleContractSchema",
                "ajv.errors"
              ],
              "technology_hints": [
                "Ajv"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal of creating a JSON schema validator. It directly implements a core function for the Ironclad Blueprint Architect (IBA) as described in the project constitution (Section 3.1), which mandates the validation of module contract files.",
              "sequence_critique": "The internal logic described (initialize Ajv, compile schema, validate) is in the correct sequence. The step correctly assumes a `moduleContractSchema` is available, which would be a logical prerequisite.",
              "clarity_critique": "The step is clear and highly actionable. It specifies the file path, function signature, return type, and implementation details (using `Ajv` and `ajv.errors`) with sufficient precision for an AI coding agent to execute."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Create a test fixture directory `__tests__/fixtures/contracts`. Inside this directory, create two files: `validContract.json` and `invalidContract.json`. Populate `validContract.json` with a contract that strictly adheres to the schema (using the example in Appendix A.2 is a good start). Populate `invalidContract.json` with a contract that violates the schema in at least three ways (e.g., missing a required field like 'purpose', a 'name' that doesn't match the pattern, and a 'dependencies' field that is a string instead of an array).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Consult Appendix A.2 for a valid contract example"
              ],
              "key_entities_dependencies": [
                "__tests__/fixtures/contracts",
                "validContract.json",
                "invalidContract.json",
                "Appendix A.2",
                "purpose",
                "name",
                "dependencies"
              ],
              "technology_hints": [
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the task of implementing a JSON schema validator. Creating valid and invalid test fixtures based on the project's defined schema (Appendix A.2) is a fundamental prerequisite for this task and directly supports the IBA's core responsibility of validating contracts.",
              "sequence_critique": "The sequence is logical. Creating test assets before or concurrently with writing the test code that uses them is a standard and effective development practice. No prerequisite steps are missing for this action.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify exact file paths, file names, the source for the valid data, and provide concrete, schema-consistent examples of violations for the invalid data. This level of detail is ideal for an AI coding agent."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Create a new test file `src/iba/validators/contractValidator.test.ts`. Write unit tests for the `validateModuleContract` function using a testing framework like Jest. Import the test fixture files you created. Your tests should cover: 1. A valid contract passing validation. 2. An invalid contract failing validation. 3. Asserting that the returned `errors` array for the invalid contract is not null and contains specific, expected error messages.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/validators/contractValidator.test.ts",
                "validateModuleContract",
                "test fixture files",
                "errors"
              ],
              "technology_hints": [
                "Jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "Steps are perfectly aligned with the task. Testing the contract validator is a critical part of implementing it, directly supporting the IBA's responsibility to 'Parse and validate all individual module contract files' as defined in Section 3.1 of the architecture document. This also adheres to the project's Testing Strategy (Section 12).",
              "sequence_critique": "The sequence is logical. It correctly outlines the standard procedure for creating unit tests: create the test file, write tests for specific scenarios (valid/invalid cases), and assert on the outcomes.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify the exact file path, the function to test, the testing framework, and provide explicit requirements for the test cases, including how to assert on the error output. This level of detail is ideal."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Refactor the `contractValidator.ts` and its test file for clarity and maintainability. Add JSDoc comments to the `validateModuleContract` function explaining its purpose, parameters, and return value. Ensure the Ajv instance is created with sensible options, for example `new Ajv({ allErrors: true })` to get all validation errors.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "contractValidator.ts",
                "validateModuleContract",
                "Ajv"
              ],
              "technology_hints": [
                "JSDoc",
                "Ajv"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task and project constitution. Refactoring for clarity and adding JSDoc directly supports the 'Maintainability' NFR. Configuring the validator to return all errors supports the 'Reliability' NFR by enabling the creation of detailed 'FailureReport' data structures, which is a core system concept.",
              "sequence_critique": "The sequence is logical. The actions described (refactoring, documenting, and configuring) are cohesive steps toward hardening and improving the quality of the validator component, which would naturally follow an initial implementation.",
              "clarity_critique": "The instructions are very clear and actionable. The step provides a specific function name (`validateModuleContract`), details the required content for the JSDoc, and gives a concrete code example (`new Ajv({ allErrors: true })`) for the configuration, leaving little room for ambiguity."
            }
          }
        }
      ],
      "Task 1.4: Implement file parsers for `ARCHITECTURE_SPEC.md` (to get module list) and `contracts/*.json`.": [
        {
          "id": "step_1",
          "description": "Create the necessary directory structure and files for the IBA parsers. Create the directory `src/iba/` and inside it, create three empty files: `types.ts`, `parsers.ts`, and `parsers.test.ts`. These files will house the type definitions, parser logic, and unit tests, respectively.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/",
                "types.ts",
                "parsers.ts",
                "parsers.test.ts"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step partially aligns by creating the `src/iba/parsers.ts` and `parsers.test.ts` files, which directly support the task. However, creating `src/iba/types.ts` contradicts the Project Constitution. The constitution explicitly defines `src/core/types.ts` for core system types like `ModuleContract`. Placing these shared type definitions within the `iba` component would violate the established file structure and the 'Maintainability' requirement for minimizing coupling.",
              "sequence_critique": "The sequence of creating a directory and then the necessary files within it is logical.",
              "clarity_critique": "The instructions are clear and specific enough for an AI agent to execute."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Install the necessary dependencies for parsing and validation. Using your package manager (e.g., npm or yarn), install `zod` for schema validation, and `remark`, `remark-parse`, and `unist-util-visit` for robust Markdown parsing. Also install development dependencies for testing: `jest`, `ts-jest`, and `@types/jest`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "install `zod` for schema validation",
                "install `remark`",
                "install `remark-parse`",
                "install `unist-util-visit` for robust Markdown parsing",
                "install development dependencies for testing: `jest`, `ts-jest`, and `@types/jest`"
              ],
              "key_entities_dependencies": [],
              "technology_hints": [
                "npm",
                "yarn",
                "zod",
                "remark",
                "remark-parse",
                "unist-util-visit",
                "Markdown",
                "jest",
                "ts-jest",
                "@types/jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The choice of dependencies (`zod` for schema validation, `remark` for Markdown parsing, and `jest` for testing) directly supports the task of implementing parsers for JSON and Markdown files. These packages are consistent with the project's TypeScript/Node.js tech stack and the testing strategy outlined in the Project Constitution.",
              "sequence_critique": "The step is logically sequenced. Installing required libraries is the necessary prerequisite before starting the implementation of the file parsers.",
              "clarity_critique": "The step is clear and actionable. It explicitly names the packages to be installed and correctly categorizes them as production or development dependencies, leaving no ambiguity for execution."
            }
          }
        },
        {
          "id": "step_3",
          "description": "In `src/iba/types.ts`, define the `ModuleContract` schema and type using `zod`. Refer to the JSON schema in section `A.2` of the architecture document. Translate this schema into a `zod` object schema. Export both the `zod` schema object (e.g., `ModuleContractSchema`) and the inferred TypeScript type (e.g., `export type ModuleContract = z.infer<typeof ModuleContractSchema>;`). Ensure all fields from the example contract are represented, including nested objects and arrays.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Refer to the JSON schema in section `A.2` of the architecture document."
              ],
              "key_entities_dependencies": [
                "src/iba/types.ts",
                "ModuleContract",
                "ModuleContractSchema",
                "z.infer<typeof ModuleContractSchema>"
              ],
              "technology_hints": [
                "zod",
                "TypeScript",
                "JSON schema"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Defining the `ModuleContract` type and schema is a fundamental prerequisite for the task of implementing a parser for `contracts/*.json` files.",
              "sequence_critique": "The logical sequence is correct. The data structure must be defined before the parsing logic that will operate on it can be implemented.",
              "clarity_critique": "The instruction is clear but creates a conflict with the `project_file_map` in the Project Constitution. It directs the implementation to `src/iba/types.ts`, whereas the constitution specifies `src/schemas/moduleContract.schema.ts` for the schema definition and `src/core/types.ts` for core types like `ModuleContract`. The step should be revised to align with the established file structure to maintain architectural consistency."
            }
          }
        },
        {
          "id": "step_4",
          "description": "In `src/iba/parsers.ts`, implement the JSON contract parser. Create an async function `parseModuleContract(filePath: string): Promise<ModuleContract>`. This function should: 1. Read the file content from `filePath` using Node.js's `fs/promises`. 2. Parse the content as JSON. 3. Use the `ModuleContractSchema.parse()` method from `zod` to validate the object's structure. 4. Return the validated contract object. Implement robust error handling for file-not-found, invalid JSON, and schema validation errors, throwing specific, informative errors for each case.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/parsers.ts",
                "parseModuleContract",
                "ModuleContract",
                "ModuleContractSchema",
                "ModuleContractSchema.parse()"
              ],
              "technology_hints": [
                "Node.js",
                "fs/promises",
                "JSON",
                "zod"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task of implementing a parser for `contracts/*.json`. This is a foundational responsibility of the Ironclad Blueprint Architect (IBA) as detailed in the Project Constitution (Section 3.1), which requires parsing and validating all module contract files.",
              "sequence_critique": "The sequence of operations\u2014read file, parse JSON, validate against schema, and then return\u2014is logical and correct. No steps are missing or out of order for processing a single file.",
              "clarity_critique": "The instructions are clear, specific, and fully actionable. The prompt specifies the target file, function signature, technology to use (Node.js `fs/promises`, `zod`), and precise error handling requirements, leaving no ambiguity for execution."
            }
          }
        },
        {
          "id": "step_5",
          "description": "In `src/iba/parsers.test.ts`, write unit tests for the `parseModuleContract` function. Create a temporary `test-data/contracts` directory for your test files. Add at least three test cases: 1. A valid `valid-contract.json` that conforms to the schema. 2. An `invalid-syntax.json` file with a JSON syntax error. 3. A `invalid-schema.json` file that is valid JSON but fails schema validation (e.g., missing a required 'purpose' field). Use Jest to assert that the function returns the correct object for the valid case and throws the expected errors for the invalid cases.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/parsers.test.ts",
                "parseModuleContract",
                "test-data/contracts",
                "valid-contract.json",
                "invalid-syntax.json",
                "invalid-schema.json",
                "purpose"
              ],
              "technology_hints": [
                "Jest",
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the task. Writing unit tests for the parser, including valid, syntax-error, and schema-error cases, directly addresses the need to create a robust contract parser as part of the IBA implementation.",
              "sequence_critique": "The sequence is logical for a test-driven development approach. However, the step specifies creating a temporary `test-data/contracts` directory. This contradicts the `Project Constitution`'s `project_file_map`, which explicitly defines a `__tests__/fixtures/contracts` directory for this purpose. The test fixture files should be created in the constitutionally-defined location to maintain architectural consistency.",
              "clarity_critique": "The steps are very clear and actionable. The function to be tested (`parseModuleContract`), the required test cases, and the expected outcomes (successful parsing vs. throwing errors) are explicitly defined, making the instructions unambiguous for an AI agent."
            }
          }
        },
        {
          "id": "step_6",
          "description": "In `src/iba/parsers.ts`, implement the `ARCHITECTURE_SPEC.md` parser. Create an async function `parseArchitectureSpecForModuleList(filePath: string): Promise<string[]>`. This function should: 1. Read the file content. 2. Use `remark` to parse the Markdown into an AST. 3. Use `unist-util-visit` to traverse the AST, find a heading with the text 'Modules' (assume a specific level, e.g., h2 or h3), and extract the text content of all list items immediately following that heading. 4. Return an array of these module names. Handle cases where the file or the 'Modules' section is not found by returning an empty array or throwing an error.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/parsers.ts",
                "ARCHITECTURE_SPEC.md",
                "parseArchitectureSpecForModuleList"
              ],
              "technology_hints": [
                "remark",
                "unist-util-visit",
                "Markdown",
                "AST"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task of creating parsers for the IBA. The `ARCHITECTURE_SPEC.md` is the primary input file, and extracting the module list from it is a foundational requirement for the IBA as defined in the project constitution (Section 3.1).",
              "sequence_critique": "The internal sequence of operations (read file, parse to AST, traverse AST, return data) is logical and correct for processing a structured text file.",
              "clarity_critique": "The instruction is highly clear and actionable, specifying the function signature, file location, and recommended libraries. The error handling directive to 'return an empty array or throw an error' presents an ambiguous choice. To align with the constitution's emphasis on strictness, it should be more prescriptive, recommending to throw a specific error if the required 'Modules' section is missing, as this constitutes a fatal validation failure of a core input file."
            }
          }
        },
        {
          "id": "step_7",
          "description": "In `src/iba/parsers.test.ts`, write unit tests for `parseArchitectureSpecForModuleList`. Create a `test-data/specs` directory. Add at least three test cases: 1. A `valid-spec.md` with a '## Modules' section and a bulleted list of module names. 2. A `no-modules-section.md` file that lacks the modules section. 3. An `empty-list.md` with the heading but no list items. Assert that the function returns the correct array of names for the valid case and an empty array for the other cases.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/parsers.test.ts",
                "parseArchitectureSpecForModuleList",
                "test-data/specs",
                "valid-spec.md",
                "no-modules-section.md",
                "empty-list.md"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step logically contributes to the task of implementing the IBA's parsers by following a test-driven approach. However, the instruction to create a `test-data/specs` directory is inconsistent with the `project_file_map` in the Project Constitution, which establishes a top-level `__tests__/fixtures/` directory for test data. The directory should be `__tests__/fixtures/specs` to maintain consistency.",
              "sequence_critique": "The sequence is logical. It follows a standard Test-Driven Development (TDD) pattern: set up test data, write tests covering valid and edge cases, and then assert the expected outcomes.",
              "clarity_critique": "The steps are clear and actionable. The function to be tested, the test cases, and the expected assertions are explicitly defined."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Review and refine the implemented code. Add JSDoc comments to all new functions and exported types explaining their purpose, parameters, and return values. Ensure all code conforms to strict TypeScript standards and is formatted consistently. Run all tests to confirm that all functionality is working as expected.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [],
              "technology_hints": [
                "JSDoc",
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are well-aligned with the task of implementing robust and maintainable file parsers. Adding documentation, enforcing strict standards, and running tests directly support the project's core mission of producing high-quality, validated software and the non-functional requirement for maintainability.",
              "sequence_critique": "The sequence is logical. This step acts as a quality assurance and refinement stage after the initial code implementation, which is the correct order for ensuring robustness and correctness.",
              "clarity_critique": "The instructions are clear, specific, and actionable. The breakdown into adding JSDoc, conforming to standards, formatting, and running tests provides a precise checklist for an AI agent to execute."
            }
          }
        }
      ],
      "Task 1.5: Implement the module dependency graph construction logic based on contract `dependencies`.": [
        {
          "id": "step_1",
          "description": "Create a new file at `src/iba/dependency-graph.ts`. Inside this file, define the necessary types for representing the graph and the module contracts. Specifically, add a minimal `ModuleContract` interface and an `AdjacencyList` type alias. Hint: Use `export interface ModuleContract { name: string; dependencies: string[]; }` and `export type AdjacencyList = Map<string, string[]>;`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/dependency-graph.ts",
                "ModuleContract",
                "AdjacencyList"
              ],
              "technology_hints": [
                "Map"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with Task 1.5. Creating the foundational types (`ModuleContract`, `AdjacencyList`) is a necessary prerequisite for building the dependency graph. The use of a minimal `ModuleContract` interface, containing only the fields relevant to this task (`name`, `dependencies`), is an excellent design choice consistent with the 'Maintainability' NFR (minimal coupling).",
              "sequence_critique": "The sequence is logical. Defining the core data structures is the correct first step before implementing the functions that will operate on them.",
              "clarity_critique": "The instructions are exceptionally clear and actionable, providing the exact file path, type names, and even the precise code snippets to be implemented. This leaves no room for ambiguity."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `src/iba/dependency-graph.ts`, define an exported class named `DependencyGraph`. This class should encapsulate the graph structure. It should have a private readonly property `graph` of type `AdjacencyList` initialized in the constructor, and a public method `getAdjacencyList(): AdjacencyList` that returns the graph.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/dependency-graph.ts",
                "DependencyGraph",
                "AdjacencyList",
                "graph",
                "getAdjacencyList"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal. Creating a dedicated `DependencyGraph` class is a logical and maintainable approach to fulfilling the IBA's responsibility of constructing and validating the module dependency graph, as outlined in section 3.1 of the constitution.",
              "sequence_critique": "The sequence is logical as a foundational step. However, it omits the definition of the `AdjacencyList` type itself. A prerequisite step to define this type (e.g., `type AdjacencyList = Map<string, string[]>;`) within the same file would make the overall task more complete and less ambiguous for the agent.",
              "clarity_critique": "The instructions are clear regarding the file path, class name, property, and method. The lack of a specific definition for the `AdjacencyList` type is a minor ambiguity, requiring the agent to infer a suitable structure, which could be made more explicit."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Implement a public static method `build(contracts: Map<string, ModuleContract>): DependencyGraph` on the `DependencyGraph` class. This factory method will be responsible for creating and returning a new `DependencyGraph` instance. For now, you can leave the implementation empty, just return a new `DependencyGraph` with an empty map.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "build",
                "contracts",
                "Map",
                "ModuleContract",
                "DependencyGraph"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Creating a static factory method with a placeholder implementation is a logical and foundational first step towards the overall task of implementing the dependency graph construction logic.",
              "sequence_critique": "The step is a correct and logical starting point for the task. It establishes the method that will later contain the full construction logic, making it a necessary prerequisite for subsequent implementation steps.",
              "clarity_critique": "The instructions are clear and highly actionable. They precisely define the method signature, its purpose as a factory, and the exact placeholder implementation required, leaving no ambiguity for an AI coding agent."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Implement the core logic within the `DependencyGraph.build` method. It should iterate through the provided `contracts` map. For each contract, add the module name as a key to a new `AdjacencyList`. Then, iterate through the `dependencies` array of that contract and populate the adjacency list. For example, if module 'A' depends on 'B' and 'C', the map entry should be `['A', ['B', 'C']]`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "DependencyGraph.build",
                "contracts",
                "AdjacencyList",
                "dependencies"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Building an adjacency list from the `dependencies` array in module contracts is the foundational action required to construct the dependency graph, a core responsibility of the IBA as specified in the Project Constitution (Section 3.1).",
              "sequence_critique": "The logical sequence is correct. It correctly assumes a map of contracts as input and describes the standard, efficient algorithm for building an adjacency list representation of the graph.",
              "clarity_critique": "The instruction is clear and unambiguous. It specifies the method, the data structures involved, and provides a concrete example (`['A', ['B', 'C']]`) that perfectly clarifies the expected structure of the adjacency list and the direction of the dependency relationship."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Enhance the `build` method to include validation. While iterating through the dependencies of a module, check if each dependency name exists as a key in the input `contracts` map. If a dependency is not found, throw an `Error` with a precise message, such as `Unresolved dependency: Module 'ModuleA' depends on 'ModuleB', but 'ModuleB' is not a defined contract.`",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "build method",
                "dependencies",
                "contracts map",
                "Error"
              ],
              "technology_hints": [
                "Error"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Validating the existence of all declared dependencies is a fundamental part of constructing a valid dependency graph, a core responsibility of the IBA as outlined in section 3.1 of the Project Constitution. This supports the NFR for 'Reliability (Input Integrity)' by catching configuration errors early.",
              "sequence_critique": "The sequence is logical. This check for unresolved dependencies must occur as the contracts are being processed to build the graph, and it is a necessary prerequisite before more complex validations like cycle detection can be performed.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. The step specifies the exact check to perform, the data structure to reference (the 'contracts' map), the failure action (throwing an Error), and provides a precise, well-formatted example of the required error message."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Create a new test file `src/iba/dependency-graph.test.ts`. Import the `DependencyGraph` class and the `ModuleContract` interface. Set up a `describe` block for `DependencyGraph` and prepare mock `ModuleContract` data for your tests. Hint: Use Jest as the test runner.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/dependency-graph.test.ts",
                "DependencyGraph",
                "ModuleContract"
              ],
              "technology_hints": [
                "Jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task of implementing the dependency graph logic. Creating a test file first is a standard practice in test-driven development, ensuring that the resulting implementation is robust and verifiable, which directly supports the project's core mission of reliability.",
              "sequence_critique": "The sequence is logical. Establishing the test harness by creating the test file, setting up the describe block, and preparing mock data is the correct foundational step before writing the actual test cases and the implementation logic itself.",
              "clarity_critique": "The instructions are clear, specific, and actionable. The step correctly identifies the file to be created, the necessary imports (`DependencyGraph`, `ModuleContract`), and the testing framework (Jest), all of which are consistent with the Project Constitution."
            }
          }
        },
        {
          "id": "step_7",
          "description": "In `dependency-graph.test.ts`, write unit tests for the 'happy path' scenarios of the `build` method. Cover the following cases: 1. A simple graph with a few interconnected modules. 2. A module with no dependencies. 3. An empty map of contracts. For each case, assert that the `getAdjacencyList()` method returns the expected `Map` structure.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "dependency-graph.test.ts",
                "build",
                "getAdjacencyList"
              ],
              "technology_hints": [
                "Map"
              ]
            },
            "step_critique": {
              "alignment_critique": "Steps are perfectly aligned. Creating unit tests for the dependency graph construction directly supports Task 1.5 and fulfills a core responsibility of the IBA outlined in the architecture (Section 3.1), which is essential for ensuring the system's reliability.",
              "sequence_critique": "The sequence is logical. Following a test-first or TDD approach, creating tests for the 'happy path' scenarios is the correct initial step before implementing the core logic or handling error conditions like cycle detection.",
              "clarity_critique": "The instructions are clear and actionable. The prompt specifies the file, the method to test, the exact scenarios to cover, and the specific assertion to make (`getAdjacencyList()`), which is sufficiently precise for an AI agent to execute correctly."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Write a unit test specifically for the validation logic. Create a set of mock contracts where one module has a dependency that is not present in the contract map. Use `expect(...).toThrow()` to assert that the `build` method throws an error and that the error message matches the expected format for an unresolved dependency.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "validation logic",
                "mock contracts",
                "contract map",
                "build method"
              ],
              "technology_hints": [
                "expect(...).toThrow()"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Testing for unresolved dependencies is a critical validation aspect of the 'module dependency graph construction' task, directly fulfilling the IBA's responsibility to validate inputs as defined in the Project Constitution (Section 3.1).",
              "sequence_critique": "The sequence is logical. Writing a unit test for a failure case is a fundamental part of implementing robust logic, consistent with a Test-Driven Development approach or standard validation practices.",
              "clarity_critique": "The step is highly clear and actionable. It precisely describes the test setup (mock contracts with a missing dependency), the assertion mechanism (`toThrow`), and the verification target (a specific error message format), leaving no ambiguity for the developer."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Finally, add comprehensive JSDoc comments to the `DependencyGraph` class, its constructor, the `build` method, and the `getAdjacencyList` method. Clearly document what each part does, its parameters, and what it returns or throws. This is crucial for maintainability as per the project constitution.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "DependencyGraph",
                "constructor",
                "build",
                "getAdjacencyList"
              ],
              "technology_hints": [
                "JSDoc"
              ]
            },
            "step_critique": {
              "alignment_critique": "This step is well-aligned. The Project Constitution explicitly requires 'Maintainability', and providing clear JSDoc comments for core data structures like the dependency graph directly supports this non-functional requirement.",
              "sequence_critique": "The sequence is logical. The use of 'Finally' correctly positions this documentation step as the concluding action for the implementation of the `DependencyGraph` class within the broader task.",
              "clarity_critique": "The step is clear and highly actionable. It specifies the exact class and methods to document and details the required content (purpose, params, returns, throws), leaving no room for ambiguity."
            }
          }
        }
      ],
      "Task 1.6: Implement a Directed Acyclic Graph (DAG) cycle detection algorithm and integrate it into the validation flow.": [
        {
          "id": "step_1",
          "description": "Create a new file at `src/iba/graph-validator.ts` to house the dependency graph validation logic. Also, create a corresponding test file at `src/iba/graph-validator.test.ts`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/graph-validator.ts",
                "src/iba/graph-validator.test.ts"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "This step is misaligned with the Project Constitution. While the goal of creating validation logic is correct for Task 1.6, the constitution already designates `src/iba/dependency-graph.ts` as the location for 'constructing and validating the module dependency graph'. Creating a new, separate `src/iba/graph-validator.ts` file would fragment the logic and contradict the established file map. The DAG validation logic should be implemented within the existing `DependencyGraph` class located in `src/iba/dependency-graph.ts`.",
              "sequence_critique": "The sequence is logical; creating files is a necessary first step before adding code to them.",
              "clarity_critique": "The instruction is clear and actionable, specifying the exact file paths to be created."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `src/iba/graph-validator.ts`, define the necessary data structures. Create a type alias for the graph's adjacency list representation: `export type AdjacencyList = Map<string, string[]>;`. Also, define a custom error class `export class CycleError extends Error` that accepts a `path` (string array) in its constructor to report the detected cycle.",
          "hints": [
            "The constructor for CycleError should format a user-friendly message like `Dependency cycle detected: A -> B -> C -> A`."
          ],
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/graph-validator.ts",
                "AdjacencyList",
                "CycleError",
                "Error",
                "path"
              ],
              "technology_hints": [
                "TypeScript",
                "Map"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal of defining data structures for cycle detection aligns perfectly with the task. However, the proposed file path `src/iba/graph-validator.ts` and the re-definition of `AdjacencyList` conflict with the Project Constitution. Task 1.5 already established `src/iba/dependency-graph.ts` as the location for graph logic and defined the `AdjacencyList` type. To maintain component cohesion and avoid redundancy, the new `CycleError` class should be added to `src/iba/dependency-graph.ts`.",
              "sequence_critique": "The sequence is logical. Defining necessary data structures and custom errors before implementing the algorithm that uses them is a correct prerequisite step.",
              "clarity_critique": "The instructions are clear in what to create. However, they are flawed because they direct the creation of a new file and a redundant type, contradicting the established project structure. The instruction should be to add the `CycleError` to the existing `src/iba/dependency-graph.ts` file and to use (or import) the `AdjacencyList` type from its canonical definition."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Implement the core DAG cycle detection function in `graph-validator.ts`. This function will perform a Depth First Search (DFS) traversal. Name it `findCycle(graph: AdjacencyList): string[] | null`.",
          "hints": [
            "Use two sets to track node states during traversal: `visiting` (for nodes in the current recursion stack) and `visited` (for nodes that have been completely explored).",
            "The function should iterate through all nodes in the graph. For each unvisited node, it should call a recursive helper function, e.g., `dfs(node: string, path: string[]): string[] | null`.",
            "Inside the `dfs` helper, if you encounter a node that is already in the `visiting` set, you have found a cycle. Return the path.",
            "If the entire graph is traversed without finding a cycle, `findCycle` should return `null`."
          ],
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "graph-validator.ts",
                "findCycle",
                "AdjacencyList"
              ],
              "technology_hints": [
                "Depth First Search (DFS)"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is directly aligned with the task's goal of implementing DAG cycle detection, which is a core responsibility of the Ironclad Blueprint Architect (IBA) as defined in the Project Constitution (Section 3.1).",
              "sequence_critique": "The sequence is logical. Implementing the core cycle detection function is a necessary first step before it can be integrated into the larger validation pipeline.",
              "clarity_critique": "The step proposes creating a new file `graph-validator.ts`. This deviates from the established pattern in the constitution, where the `DependencyGraph` class (defined in `src/iba/dependency-graph.ts`) is intended to encapsulate graph-related logic and validation. To improve cohesion and adhere to the established architecture, this function should be implemented as a method within the `DependencyGraph` class itself."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Create a public-facing validation function `validateDAG(graph: AdjacencyList): void` in `graph-validator.ts`. This function will call `findCycle`. If a cycle is found (the result is not null), it should throw a new `CycleError` with the returned cycle path. If no cycle is found, it should return void.",
          "hints": [
            "This function serves as the main entry point for the graph validation logic, abstracting the `findCycle` implementation detail."
          ],
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "validateDAG",
                "AdjacencyList",
                "graph-validator.ts",
                "findCycle",
                "CycleError"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task and the overall project goal. The Project Constitution, under Section 3.1 'Ironclad Blueprint Architect (IBA) Responsibilities', explicitly states a requirement to 'Construct and validate the module dependency graph ... to ensure it is a Directed Acyclic Graph (DAG). Reject input if cycles are detected.' This step directly implements that critical validation requirement.",
              "sequence_critique": "The sequence is logical. It correctly creates a higher-level, public-facing validation function that consumes the output of a lower-level cycle detection algorithm (`findCycle`) and translates it into a clear outcome (return void on success, throw a specific error on failure). This properly encapsulates the validation logic.",
              "clarity_critique": "The step is clear and actionable but could be improved by specifying the full path for the new file. To be consistent with the established `project_file_map` which contains `src/iba/validators/contractValidator.ts`, the new file should be located at `src/iba/validators/graph-validator.ts`. Adding this detail would prevent ambiguity."
            }
          }
        },
        {
          "id": "step_5",
          "description": "In `src/iba/graph-validator.test.ts`, write comprehensive unit tests for the `validateDAG` function using Jest. Your tests must cover multiple scenarios.",
          "hints": [
            "Scenario 1: A valid DAG (e.g., A -> B, B -> C). Assert that `validateDAG` does not throw an error.",
            "Scenario 2: A disconnected but valid DAG (e.g., A -> B, C -> D). Assert no error is thrown.",
            "Scenario 3: A simple direct cycle (A -> B, B -> A). Assert that `validateDAG` throws a `CycleError` and that the error message contains the correct path.",
            "Scenario 4: A longer cycle (A -> B -> C -> A). Assert that a `CycleError` is thrown with the correct path.",
            "Scenario 5: A self-referencing node (A -> A). Assert a `CycleError` is thrown.",
            "Scenario 6: An empty graph. Assert no error is thrown."
          ],
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/graph-validator.test.ts",
                "validateDAG"
              ],
              "technology_hints": [
                "Jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Writing unit tests for the `validateDAG` function is a critical part of implementing and verifying the DAG cycle detection algorithm, which is a core responsibility of the Ironclad Blueprint Architect (IBA) as defined in the constitution (Section 3.1).",
              "sequence_critique": "The step is logically sound. It represents a necessary testing activity for the `validateDAG` function, fitting well into either a traditional or Test-Driven Development workflow for Task 1.6.",
              "clarity_critique": "The instruction is clear, but the term 'comprehensive' could be made more specific to ensure robust testing. It would be beneficial to explicitly enumerate the required test scenarios, such as: a valid DAG, a simple 2-node cycle, a multi-node cycle, a graph with disconnected components (one with a cycle), an empty graph, and a graph with a single node."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Now, integrate the DAG validation into the IBA's main execution flow. Assuming there is a primary IBA orchestrator file (e.g., `src/iba/index.ts` or similar), import and use the `validateDAG` function.",
          "hints": [
            "After parsing all `ModuleContract` files and building the adjacency list representation of the dependencies, call `validateDAG` with the graph.",
            "Wrap the call in a `try...catch` block. If a `CycleError` is caught, log the error message to the console and terminate the IBA process with a non-zero exit code, as this is a critical blueprint failure."
          ],
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "validateDAG function",
                "src/iba/index.ts"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Integrating the already-created DAG validation logic into the main IBA orchestrator is the primary goal of Task 1.6. This directly fulfills the IBA's core responsibility, as stated in the constitution, to 'Construct and validate the module dependency graph ... Reject input if cycles are detected.'",
              "sequence_critique": "The sequence is logical. This step correctly occurs after the `DependencyGraph` has been built from parsed contracts and before any further blueprint artifacts (like interface stubs) are generated. This adheres to the fail-fast principle by validating the entire dependency structure before performing more work.",
              "clarity_critique": "The step is clear and actionable. It specifies what function to use (`validateDAG`) and suggests a likely location for the integration (`src/iba/index.ts` or similar), which is consistent with the project's file map. The instruction is specific enough for an AI agent to execute without ambiguity."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Add JSDoc comments to all new public functions and types (`AdjacencyList`, `CycleError`, `validateDAG`) in `src/iba/graph-validator.ts`. Ensure the comments clearly explain the purpose, parameters, return values, and any thrown errors, adhering to the project's documentation standards.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "AdjacencyList",
                "CycleError",
                "validateDAG",
                "src/iba/graph-validator.ts"
              ],
              "technology_hints": [
                "JSDoc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step directly supports the project's non-functional requirement for 'Maintainability' by ensuring new, critical validation logic is properly documented. This aligns perfectly with the project's quality standards and the goal of creating a well-engineered system.",
              "sequence_critique": "The sequence is logical. Adding documentation is a standard final step after implementing the core logic of a feature, ensuring the work is complete and understandable before being considered 'done'.",
              "clarity_critique": "The instruction is clear and actionable. However, it directs documentation for all listed items to be added within `src/iba/graph-validator.ts`. According to the Project Constitution (Task 1.5), `AdjacencyList` is a type alias associated with the `DependencyGraph` class in `src/iba/dependency-graph.ts`. The agent must ensure documentation is added at the canonical definition site of each type and function, which may not be the same file."
            }
          }
        }
      ],
      "Task 1.7: Implement the logic to generate TypeScript interface stub files (`src/modules/I<ModuleName>.ts`) from parsed `ModuleContract` data.": [
        {
          "id": "step_1",
          "description": "Create a new file at `src/iba/interface-generator.ts`. This file will contain the logic for generating TypeScript interface stubs from module contracts.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/interface-generator.ts",
                "ModuleContract"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Creating a dedicated file `src/iba/interface-generator.ts` for this logic is consistent with the IBA's responsibilities (Section 3.1) and the project's maintainability NFR, which calls for clear component responsibilities.",
              "sequence_critique": "The sequence is logical. Creating the file is the necessary first step before its contents can be implemented.",
              "clarity_critique": "The step is perfectly clear and actionable. It specifies the exact file path and its purpose, leaving no room for ambiguity."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `src/iba/interface-generator.ts`, define the main function signature: `export function generateInterfaceStub(contract: ModuleContract): string`. You will need to import the `ModuleContract` type, which should be defined in a shared types file (e.g., `src/types.ts`). Ensure this type definition matches the schema in the architecture document (Appendix A.2).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/interface-generator.ts",
                "generateInterfaceStub",
                "contract",
                "ModuleContract",
                "src/types.ts",
                "architecture document (Appendix A.2)"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task of creating the interface stub generation logic. Defining the function signature and its input type is a direct and necessary contribution to Task 1.7.",
              "sequence_critique": "The sequence is logical. Defining the function signature and establishing the data contract (the `ModuleContract` type) is the correct prerequisite before implementing the generation logic itself.",
              "clarity_critique": "The step is clear, but contains a minor inconsistency with the Project Constitution regarding the file path for type definitions. It suggests `src/types.ts`, whereas the constitution's `project_file_map` specifies `src/core/types.ts` as the canonical location for core data structures like `ModuleContract`."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Implement a helper function within `interface-generator.ts` to extract all unique, non-primitive type names from a contract's `functionSignatures`. This function should iterate through all parameters and return types, collecting type names like 'CreateUserDTO' and 'User', while ignoring built-in types like 'string', 'number', 'boolean', 'void', 'Promise<any>', etc. Name it `getRequiredTypes(contract: ModuleContract): string[]`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "interface-generator.ts",
                "functionSignatures",
                "CreateUserDTO",
                "User",
                "getRequiredTypes",
                "ModuleContract"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It implements a core piece of logic required to fulfill Task 1.7. Identifying the unique, non-primitive types within a module's contract is a necessary prerequisite for generating the correct `import` statements in the `I<ModuleName>.ts` interface stub file. This directly supports the IBA's responsibilities as outlined in the Project Constitution.",
              "sequence_critique": "The step is logically sequenced. Creating this helper function is a foundational action that precedes or is part of the main interface generation logic. It correctly isolates a piece of reusable logic before it's needed to assemble the final file content.",
              "clarity_critique": "The step is clear but could be more precise about handling generic type wrappers. While it correctly specifies ignoring primitives, it should explicitly state that the function must extract the base type from wrapped types like `Promise<User>`, `Array<CustomType>`, or `CustomType[]`, to ensure types like 'User' and 'CustomType' are correctly identified."
            }
          }
        },
        {
          "id": "step_4",
          "description": "In `generateInterfaceStub`, use the `getRequiredTypes` helper. If the resulting array of types is not empty, generate an import statement. Adhere to the project structure: the generated interface will be in `src/modules/`, so the import path to the shared `idl` directory should be relative, e.g., `import type { ... } from '../../idl';`. Assume a barrel file (`index.ts`) exists in the `idl` directory. Use `import type` for type-only imports. Hint: `const typeImports = getRequiredTypes(contract); if (typeImports.length > 0) { ... }`",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "generateInterfaceStub",
                "getRequiredTypes",
                "src/modules/",
                "idl",
                "index.ts",
                "typeImports",
                "contract"
              ],
              "technology_hints": [
                "TypeScript",
                "import type"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It directly contributes to the task of generating a valid TypeScript interface stub by ensuring that all necessary shared types from the `idl` directory are correctly imported.",
              "sequence_critique": "The logical sequence is correct. Determining and generating import statements is a necessary prerequisite before generating the body of the interface that uses those types. The step fits naturally within the overall `generateInterfaceStub` function.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. The step specifies the exact relative path (`../../idl`), the precise syntax to use (`import type`), the condition for execution, and even makes the important design assumption about a barrel file explicit. This level of detail is ideal for an AI agent."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Implement the logic to generate the method signatures. Iterate over `contract.functionSignatures`. For each signature: \n1. Generate a JSDoc block using the `description` property if it exists.\n2. Format the parameters string (e.g., `dto: CreateUserDTO, id: string`).\n3. Format the method signature string, ensuring the return type is wrapped in a `Promise<>` as specified in Appendix A.7. Example: `createUser(dto: CreateUserDTO): Promise<User>;`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "contract.functionSignatures",
                "description",
                "Promise<>",
                "Appendix A.7"
              ],
              "technology_hints": [
                "JSDoc",
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the task of generating method signatures for the TypeScript interface stub. They correctly map the `functionSignatures` from the `ModuleContract` to the required TypeScript interface format, including JSDoc generation, which supports the project's goal of producing high-quality, documented code.",
              "sequence_critique": "The sequence is logical. Generating the documentation block, then the parameter list, and finally combining them into the full method signature is a correct and standard approach for code generation.",
              "clarity_critique": "The steps are clear and actionable. The explicit instruction to wrap the return type in `Promise<>` and the direct reference to Appendix A.7 provide excellent clarity and remove ambiguity for the agent implementing this logic."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Assemble the final interface string in `generateInterfaceStub`. The structure should be:\n1. A top-level comment indicating the file is auto-generated.\n2. The generated import statements (if any).\n3. A JSDoc block for the interface itself, using the `contract.purpose` field.\n4. The `export interface I${contract.name} { ... }` declaration.\n5. The generated method signatures inside the interface body.\nReturn the complete, formatted string.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "generateInterfaceStub",
                "contract",
                "contract.purpose",
                "contract.name"
              ],
              "technology_hints": [
                "JSDoc",
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It directly implements a core responsibility of the Ironclad Blueprint Architect (IBA)\u2014generating interface stubs (`I<ModuleName>.ts`) from module contracts, as detailed in sections 3.1 and A.7 of the constitution.",
              "sequence_critique": "The described sequence of elements within the generated file string (comment, imports, JSDoc, interface, methods) is logical and follows standard TypeScript conventions.",
              "clarity_critique": "The step is mostly clear, but could be improved. Appendix A.7 shows generated methods returning a `Promise`, but the `ModuleContract` definition doesn't explicitly require it. The step should clarify that method return types from the contract should be wrapped in `Promise<>` to align with the example and standard service interface patterns. Additionally, specifying that the final string should be processed by a code formatter (like Prettier) would better ensure the 'high-quality' output goal."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Create a unit test file `src/iba/interface-generator.test.ts`. Use a testing framework like Jest. You will need to mock the `ModuleContract` object for your tests.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/interface-generator.test.ts",
                "ModuleContract"
              ],
              "technology_hints": [
                "Jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Creating a unit test file is a fundamental part of robustly implementing the interface generation logic, which is a core responsibility of the IBA as defined in the project architecture (Section 3.1) and testing strategy (Section 12).",
              "sequence_critique": "The sequence is logical, representing the first step in a Test-Driven Development (TDD) workflow. Creating the test file before or alongside the implementation file is a standard and sound practice.",
              "clarity_critique": "The instructions are clear and actionable. The file path `src/iba/interface-generator.test.ts` is consistent with the project's file map. The explicit mention of using Jest and mocking the `ModuleContract` provides sufficient and accurate guidance for an AI agent."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Write your first unit test. Use the 'UserService' example from Appendix A.2 of the architecture document as your mock contract data. Assert that the `generateInterfaceStub` function produces the exact expected output string, including imports for 'CreateUserDTO' and 'User', the interface JSDoc, and the two method signatures wrapped in `Promise`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "'UserService' example from Appendix A.2",
                "generateInterfaceStub",
                "CreateUserDTO",
                "User",
                "Promise"
              ],
              "technology_hints": [
                "unit test",
                "JSDoc",
                "Promise"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. It correctly uses Test-Driven Development principles by defining the expected outcome before implementation. It astutely synthesizes requirements from multiple sections of the constitution: using the `ModuleContract` from Appendix A.2 as the input source, while asserting an output structure (`Promise<...>`) consistent with the generated stub example in Appendix A.7. This demonstrates a correct interpretation of the project's architectural intent.",
              "sequence_critique": "The sequence is logical. Starting with a comprehensive unit test that defines the contract for the `generateInterfaceStub` function is an ideal first step for this task.",
              "clarity_critique": "The step is exceptionally clear and actionable. It specifies the exact input data to use, and explicitly lists the critical characteristics of the expected output string (imports, JSDoc, Promise-wrapped methods), leaving no ambiguity for the AI agent."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Add more unit tests to cover edge cases:\n- A contract with no `functionSignatures` (should produce an empty interface).\n- A contract whose methods only use primitive types (should produce no import statements).\n- A contract with a method that has no parameters.\n- A contract with a method that has a `void` return type (should result in `Promise<void>`).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "functionSignatures",
                "Promise<void>"
              ],
              "technology_hints": [
                "TypeScript",
                "Promise"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the task. Adding comprehensive unit tests for edge cases directly supports the implementation of a robust interface stub generator, which is a core responsibility of the IBA as defined in the Project Constitution (Sections 3.1, 12).",
              "sequence_critique": "The sequence is logical. As a set of test cases to be added, their internal order is not significant, and the step fits correctly as a refinement within the overall implementation task.",
              "clarity_critique": "The steps are exceptionally clear and actionable. Each test case specifies both the input condition (the nature of the `ModuleContract`) and the expected output behavior (the content of the generated interface), making the requirements unambiguous."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Finally, navigate to the main IBA entry point file (e.g., `src/iba/index.ts` or similar). Add a `// TODO:` comment indicating where the `generateInterfaceStub` function will be called for each contract and its output written to `src/modules/I<ModuleName>.ts`. This marks the integration point for a future task.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/index.ts",
                "generateInterfaceStub",
                "src/modules/I<ModuleName>.ts"
              ],
              "technology_hints": [
                "// TODO:"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is misaligned with the stated goal of Task 1.7, which is to 'Implement the logic to generate TypeScript interface stub files'. Adding a `// TODO:` comment is a planning or scaffolding action, not an implementation one. It defers the actual integration and use of the implemented logic to a future task, leaving the current task functionally incomplete.",
              "sequence_critique": "While placing this step 'Finally' is logical in the sequence of sub-steps, the action itself is a weak conclusion. A task focused on implementation should culminate in the implemented code being called and integrated into the main workflow, not just noted with a comment. This breaks the implementation flow and delays achieving the task's objective.",
              "clarity_critique": "The instruction is clear and actionable. The target file (`src/iba/index.ts`) is well-defined in the constitution, and the content of the comment is explicitly described."
            }
          }
        }
      ],
      "Task 1.8: Implement the `BlueprintLock` hashing mechanism for embedding SHA-256 integrity checks in file headers.": [
        {
          "id": "step_1",
          "description": "Create a new utility file at `src/utils/blueprintLock.ts` to house the hashing and verification logic. Also, create its corresponding test file at `src/utils/blueprintLock.test.ts`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/utils/blueprintLock.ts",
                "src/utils/blueprintLock.test.ts"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's purpose aligns perfectly with the task. However, the proposed file path `src/utils/blueprintLock.ts` introduces a directory (`src/utils`) not defined in the Project Constitution's `project_file_map`. To maintain consistency with the established architecture, this core utility should be placed within the `src/core` directory, which is designated for 'non-generated logic for the Ironclad tool itself'.",
              "sequence_critique": "The sequence is logical. Creating the implementation file and its corresponding test file is the correct initial step before implementing the logic.",
              "clarity_critique": "The instruction is clear, specifying the exact file paths and their purpose, making it fully actionable."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `src/utils/blueprintLock.ts`, import the `crypto` module from Node.js. Define and export two string constants: `BLUEPRINT_LOCK_HEADER_PREFIX` with the value 'BlueprintLock: sha256:' and `BLUEPRINT_LOCK_PLACEHOLDER` with a string of 64 zeros, which will be used as a temporary hash.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/utils/blueprintLock.ts",
                "crypto",
                "BLUEPRINT_LOCK_HEADER_PREFIX",
                "BLUEPRINT_LOCK_PLACEHOLDER"
              ],
              "technology_hints": [
                "Node.js",
                "crypto"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task of implementing the `BlueprintLock` mechanism. Defining constants for the header prefix and a temporary placeholder is a necessary and foundational action, consistent with the `BlueprintLock` data structure and the IBA's responsibilities outlined in the constitution.",
              "sequence_critique": "The sequence is logical. Establishing these core constants is the correct first step before creating the functions that will perform the hashing and file manipulation. No prerequisite steps are missing.",
              "clarity_critique": "The instructions are clear and highly actionable. The prompt specifies the exact file path, module to import, constant names, and their precise string values, which is ideal for an AI coding agent."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Implement the `addBlueprintLock` function in `src/utils/blueprintLock.ts`. This function should accept a `Buffer` of file content and return a new `Buffer` with the lock header. Follow the two-pass hashing logic: 1. Create a temporary header line with the placeholder hash and prepend it to the content. 2. Calculate the SHA-256 hash of this combined temporary buffer. 3. Create the final, correct header line using the calculated hash. 4. Prepend the final header line to the original input content and return the result. Ensure the header line is followed by a newline character (`\\n`).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "addBlueprintLock",
                "src/utils/blueprintLock.ts"
              ],
              "technology_hints": [
                "Buffer",
                "SHA-256"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal of implementing the `BlueprintLock` is fully aligned with the Project Constitution's requirements for auditability and integrity. However, the specific method described will fail to produce a verifiable file, thus undermining the intended goal.",
              "sequence_critique": "The described sequence of operations is logically flawed. The hash is calculated on a temporary buffer containing a placeholder header (`temp_buffer = placeholder_header + content`). The final buffer is then constructed with a different header containing the calculated hash (`final_buffer = final_header + content`). A verification script hashing the final file (`sha256(final_buffer)`) will produce a result that does not match the hash stored inside it, because `placeholder_header` and `final_header` are different. This makes the integrity check unverifiable and defeats its purpose.",
              "clarity_critique": "The instructions are clear about the sequence of actions to perform. However, the described procedure is incorrect for achieving a verifiable, self-consistent file hash, which could confuse an agent trying to satisfy the higher-level goal of integrity."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Implement the `verifyBlueprintLock` function in `src/utils/blueprintLock.ts`. This function should accept a `Buffer` of file content (presumably with a lock header) and return a `boolean`. It must: 1. Read the first line of the buffer. 2. Use a regular expression to check if the line matches the `BlueprintLock` format and extract the expected hash. If not, return `false`. 3. Calculate the actual SHA-256 hash of the entire input buffer. 4. Compare the expected hash from the header with the actual calculated hash. Return `true` if they match, `false` otherwise.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "verifyBlueprintLock",
                "src/utils/blueprintLock.ts",
                "BlueprintLock"
              ],
              "technology_hints": [
                "Buffer",
                "regular expression",
                "SHA-256"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task and the overall project goal. Implementing `verifyBlueprintLock` is a crucial part of the `BlueprintLock` mechanism, which directly supports the 'Auditability' non-functional requirement and the integrity checks specified in the IBA's responsibilities.",
              "sequence_critique": "The sequence of operations is logical and correct. It correctly specifies extracting the expected hash from the header first, then calculating the actual hash of the entire content (including the header), and finally comparing the two. This matches the verification logic implied by the constitution and the example snippet in Appendix A.8.",
              "clarity_critique": "The instructions are clear, specific, and fully actionable. The prompt defines the function signature, its location, input/output types, and the precise internal logic, including the use of a regular expression and the SHA-256 algorithm. There is no ambiguity."
            }
          }
        },
        {
          "id": "step_5",
          "description": "In `src/utils/blueprintLock.test.ts`, write unit tests for the `addBlueprintLock` and `verifyBlueprintLock` functions working together. The primary test case should take sample content, pass it to `addBlueprintLock`, and then pass the result to `verifyBlueprintLock`, asserting that the verification returns `true`. Include a test case for an empty input buffer.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/utils/blueprintLock.test.ts",
                "addBlueprintLock",
                "verifyBlueprintLock"
              ],
              "technology_hints": [
                "buffer"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal. Testing the `addBlueprintLock` and `verifyBlueprintLock` functions is essential for implementing the `BlueprintLock` mechanism, which directly supports the 'Auditability' non-functional requirement defined in the Project Constitution.",
              "sequence_critique": "The sequence of testing the round-trip functionality (`add` then `verify`) is logical. However, the test plan is incomplete. To be robust, it should also include negative test cases for the `verifyBlueprintLock` function, such as verifying content that has been tampered with (i.e., the hash does not match the content) and verifying content that has no `BlueprintLock` header at all. These are critical for ensuring the integrity check is reliable.",
              "clarity_critique": "The instructions are clear and specific enough for an AI agent to execute the described positive test cases (round-trip and empty input)."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Add specific unit tests for `verifyBlueprintLock` failure scenarios. Create test cases for: 1. A buffer where the content has been tampered with after the lock was added (the hash should not match). 2. A buffer with a malformed `BlueprintLock` header. 3. A buffer that does not contain a `BlueprintLock` header at all. All of these cases should result in `verifyBlueprintLock` returning `false`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "verifyBlueprintLock",
                "BlueprintLock"
              ],
              "technology_hints": [
                "buffer"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the task's goal. Testing failure scenarios for an integrity-checking mechanism like `BlueprintLock` is critical to fulfilling the 'Auditability' and 'Security' non-functional requirements outlined in the Project Constitution.",
              "sequence_critique": "The sequence is logical. These tests should be created alongside or immediately after the implementation of the `verifyBlueprintLock` function to ensure its robustness. No prerequisite steps are missing.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. Each test case is described with a specific input condition and an explicit expected outcome (`false`), which is ideal for an AI coding agent."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Review the code in `src/utils/blueprintLock.ts`. Add comprehensive JSDoc comments to both exported functions, especially explaining the rationale for the two-pass hashing mechanism in `addBlueprintLock`. Ensure the code is clean, readable, and adheres to the project's coding standards.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/utils/blueprintLock.ts",
                "addBlueprintLock",
                "two-pass hashing mechanism"
              ],
              "technology_hints": [
                "JSDoc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Documenting the rationale for the 'two-pass hashing mechanism' directly supports the 'Maintainability' NFR and clarifies a complex but critical algorithm (`BlueprintLock`) defined in the Project Constitution and detailed design.",
              "sequence_critique": "The sequence is logical. This step presumes the initial implementation of `blueprintLock.ts` exists, and it focuses on the subsequent, necessary actions of review, refinement, and documentation. This is a standard and correct workflow.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify the exact file, the required action (add JSDoc), and highlight the most important concept to explain (the two-pass hash), leaving no room for ambiguity."
            }
          }
        }
      ],
      "Task 1.9: Implement the repository scaffolding logic to create the required directory structure (`src/modules`, `contracts`, etc.).": [
        {
          "id": "step_1",
          "description": "Create a new file at `src/iba/scaffolder.ts`. This file will contain the logic for creating the initial repository directory structure.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/scaffolder.ts",
                "src/modules",
                "contracts"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Creating a dedicated `scaffolder.ts` file within the `src/iba/` directory correctly isolates the scaffolding responsibility, which is consistent with the component-based architecture and maintainability goals defined in the Project Constitution.",
              "sequence_critique": "The step is logically sound. Creating the file is the necessary prerequisite before any scaffolding functions can be implemented within it.",
              "clarity_critique": "The instruction is clear, specific, and actionable. It provides an exact file path and a concise description of the file's intended purpose, leaving no room for ambiguity."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `src/iba/scaffolder.ts`, import the `mkdir` function from the `fs/promises` module.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/scaffolder.ts",
                "mkdir",
                "fs/promises"
              ],
              "technology_hints": [
                "Node.js"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Implementing directory scaffolding requires file system operations, and importing `mkdir` from Node.js's native `fs/promises` module is the correct first action within the specified TypeScript/Node.js tech stack.",
              "sequence_critique": "The sequence is logical. Before any directory creation logic can be written, the necessary function must be imported. This is the correct foundational step for the `scaffolder.ts` file.",
              "clarity_critique": "The instruction is exceptionally clear and actionable. It precisely identifies the function to import (`mkdir`), the source module (`fs/promises`), and the target file (`src/iba/scaffolder.ts`), leaving no room for ambiguity."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Define and export an asynchronous function named `scaffoldRepository`. This function should accept one argument: `basePath: string`, which is the root directory for the new repository.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "scaffoldRepository",
                "basePath"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal. Implementing the repository scaffolding logic for the IBA, as defined in the architecture (Section 3.1), requires a function to orchestrate the creation of directories. This step defines that exact function.",
              "sequence_critique": "The sequence is logical. Defining the function signature is the correct initial step before adding the implementation logic (i.e., the actual directory creation calls) in subsequent steps.",
              "clarity_critique": "The instruction is clear and actionable. It precisely defines the function's name (`scaffoldRepository`), its asynchronous nature, its single parameter (`basePath: string`), and its export status, making it unambiguous for an AI agent to implement."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Inside `scaffoldRepository`, define a constant array named `requiredDirs`. Populate this array with the relative paths of all directories that need to be created, as specified in the `project_file_map`. The paths are: 'contracts', 'idl', 'prompt_templates', 'src/modules/__tests__', and '.tmp/ironclad_tasks'.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "scaffoldRepository",
                "requiredDirs",
                "project_file_map"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is aligned with the task of scaffolding the repository. However, the list of directories provided is incomplete according to the Project Constitution's `project_file_map` and the IBA's responsibilities outlined in section 3.1. Specifically, it omits the `src/modules` directory, which is a required location for the generated `I<ModuleName>.ts` interface stubs. While creating `src/modules/__tests__` might implicitly create its parent, `src/modules` is a primary output directory and should be explicitly included in the list.",
              "sequence_critique": "The sequence is logical. Defining the list of required directories as a constant is a correct and standard first step before implementing the logic that creates them.",
              "clarity_critique": "The instruction is clear and specific, making it highly actionable for a coding agent. The issue lies not in its clarity but in the incomplete content it prescribes."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Implement the core logic of `scaffoldRepository`. Use `Promise.all` to iterate over the `requiredDirs` array. For each relative path, construct the full absolute path using `path.join(basePath, dirPath)` and call `await mkdir(fullPath, { recursive: true })`. This option ensures parent directories are created and does not throw an error if a directory already exists. Remember to import the `path` module.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "scaffoldRepository",
                "Promise.all",
                "requiredDirs",
                "basePath",
                "dirPath",
                "path.join",
                "mkdir",
                "fullPath",
                "path"
              ],
              "technology_hints": [
                "Node.js 'path' module"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal of creating the repository's directory structure. This directly supports the IBA's responsibilities as defined in the Project Constitution (Section 3.1) and is a foundational requirement for the 'File-based Pipeline Orchestration' paradigm.",
              "sequence_critique": "The described internal logic is sound. Using `Promise.all` is an efficient and correct approach for creating multiple, non-dependent directories concurrently. The sequence of joining the path before creating the directory is logical.",
              "clarity_critique": "The instructions are exceptionally clear, specific, and actionable. They name the exact Node.js modules (`path`), functions (`mkdir`), and options (`{ recursive: true }`) to be used, which is ideal for an AI coding agent. The explanation of why `{ recursive: true }` is used adds valuable context."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Add JSDoc comments to the `scaffoldRepository` function explaining its purpose, parameters, and what it returns (a Promise that resolves when all directories are created).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "scaffoldRepository function",
                "Promise"
              ],
              "technology_hints": [
                "JSDoc",
                "Promise"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Adding JSDoc comments directly supports the Non-Functional Requirement for 'Maintainability' outlined in the Project Constitution (Section 14), which mandates comprehensive documentation and clear interfaces for all system components.",
              "sequence_critique": "The step is logically sound. Documenting a function is an integral part of its implementation, and this step fits perfectly within the broader task of creating the scaffolding logic.",
              "clarity_critique": "The instruction is exceptionally clear and actionable for an AI agent. It specifies the target function (`scaffoldRepository`), the documentation format (JSDoc), and the exact content required (purpose, parameters, return value), ensuring a precise and correct implementation."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Create a new unit test file at `src/iba/scaffolder.test.ts`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/scaffolder.test.ts"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step logically contributes to the task. Creating a unit test file is a fundamental and expected part of implementing any new functionality, adhering to the project's overall testing strategy.",
              "sequence_critique": "The sequence is valid for a test-driven development (TDD) workflow where the test file is created first. However, the plan has not yet explicitly mentioned the creation of the corresponding source file (`src/iba/scaffolder.ts`), which is an implicit dependency for the tests to target.",
              "clarity_critique": "The step is clear, specific, and actionable, providing the exact path for the new file."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Install the `mock-fs` library as a development dependency. This will be used to mock the file system during testing, avoiding actual disk writes. Command: `npm install --save-dev mock-fs @types/mock-fs`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install the `mock-fs` library as a development dependency.",
                "npm install --save-dev mock-fs @types/mock-fs"
              ],
              "key_entities_dependencies": [],
              "technology_hints": [
                "mock-fs",
                "@types/mock-fs",
                "npm"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Task 1.9 involves implementing file system operations (creating directories), and the project's testing strategy (Section 12) requires robust testing of such interactions. Installing `mock-fs` is a direct and necessary prerequisite for writing unit or integration tests for the scaffolding logic, ensuring the implementation is verifiable without actual disk I/O.",
              "sequence_critique": "The sequence is logical. Installing testing-related dependencies is a standard initial step when beginning an implementation task, especially one that will be developed with corresponding tests. It correctly precedes the writing of tests and the implementation itself.",
              "clarity_critique": "The step is exceptionally clear and actionable. It specifies the exact packages to install (`mock-fs`, `@types/mock-fs`), the reason for their installation (mocking the file system for tests), and provides the precise, correct command (`npm install --save-dev ...`) for execution."
            }
          }
        },
        {
          "id": "step_9",
          "description": "In `src/iba/scaffolder.test.ts`, write a test suite for `scaffoldRepository`. Use `mock-fs` in `beforeEach` and `afterEach` blocks to set up and tear down the virtual file system. Import `fs` to check for directory existence.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/scaffolder.test.ts",
                "scaffoldRepository",
                "beforeEach",
                "afterEach",
                "fs"
              ],
              "technology_hints": [
                "mock-fs",
                "fs"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Testing the scaffolding logic is a fundamental part of implementing it, and this function is a core responsibility of the IBA as defined in the architecture document (Section 3.1). The use of `mock-fs` is consistent with the project's testing strategy (Section 12), which calls for mocking file system operations.",
              "sequence_critique": "The sequence is logical. In a Test-Driven Development (TDD) workflow, writing the test before the implementation is the correct order. Even outside of strict TDD, creating the test file concurrently with the implementation file is standard practice.",
              "clarity_critique": "The step is clear and actionable. For enhanced precision, it could explicitly state that the test should verify the creation of the specific directories required by the `project_file_map` in the Project Constitution (e.g., `contracts`, `idl`, `src/modules`, `.tmp`). This would ensure the test comprehensively covers the architectural requirements."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Write a test case titled 'should create all required directories in the specified base path'. In this test, call `scaffoldRepository` with a virtual path (e.g., '/ironclad-repo'). After it completes, iterate through the `requiredDirs` list and use `fs.existsSync` or `fs.statSync` to assert that each directory was created correctly in the mocked file system.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "scaffoldRepository",
                "requiredDirs"
              ],
              "technology_hints": [
                "fs.existsSync",
                "fs.statSync",
                "mocked file system"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Writing a test case for repository scaffolding directly supports the implementation task (1.9) and the broader project goals of reliability and correctness outlined in the constitution.",
              "sequence_critique": "The sequence described within the step (call the function, then assert the outcome) is logical for a unit test.",
              "clarity_critique": "The instruction is clear and actionable. For maximum precision, it could explicitly state that the `requiredDirs` list must be derived from the `project_file_map` in the Project Constitution to ensure the test is comprehensive and validates the creation of all architecturally required directories (e.g., `src/modules/__tests__`, `.tmp/ironclad_tasks`)."
            }
          }
        },
        {
          "id": "step_11",
          "description": "Write another test case titled 'should not throw an error if directories already exist'. Set up the mock file system with some of the directories already present, then call `scaffoldRepository` and assert that the function completes without throwing an error.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "scaffoldRepository",
                "'should not throw an error if directories already exist'"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Testing for idempotency (i.e., not failing if directories already exist) is a crucial non-functional requirement for a robust build tool like the IBA, consistent with the project's emphasis on reliability.",
              "sequence_critique": "The sequence is logical. This step describes a test case for an alternative scenario (pre-existing state), which should naturally follow the test for the primary 'happy path' scenario (creating directories from scratch).",
              "clarity_critique": "The step is exceptionally clear and actionable. It provides specific instructions for the test's title, setup, execution, and assertion, making it easy for an AI agent to implement correctly."
            }
          }
        },
        {
          "id": "step_12",
          "description": "Finally, integrate the `scaffoldRepository` function into the main IBA CLI command handler. Import it, and call it with the output path provided by the user via the `--output` command-line option.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "scaffoldRepository function",
                "main IBA CLI command handler",
                "output path",
                "--output command-line option"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task of implementing repository scaffolding. Integrating the function into the main CLI command is the final and necessary action to make the feature functional, directly contributing to the IBA's responsibilities as defined in the Project Constitution (Section 3.1 and 5).",
              "sequence_critique": "The sequence is logical. The use of the word 'Finally' implies that the `scaffoldRepository` function has already been created in prior steps. This integration step is the correct final action for the task.",
              "clarity_critique": "The instructions are clear, specific, and actionable. They explicitly name the function to use (`scaffoldRepository`), the location for the change (the main IBA CLI command handler), and the source of the function's argument (`--output` option), which is consistent with the CLI design in the constitution."
            }
          }
        }
      ],
      "Task 1.10: Assemble the main `ironclad blueprint build` command, orchestrating the parsing, validation, and file generation steps.": [
        {
          "id": "step_1",
          "description": "Create a new file `src/cli.ts`. In this file, import the `commander` library. Instantiate a new `Command` object for the main `ironclad` program. This will be the entry point for all CLI commands.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/cli.ts",
                "Command",
                "ironclad"
              ],
              "technology_hints": [
                "commander"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step logically initiates the task of building the CLI. However, it proposes creating `src/cli.ts`, which contradicts the Project Constitution's `project_file_map`. The constitution already designates `src/index.ts` as 'The primary CLI entry point for the application', and this established path should be used to maintain architectural consistency.",
              "sequence_critique": "The sequence is logical. Creating the main command object is the correct first step before defining subcommands like `blueprint build`.",
              "clarity_critique": "The instructions are clear, specific, and directly actionable for an AI agent."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a new file `src/commands/blueprint.ts`. This file will contain the logic for the `blueprint` command. Export a function that takes a `Command` instance as an argument and adds the `build` subcommand to it.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/commands/blueprint.ts",
                "blueprint",
                "Command",
                "build"
              ],
              "technology_hints": [
                "commander.js"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step introduces a new directory `src/commands/` which is not defined in the Project Constitution's `project_file_map`. The constitution designates `src/index.ts` as the 'primary CLI entry point for the application, responsible for parsing commands and arguments'. This step proposes an alternative architectural pattern for command handling that conflicts with the established file structure.",
              "sequence_critique": "The step is logically flawed because it directs work to a new, non-standard location. The correct sequence, according to the constitution, would be to modify the existing `src/index.ts` file to define the `blueprint` command, which would then call the orchestrating logic from `src/iba/index.ts`.",
              "clarity_critique": "The instruction is clear and specific. An AI agent could execute the request to create the file and function as described. However, its clarity does not resolve the architectural inconsistency with the Project Constitution."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Inside `src/commands/blueprint.ts`, define the `build` subcommand. It should take one required argument `<architecture-spec-path>` and the required option `--output <repository-path>`. Add all optional options as specified in the architecture document (section 5): `--contracts-dir`, `--idls-dir`, `--context-file`, `--dsl-file`, and `--template-dir`. Add descriptive help text for each.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Consult the architecture document (section 5)"
              ],
              "key_entities_dependencies": [
                "src/commands/blueprint.ts",
                "build",
                "<architecture-spec-path>",
                "--output <repository-path>",
                "--contracts-dir",
                "--idls-dir",
                "--context-file",
                "--dsl-file",
                "--template-dir",
                "architecture document (section 5)"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal. Defining the CLI command's signature is the foundational action for assembling the `ironclad blueprint build` command, directly implementing the specification from Section 5 of the architecture document.",
              "sequence_critique": "The sequence is logical. Establishing the command's interface is the correct initial step for this task, as the subsequent steps will involve wiring the underlying IBA logic (parsing, validation, generation) into this command's action handler.",
              "clarity_critique": "The step is clear and actionable. It specifies the command, its required arguments, and its optional flags, all consistent with the Project Constitution. The introduction of the `src/commands/blueprint.ts` file is a reasonable implementation detail for organizing CLI code and does not conflict with the constitution's definition of `src/index.ts` as the primary entry point."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Attach an `async` action handler to the `build` command. This handler will receive the arguments and options. Inside the handler, start by resolving all input and output paths using the Node.js `path` module. Remember that optional paths default to being relative to the `architecture-spec-path`'s directory.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "build command",
                "action handler",
                "arguments",
                "options",
                "path module",
                "architecture-spec-path"
              ],
              "technology_hints": [
                "async",
                "Node.js",
                "path module"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task of orchestrating the `build` command. Resolving all input and output paths is the necessary first action for the IBA to perform its duties as defined in the constitution, such as reading the ARCHITECTURE_SPEC file.",
              "sequence_critique": "The sequence is logical. Path resolution must occur before any file I/O operations, making it the correct first step within the command's action handler.",
              "clarity_critique": "The step is clear and actionable. It specifies the use of an `async` handler, the `path` module, and provides the crucial business logic for resolving optional paths relative to the main specification file, which is consistent with the CLI design in Section 5 of the architecture document."
            }
          }
        },
        {
          "id": "step_5",
          "description": "In the `build` action handler, implement the core orchestration logic within a `try...catch` block. Begin by importing and calling the necessary functions in sequence: 1. `validateContracts` (from `contract-validator.ts`), 2. `buildDependencyGraph` (from `dependency-graph.ts`), 3. `validateDAG` (from `dag-validator.ts`). Add a `console.log` message before each step (e.g., `[IBA] Validating contracts...`). If any of these functions throw an error, the `catch` block should handle it.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "build action handler",
                "try...catch",
                "validateContracts",
                "contract-validator.ts",
                "buildDependencyGraph",
                "dependency-graph.ts",
                "validateDAG",
                "dag-validator.ts",
                "console.log"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the IBA's core responsibilities as defined in the Project Constitution, specifically contract validation and dependency graph integrity checks. The use of a `try...catch` block directly supports the 'Fail-Fast' non-functional requirement.",
              "sequence_critique": "The sequence is logically correct and mandatory: contracts must be validated before a graph can be built from them, and the graph must be built before it can be checked for cycles. This establishes the blueprint's structural integrity before any subsequent file generation would occur.",
              "clarity_critique": "The instructions are clear, specific, and highly actionable. They explicitly name the functions to be called, their source files, the required control flow (`try...catch`), and logging requirements, leaving no room for ambiguity."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Continue the orchestration by importing and calling the file generation functions: 1. `scaffoldDirectories` (to create the output directory structure), 2. `generateInterfaceStubs` (to create the TypeScript interface strings from the validated contracts). Log progress for each step.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "scaffoldDirectories",
                "generateInterfaceStubs",
                "validated contracts",
                "TypeScript interface strings"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are well-aligned with the task and the IBA's responsibilities outlined in the Project Constitution (Section 3.1), specifically scaffolding the repository and generating interface content from contracts.",
              "sequence_critique": "The sequence is logical (creating directories before generating content for them). However, the step is incomplete. It describes generating interface *strings* but omits the subsequent, constitutionally-mandated step of writing these strings to files using the `writeFilesWithBlueprintLock` function to embed the `BlueprintLock` integrity hash.",
              "clarity_critique": "The step is clear about generating in-memory content but is critically unclear about persistence. It should explicitly state that the generated interface strings must be written to disk using the dedicated function that also calculates and embeds the `BlueprintLock` hash, a non-negotiable requirement from the constitution's auditability and integrity NFRs."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Orchestrate the final file writing step. You will need to read the content of all blueprint files (validated contracts, IDLs, context, DSL, templates) and combine them with the generated interface stubs into a single data structure (e.g., `Map<string, string>`). Then, call the pre-existing `writeFilesWithBlueprintLock` function to write all these files to their correct destination within the output directory. Ensure this happens inside the `try` block.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "blueprint files",
                "validated contracts",
                "IDLs",
                "context",
                "DSL",
                "templates",
                "generated interface stubs",
                "Map<string, string>",
                "writeFilesWithBlueprintLock",
                "output directory",
                "try block"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal of orchestrating the `blueprint build` command. Writing the final set of blueprint files and generated stubs to disk is the primary output and culmination of the IBA's responsibilities, as defined in the Project Constitution.",
              "sequence_critique": "The step is logically sequenced. It correctly assumes that parsing, validation, and interface stub generation have already occurred, and it positions the file writing as the final action within the command's successful execution path. Placing this I/O-heavy operation inside a `try` block is also a correct and robust sequence.",
              "clarity_critique": "The instruction is mostly clear. However, the phrase \"read the content of all blueprint files\" could be misinterpreted as performing a new file-read operation from the original source directories. To improve clarity, it should specify gathering the *already-validated content* (which should be held in memory from previous steps) along with the newly generated stub content, rather than re-reading files from disk."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Finalize the `build` action handler's control flow. If the `try` block completes successfully, log a success message (e.g., `[IBA] Blueprint built successfully at <output-path>`) and call `process.exit(0)`. In the `catch` block, log the error message to `console.error` and call `process.exit(1)` to indicate failure.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "build action handler",
                "process.exit",
                "console.error"
              ],
              "technology_hints": [
                "Node.js"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of assembling the main command. It directly addresses the need for a clear success/failure outcome, which is consistent with the project's goal of creating a robust CLI tool for CI/CD pipelines.",
              "sequence_critique": "The sequence is logical. This step correctly represents the final control flow mechanism (a top-level try/catch block) that wraps the entire command's execution and handles its ultimate termination.",
              "clarity_critique": "The instruction is clear but not fully consistent with the established Project Constitution. While using `process.exit(1)` for failure adheres to the basic requirement for a non-zero exit code, it ignores the detailed 'Exit-Code Taxonomy' specified in Appendix A.6. The constitution advocates for specific exit codes for different failure types to improve CI/CD integration. A generic `exit(1)` for all possible blueprint failures (e.g., file not found, invalid contract, dependency cycle) is a simplification that deviates from this more sophisticated requirement."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Go back to `src/cli.ts`. Import the function from `src/commands/blueprint.ts` and call it, passing the main `program` instance. Finally, add `program.parse(process.argv);` to execute the CLI logic. This connects the `blueprint build` command to the main entry point.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/cli.ts",
                "the function from `src/commands/blueprint.ts`",
                "program",
                "program.parse(process.argv)"
              ],
              "technology_hints": [
                "Node.js"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal. Connecting the command logic to the main program instance and triggering the parser is the essential final action to make the `blueprint build` command operational.",
              "sequence_critique": "The sequence is logical. This step correctly serves as the final wiring after all underlying command logic has been implemented, orchestrating the execution of the entire CLI application.",
              "clarity_critique": "The instruction is clear and actionable. However, it directs the action to `src/cli.ts`, whereas the Project Constitution's file map explicitly designates `src/index.ts` as 'The primary CLI entry point'. For strict constitutional adherence, the target file should be `src/index.ts`."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Update `package.json` to make the CLI tool executable. Add a `bin` field: `\"bin\": { \"ironclad\": \"./dist/cli.js\" }`. You may need to add a shebang `#!/usr/bin/env node` to the top of `src/cli.ts`. Also, add a `build` script to your `package.json` to compile TypeScript to JavaScript (e.g., `\"build\": \"tsc\"`).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "package.json",
                "bin",
                "ironclad",
                "./dist/cli.js",
                "src/cli.ts",
                "build",
                "tsc"
              ],
              "technology_hints": [
                "package.json",
                "TypeScript",
                "JavaScript",
                "Node.js",
                "tsc"
              ]
            },
            "step_critique": {
              "alignment_critique": "Steps are perfectly aligned with the task of creating an executable CLI tool. They directly implement the standard Node.js mechanism for this purpose, which is consistent with the project's tech stack (TypeScript/Node.js CLI) and file structure (`dist` directory) as defined in the Project Constitution.",
              "sequence_critique": "The steps are logically cohesive and represent a single, atomic change to make the tool executable. The order of the individual actions within this step is not critical, and the step correctly builds upon the assumed project state.",
              "clarity_critique": "The instructions are highly clear and actionable, providing the exact code and configuration snippets to be added. The file paths and script definitions are specific and unambiguous."
            }
          }
        }
      ],
      "Task 1.11: Set up a testing framework (e.g., Jest) and write unit tests for critical logic like DAG detection and contract validation.": [
        {
          "id": "step_1",
          "description": "Install Jest and its necessary TypeScript dependencies. Use the following command: `npm install --save-dev jest ts-jest @types/jest`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "npm install --save-dev jest ts-jest @types/jest"
              ],
              "key_entities_dependencies": [],
              "technology_hints": [
                "Jest",
                "TypeScript",
                "npm",
                "ts-jest",
                "@types/jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task and Project Constitution. It installs the exact testing framework (Jest, ts-jest, @types/jest) explicitly listed as required project dependencies.",
              "sequence_critique": "The sequence is logical. Installing the testing framework is the correct prerequisite step before any test configuration or authoring can begin.",
              "clarity_critique": "The step is clear and directly actionable. It provides a precise, complete, and correct `npm` command that can be executed without ambiguity."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a Jest configuration file named `jest.config.js` in the project root. Configure it to use `ts-jest` for TypeScript files. A basic configuration should look like this: `module.exports = { preset: 'ts-jest', testEnvironment: 'node' };`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "jest.config.js",
                "module.exports"
              ],
              "technology_hints": [
                "Jest",
                "ts-jest",
                "TypeScript",
                "node"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal. Creating a Jest configuration file is a fundamental and necessary action for setting up the Jest testing framework in a TypeScript project, consistent with the project's tech stack and testing strategy.",
              "sequence_critique": "The step is logically sequenced. It correctly assumes that Jest and its related dependencies (`ts-jest`) have been installed and precedes the actual writing of test files.",
              "clarity_critique": "The instruction is clear and actionable, providing a specific filename, location, and content. However, it is slightly misaligned with the broader project constitution. Section 12 ('Testing Strategy') explicitly calls for code coverage targets. The proposed 'basic' configuration omits necessary coverage settings (e.g., `collectCoverage`, `coverageDirectory`), which are required to fulfill the project's non-functional requirements for testing."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Add a `test` script to your `package.json` file to easily run the tests. The script should be: `\"test\": \"jest\"`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "package.json",
                "test script"
              ],
              "technology_hints": [
                "jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal of setting up a testing framework. Adding a `test` script to `package.json` is a standard and essential part of integrating a test runner like Jest into a Node.js project, consistent with the testing strategy outlined in Section 12 of the constitution.",
              "sequence_critique": "The step is logically placed within the task of setting up the testing framework. It naturally follows the installation of testing dependencies and precedes the execution of the tests themselves.",
              "clarity_critique": "The step is clear but overly simplistic for the project's specified tech stack. The Project Constitution identifies the project uses TypeScript and lists `ts-jest` as a dependency. A simple `\"test\": \"jest\"` command may fail without a `jest.config.js` file correctly configured for TypeScript transformation. Furthermore, Appendix A.4 suggests using environment variables like `TS_NODE_TRANSPILE_ONLY` for performance, which are omitted from this basic script."
            }
          }
        },
        {
          "id": "step_4",
          "description": "To ensure testability, refactor the core logic of the Ironclad Blueprint Architect (IBA). Create a new file `src/iba/graph.ts`. Move any existing logic for dependency graph construction and DAG cycle detection into this file. Export the functions.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "Ironclad Blueprint Architect (IBA)",
                "src/iba/graph.ts",
                "dependency graph construction",
                "DAG cycle detection"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step's intent to refactor for testability aligns with the task. However, its proposed action\u2014creating a new `src/iba/graph.ts`\u2014conflicts with the established Project Constitution. The constitution already modularizes this logic into `src/iba/dependency-graph.ts` (for the graph data structure) and `src/iba/validators/graph-validator.ts` (for cycle detection). This proposed change deviates from the agreed-upon architecture.",
              "sequence_critique": "The sequence is logical. Refactoring code to make it more testable is a correct prerequisite to writing the actual unit tests.",
              "clarity_critique": "The step is unclear as it fails to reference the source files (`src/iba/dependency-graph.ts` and `src/iba/validators/graph-validator.ts`) from which the logic should be moved. By instructing the creation of a new, conflicting file, it creates ambiguity regarding the fate of the existing, constitution-defined files."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Create a new test file `src/iba/graph.test.ts`. This file will contain unit tests for the DAG cycle detection logic.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/graph.test.ts",
                "DAG cycle detection logic"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal of writing unit tests for critical logic, specifically DAG detection, which is a core responsibility of the IBA component as defined in the constitution.",
              "sequence_critique": "The proposed file path `src/iba/graph.test.ts` is inconsistent with the established Project Constitution. The constitution specifies the location for DAG validation tests as `src/iba/validators/graph-validator.test.ts` to be co-located with its source file `src/iba/validators/graph-validator.ts`. The step should be updated to use the correct path to maintain architectural consistency.",
              "clarity_critique": "The instruction to create a file is clear. However, by specifying a path that contradicts the project's file map, it creates a point of confusion and risks creating a disorganized file structure. The action should be clarified to use the constitutionally correct path."
            }
          }
        },
        {
          "id": "step_6",
          "description": "In `src/iba/graph.test.ts`, write a suite of tests for the DAG cycle detection function. Use mock `ModuleContract` objects to represent the modules and their dependencies. Your tests must cover the following scenarios:\n1. A valid DAG with no cycles (e.g., A -> B, C -> B).\n2. A simple, direct cycle (e.g., A -> B -> A).\n3. A longer, indirect cycle (e.g., A -> B -> C -> A).\n4. A graph with disconnected components.\n5. A 'diamond' dependency graph, which is a valid DAG (e.g., A -> B, A -> C, B -> D, C -> D).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/graph.test.ts",
                "DAG cycle detection function",
                "ModuleContract"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal of testing critical logic. The specified test scenarios are comprehensive and directly support the project's non-functional requirement for 'Reliability (Fail-Fast)' by ensuring robust dependency cycle detection. The use of mock `ModuleContract` objects is consistent with the project's data structures.",
              "sequence_critique": "The sequence is logical. This step correctly assumes that the DAG detection logic has been implemented in a prior task (Task 1.6) and now requires testing.",
              "clarity_critique": "The step is mostly clear and actionable. However, it could be improved by specifying that tests for cyclic graphs (scenarios 2 and 3) must assert that a `CycleError` is thrown, as this custom error type is explicitly defined in the Project Constitution. Additionally, the file path `src/iba/graph.test.ts` slightly deviates from the more specific `src/iba/validators/graph-validator.test.ts` established in the constitution, which could cause minor confusion."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Now, refactor the contract validation logic. Create a new file `src/iba/validation.ts`. Move any logic for validating the structure and content of `ModuleContract` JSON files into this file and export the functions.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "contract validation logic",
                "src/iba/validation.ts",
                "ModuleContract"
              ],
              "technology_hints": [
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step, which involves refactoring validation logic into a dedicated file, is a logical prerequisite for writing isolated unit tests. It aligns well with the task's goal of testing critical logic.",
              "sequence_critique": "The sequence is correct. Isolating the validation logic before writing unit tests for it is a sound software engineering practice that improves modularity and testability.",
              "clarity_critique": "The instruction to create a new file at `src/iba/validation.ts` directly contradicts the Project Constitution. The `project_file_map` explicitly defines `src/iba/validators/contractValidator.ts` for implementing 'the JSON schema validation logic for ModuleContract files'. The step must be revised to use the constitutionally-defined file path to maintain architectural integrity."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Create a test file `src/iba/validation.test.ts` for the contract validation logic.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/iba/validation.test.ts",
                "contract validation logic"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step's intent to test contract validation logic aligns with the task. However, the specified file path `src/iba/validation.test.ts` directly contradicts the Project Constitution's `project_file_map`, which specifies `src/iba/validators/contractValidator.test.ts` for this purpose. This is a significant deviation from the established architecture.",
              "sequence_critique": "The step is logically sound within the context of writing tests for existing or new logic. No issues with sequence.",
              "clarity_critique": "The instruction is clear and actionable, but it directs the creation of a file at a location that violates the project's defined file structure. It should be corrected to `src/iba/validators/contractValidator.test.ts` to maintain consistency with the Constitution."
            }
          }
        },
        {
          "id": "step_9",
          "description": "In `src/iba/validation.test.ts`, write unit tests for the contract validation function. Refer to the `ModuleContract` JSON schema in the architecture document (section A.2). Your tests must cover at least these scenarios:\n1. A perfectly valid contract object.\n2. A contract missing a required field (e.g., `purpose`).\n3. A contract with an invalid `name` field (e.g., `userService` which doesn't match the `^[A-Z][A-Za-z0-9]+$` pattern).\n4. A contract with an empty `dependencies` array (which is valid).\n5. A contract with incorrect data types (e.g., `dependencies` is an object instead of an array).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Refer to the `ModuleContract` JSON schema in the architecture document (section A.2)"
              ],
              "key_entities_dependencies": [
                "src/iba/validation.test.ts",
                "contract validation function",
                "ModuleContract",
                "purpose",
                "name",
                "^[A-Z][A-Za-z0-9]+$",
                "dependencies",
                "architecture document (section A.2)"
              ],
              "technology_hints": [
                "JSON schema",
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Testing the ModuleContract validation logic is a critical part of ensuring the IBA is robust, which directly supports the project's core mission of ensuring strict adherence to contracts.",
              "sequence_critique": "The step is logically sound. It assumes the contract validation function already exists to be tested, which is a standard prerequisite in a test-writing task. The list of test scenarios can be implemented in any order within the test suite.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. The scenarios are specific, well-defined, and cover a good range of positive and negative cases. The reference to the architecture document for the schema is a strong point. The only minor inconsistency is the file path `src/iba/validation.test.ts`, whereas the Project Constitution specifies a more granular path `src/iba/validators/contractValidator.test.ts`. This is a minor detail that does not impede clarity."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Run all the newly created tests by executing `npm test` from the command line. Ensure all tests pass. If any tests fail, debug and fix the implementation or the test case until the suite passes.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "executing `npm test` from the command line"
              ],
              "key_entities_dependencies": [],
              "technology_hints": [
                "npm"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Running and debugging tests is the logical conclusion to the task of writing them, ensuring that the critical IBA logic (DAG detection, contract validation) is verifiably correct as required by the constitution's emphasis on correctness and a comprehensive testing strategy.",
              "sequence_critique": "The step is correctly sequenced as the final action within a task dedicated to setting up and writing tests. It validates the work performed in the preceding steps of this task.",
              "clarity_critique": "The instructions are clear and actionable. They specify the exact command (`npm test`), the success condition (all tests pass), and the iterative debugging loop, which is a standard and understandable directive for a coding agent."
            }
          }
        }
      ],
      "Task 1.12: Write integration tests for the `ironclad blueprint build` command to verify correct file system output and structure.": [
        {
          "id": "step_1",
          "description": "Install necessary development dependencies for file system management and assertions in integration tests. Use `npm install --save-dev tmp-promise fs-extra` to add libraries for managing temporary directories and enhanced file system operations.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install necessary development dependencies for file system management and assertions in integration tests.",
                "Use `npm install --save-dev tmp-promise fs-extra` to add libraries for managing temporary directories and enhanced file system operations."
              ],
              "key_entities_dependencies": [
                "tmp-promise",
                "fs-extra"
              ],
              "technology_hints": [
                "npm",
                "tmp-promise",
                "fs-extra"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Writing integration tests for a file-system-heavy command like `blueprint build` requires tools to manage temporary directories (`tmp-promise`) and assert on file/directory states (`fs-extra`). These dependencies directly support the task's goal.",
              "sequence_critique": "The sequence is logical. Installing necessary dependencies is the correct prerequisite step before writing the test code that will utilize them.",
              "clarity_critique": "The step is clear and actionable. It specifies the exact `npm` command to run and correctly identifies the libraries as development dependencies, aligning with standard project practices."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a new integration test file at `src/commands/__tests__/blueprint.integration.test.ts`. This file will contain all end-to-end tests for the `ironclad blueprint build` command.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/commands/__tests__/blueprint.integration.test.ts",
                "ironclad blueprint build"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task. The chosen file path, `src/commands/__tests__/blueprint.integration.test.ts`, correctly co-locates the test with its corresponding source code (`src/commands/blueprint.ts`). This follows the dominant pattern for testing internal tooling components established in the Project Constitution (e.g., `src/iba/parsers.ts` and `src/iba/parsers.test.ts`), which supports maintainability.",
              "sequence_critique": "The sequence is logical. Creating the test file is the correct prerequisite before any test code can be written.",
              "clarity_critique": "The instruction is clear and actionable. It provides an exact file path and explicitly states the file's purpose, leaving no ambiguity."
            }
          }
        },
        {
          "id": "step_3",
          "description": "In `blueprint.integration.test.ts`, set up the basic test structure. Import `tmp`, `fs-extra`, `path`, and Node.js's `child_process`. Create a `describe` block for 'ironclad blueprint build'. Use `beforeEach` and `afterEach` hooks with `tmp-promise` to create and automatically clean up temporary directories for test inputs and outputs, ensuring test isolation.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "blueprint.integration.test.ts",
                "tmp",
                "fs-extra",
                "path",
                "child_process",
                "describe",
                "beforeEach",
                "afterEach",
                "tmp-promise",
                "ironclad blueprint build"
              ],
              "technology_hints": [
                "tmp",
                "fs-extra",
                "path",
                "child_process",
                "Node.js",
                "tmp-promise"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task. Setting up an isolated test environment using temporary directories is a foundational and necessary prerequisite for writing integration tests that involve file system I/O and child process execution, which is central to testing the `ironclad blueprint build` command.",
              "sequence_critique": "The sequence is logical. Establishing the test suite structure with imports and `beforeEach`/`afterEach` hooks is the correct first step before implementing the specific `it` test cases.",
              "clarity_critique": "The instructions are clear but introduce the `tmp` (or `tmp-promise`) and `fs-extra` libraries, which are not listed in the Project Constitution's `global_dependencies_and_interfaces`. While these are appropriate tools for the task, this constitutes a deviation from the established tech stack that should be formally documented."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Create a helper function `runBlueprintCommand(args: string[]): { stdout: string; stderr: string; status: number | null }`. This function should use `child_process.spawnSync` to execute your main CLI entry point (e.g., `ts-node src/index.ts blueprint build ...args`) and return the result. This encapsulates the command execution logic for all tests.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runBlueprintCommand",
                "args",
                "stdout",
                "stderr",
                "status",
                "src/index.ts",
                "blueprint build"
              ],
              "technology_hints": [
                "child_process.spawnSync",
                "ts-node"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Creating a helper to execute the CLI command is a foundational and necessary step for writing the integration tests required by the task, consistent with the testing strategy outlined in the constitution.",
              "sequence_critique": "The sequence is logical. Creating a reusable helper function before writing individual test cases is a standard best practice that improves test maintainability and clarity.",
              "clarity_critique": "The step is very clear and actionable. It specifies the function signature, return type, recommended Node.js module (`child_process.spawnSync`), and the precise command to execute, leaving no room for ambiguity."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Implement the first integration test for the 'happy path'. It should test a successful build with a valid project structure. In the test's setup: \n1. Create mock input files within the temporary input directory: `ARCHITECTURE_SPEC.md`, `system_context.json`, `validation_dsl_spec.md`, `prompt_templates/module.tmpl`, `idl/shared.ts`, and two valid, interdependent contracts (`contracts/User.json` and `contracts/Auth.json` where Auth depends on User). \n2. Execute the `runBlueprintCommand` pointing to the mock spec and output directory. \n3. Assert that the command's exit code (`status`) is 0.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "ARCHITECTURE_SPEC.md",
                "system_context.json",
                "validation_dsl_spec.md",
                "prompt_templates/module.tmpl",
                "idl/shared.ts",
                "contracts/User.json",
                "contracts/Auth.json",
                "runBlueprintCommand",
                "status"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is only partially aligned with the task's goal. While testing for a successful exit code (0) is a valid check for a 'happy path' integration test, it completely fails to address the core requirement of Task 1.12, which is to 'verify correct file system output and structure.' According to the constitution (Section 3.1), the IBA is responsible for scaffolding directories, generating interface stubs, and embedding `BlueprintLock` headers. The test assertions must be expanded to verify these specific outputs.",
              "sequence_critique": "The sequence of setting up mock inputs, executing the command, and then asserting the result is logical and correct.",
              "clarity_critique": "The prompt is clear in its instructions to create specific mock files and check the exit code. However, it is critically incomplete because it lacks instructions for the most important assertions. The step must be expanded to include assertions that check for the presence of generated interface stubs (e.g., `output/src/modules/IUser.ts`, `output/src/modules/IAuth.ts`), copied blueprint files (e.g., `output/contracts/User.json`), and the integrity of the `BlueprintLock` header on those files."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Add detailed file system assertions to the 'happy path' test. Use `fs-extra` to verify:\n1. The existence of the expected output directory structure (`contracts`, `idl`, `src/modules`, etc.).\n2. The existence of the copied files (e.g., `contracts/User.json`, `system_context.json`).\n3. The existence of the generated interface stubs (`src/modules/IUser.ts`, `src/modules/IAuth.ts`).\n4. The content of a generated interface stub (`IUser.ts`) matches the definition in its corresponding contract (`User.json`).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "'happy path' test",
                "contracts",
                "idl",
                "src/modules",
                "contracts/User.json",
                "system_context.json",
                "src/modules/IUser.ts",
                "src/modules/IAuth.ts",
                "IUser.ts",
                "User.json"
              ],
              "technology_hints": [
                "fs-extra"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of testing the IBA's output. However, it is incomplete as it omits a critical verification step mandated by the Project Constitution: checking for the presence and validity of the `BlueprintLock` header on all generated and copied files. This is a key `Auditability` requirement defined in section 3.1 of the architecture document.",
              "sequence_critique": "The sequence of assertions (checking directories, then file existence, then file content) is logical and follows a standard, effective pattern for this type of integration test.",
              "clarity_critique": "The step is clear and provides specific examples, making it highly actionable. The primary lack of clarity stems from the omission of the `BlueprintLock` verification, which leaves out a key part of the expected output's definition according to the constitution."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Implement the `BlueprintLock` integrity check for the 'happy path' test. For a key output file (e.g., `contracts/User.json` in the output directory):\n1. Read the file's content.\n2. Parse the `BlueprintLock: sha256:<hash>` header to extract the expected hash.\n3. Use Node's `crypto` module to calculate the SHA-256 hash of the entire file content.\n4. Assert that the extracted hash from the header matches the newly calculated hash. Refer to the verification snippet in `A.8` of the architecture spec for the logic.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Refer to the verification snippet in `A.8` of the architecture spec for the logic."
              ],
              "key_entities_dependencies": [
                "BlueprintLock",
                "contracts/User.json",
                "A.8"
              ],
              "technology_hints": [
                "Node",
                "crypto",
                "SHA-256"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal of verifying the `BlueprintLock` directly supports the 'Auditability' and 'Reliability (BlueprintLock Verifiability)' non-functional requirements defined in the constitution. The alignment is correct.",
              "sequence_critique": "The sequence of reading the file, extracting the expected hash, calculating the actual hash, and asserting equality is logical for a verification test.",
              "clarity_critique": "The instruction in Step 3 to calculate the hash of the *entire* file content is clear but reflects a logical contradiction within the Project Constitution's definition of `BlueprintLock` (NFR 'Reliability (BlueprintLock Verifiability)' and snippet 'A.8'). A file's hash cannot be contained within the file if the hash itself is part of the content being hashed. The correct verification logic should be to strip the header, hash the remaining content, and compare that to the hash from the header. The current instruction will cause the test to fail if the lock generation is implemented correctly."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Implement a new integration test for a failure case: a cyclical dependency. \n1. Create mock contracts where `contracts/A.json` depends on `B`, and `contracts/B.json` depends on `A`.\n2. Run the `runBlueprintCommand`.\n3. Assert that the command's exit code is non-zero.\n4. Assert that the `stderr` output includes a specific, clear error message identifying the dependency cycle.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "contracts/A.json",
                "contracts/B.json",
                "runBlueprintCommand",
                "exit code",
                "stderr",
                "dependency cycle"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the task. Testing for dependency cycle failures is a critical validation responsibility of the Ironclad Blueprint Architect (IBA), as specified in the constitution's 'Detailed Component Design' (Section 3.1) and supported by the `CycleError` data structure and the 'Interoperability (CLI Exit Codes)' non-functional requirement.",
              "sequence_critique": "The sequence is logical and follows a standard Arrange-Act-Assert pattern for testing: 1. Arrange (create mock contracts), 2. Act (run the command), 3. Assert (check exit code and error output). The order is correct.",
              "clarity_critique": "The steps are clear and actionable. The requirement to assert on both a non-zero exit code and a specific error message provides a robust test case that verifies both the process termination signal and the user-facing error reporting, which is consistent with the project's emphasis on reliability and interoperability."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Implement an integration test for another failure case: a missing contract file. \n1. Create mock contracts where `contracts/A.json` depends on `B`, but the file `contracts/B.json` does not exist in the input directory.\n2. Run the `runBlueprintCommand`.\n3. Assert that the command exits with a non-zero exit code.\n4. Assert that the `stderr` output contains a clear error message indicating that the contract for module 'B' could not be found.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "contracts/A.json",
                "B",
                "contracts/B.json",
                "runBlueprintCommand",
                "stderr"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. It directly tests a critical failure path (unresolved dependency) of the `ironclad blueprint build` command, which validates core responsibilities of the IBA and Non-Functional Requirements like 'Reliability (Fail-Fast)' and 'Reliability (Specific Error Handling)' as defined in the Project Constitution.",
              "sequence_critique": "The sequence follows a standard and logical 'Arrange, Act, Assert' pattern for testing. The order of setting up the mock file system, running the command, and then asserting the outcomes (exit code and error message) is correct.",
              "clarity_critique": "The instructions are clear and unambiguous. They specify the exact test condition (module A depends on missing module B), the command to execute, and the precise outcomes to verify (non-zero exit code, specific error message content in stderr)."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Refactor the test file to improve maintainability. Create a helper function, e.g., `createMockProject(config: object)`, that populates the temporary input directory with files based on the passed configuration. This will reduce code duplication across the 'happy path' and 'failure case' tests.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "createMockProject",
                "config"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step directly supports the task of writing robust and maintainable integration tests. Refactoring to create a test setup helper aligns perfectly with the project's non-functional requirement for 'Maintainability' and the overall goal of building high-quality, well-tested tooling.",
              "sequence_critique": "Creating a test setup helper function is a logical prerequisite or concurrent step when writing multiple integration test cases (e.g., success and failure paths). This sequence is efficient as it prevents code duplication from the outset.",
              "clarity_critique": "The instruction is clear and actionable. It specifies the goal (improve maintainability), the method (create a helper function), provides a concrete example (`createMockProject(config: object)`), and explains the rationale (reduce code duplication). This is sufficient for an AI agent to execute."
            }
          }
        }
      ]
    },
    "Phase 2: Core Generation Pipeline: Task Runner, Job Process, and Basic Validation": {
      "Task 2.1: Implement the `ironclad generate run` command-line interface.": [
        {
          "id": "step_1",
          "description": "To handle command-line argument parsing, install the `commander` library, which is a standard and robust choice for building Node.js CLI applications. Also, ensure `chalk` is installed for colored console output to improve user experience. Add these as production dependencies.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "install the `commander` library",
                "ensure `chalk` is installed"
              ],
              "key_entities_dependencies": [],
              "technology_hints": [
                "commander",
                "chalk",
                "Node.js"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the project constitution. Section 5, 'API Design and Endpoints', explicitly defines a CLI structure (`ironclad generate run`), and Section 2, 'Technology Stack Choices', lists `Commander.js` as the chosen library for this purpose. Installing `chalk` for colored output directly supports the UX design goals described in Section 6.",
              "sequence_critique": "The sequence is logical. Installing required libraries is the correct prerequisite step before starting any implementation that uses them.",
              "clarity_critique": "The instructions are clear and actionable. They specify the exact libraries to install (`commander`, `chalk`) and correctly classify them as 'production dependencies', which is an unambiguous instruction for a package manager."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a new file at `src/commands/generate.ts`. This file will encapsulate the logic for the `ironclad generate` command and its subcommands, adhering to the principle of separating concerns for different CLI commands.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/commands/generate.ts",
                "ironclad generate"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It establishes the file structure for the `generate` command, which is consistent with the `commands` directory pattern established for the `blueprint` command in Task 1.10 and the overall CLI design in the constitution.",
              "sequence_critique": "The sequence is logical. Creating the file is the necessary first step before its contents can be implemented.",
              "clarity_critique": "The instruction is clear, specific, and actionable, providing both the exact file path and the reasoning behind its creation."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Update your main CLI entry point file (e.g., `src/cli.ts` or `src/index.ts`). Import the command setup function from `src/commands/generate.ts` and register it with your main `commander` program instance. This will make the `generate` command available when the `ironclad` executable is run.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "main CLI entry point file",
                "src/cli.ts",
                "src/index.ts",
                "command setup function",
                "src/commands/generate.ts",
                "main commander program instance",
                "generate command",
                "ironclad executable"
              ],
              "technology_hints": [
                "commander"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is directly aligned with the task's goal. Registering the command's setup function with the main `commander` instance is a necessary and logical action to implement the `ironclad generate run` command as part of the overall CLI tool.",
              "sequence_critique": "The step is logically sequenced. It correctly assumes that the modular command file (`src/commands/generate.ts`) has been defined in a prior step and focuses on the subsequent action of integrating it into the main application entry point (`src/cli.ts` or `src/index.ts`). This follows the modular pattern established in the constitution.",
              "clarity_critique": "The instructions are clear and actionable. The step specifies the source file (`src/commands/generate.ts`), the target files (`src/cli.ts` or `src/index.ts`), the action (import and register), and the expected outcome (command becomes available). This is unambiguous for an AI agent."
            }
          }
        },
        {
          "id": "step_4",
          "description": "In `src/commands/generate.ts`, implement the structure for `ironclad generate run <repository-path>`. Use `commander` to create a top-level `generate` command. Then, add a `run` subcommand to it. The `run` command must accept one required argument, `<repository-path>`, which is the path to the repository initialized by the IBA. Define a placeholder `.action()` for the `run` command that we will implement next.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/commands/generate.ts",
                "ironclad generate run <repository-path>",
                "generate",
                "run",
                "<repository-path>",
                "IBA",
                ".action()"
              ],
              "technology_hints": [
                "commander"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task of creating the `ironclad generate run` command. It directly implements the CLI structure specified in the Project Constitution's 'API Design and Endpoints' section, using the established 'commander' library and modular command file structure (`src/commands/`).",
              "sequence_critique": "The sequence is logical. Creating the command and argument structure with a placeholder action is the correct foundational first step before implementing the complex orchestration logic that the command will trigger.",
              "clarity_critique": "The instructions are clear and highly actionable. It specifies the exact file (`src/commands/generate.ts`), the library (`commander`), the command hierarchy (`generate run`), the required argument (`<repository-path>`), and the placeholder nature of the action handler, leaving no room for ambiguity."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Implement input validation within the `.action()` handler for the `run` command. The `<repository-path>` argument must be validated to ensure it exists and is a directory. Use the Node.js `fs` and `path` modules. If validation fails, print a user-friendly error message using `chalk.red()` and exit the process with a non-zero exit code (e.g., 1).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                ".action()",
                "run",
                "<repository-path>"
              ],
              "technology_hints": [
                "Node.js",
                "fs",
                "path",
                "chalk.red()"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of creating a robust CLI command. It directly implements the 'Fail-Fast' and 'Specific Error Handling' non-functional requirements by validating the primary input path before proceeding.",
              "sequence_critique": "The sequence is correct. Validating the repository path is the logical first action within the command handler, preceding any file system operations within that path.",
              "clarity_critique": "The instruction to 'exit the process with a non-zero exit code (e.g., 1)' is inconsistent with the Project Constitution. Appendix A.6 of the architecture document defines a specific Exit-Code Taxonomy. The step should explicitly require using the designated exit code for an input data error (e.g., `65` for `EX_DATAERR`) to ensure compliance with the 'Interoperability (Specific Exit Codes)' NFR."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Implement a helper function to read and parse the `IRONCLAD_MAX_PARALLEL` environment variable. This function should: 1. Read the variable. 2. If it's not set or is empty, return the default value of `1` as specified in the architecture document (section 3.2). 3. If it is set, parse it as an integer. If parsing fails or the value is less than 1, log a warning and use the default value. Call this function from your `run` command's action handler.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "IRONCLAD_MAX_PARALLEL",
                "run` command's action handler"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It implements a core configuration requirement for the `ironclad generate run` command, directly supporting the Task Runner's responsibility to manage concurrency as defined in the constitution (Section 3.2).",
              "sequence_critique": "The sequence is logical. Creating this helper function is a necessary prerequisite for implementing the Task Runner's job management logic, making it an appropriate early step within this task.",
              "clarity_critique": "The step is clear and actionable. However, it relies on a default value (`1`) from Section 3.2 of the constitution, which is contradicted by Appendix A.4 (which lists a default of `4`). While the step's instruction to use `1` is explicit and unambiguous for an AI agent, this underlying inconsistency in the source document should be resolved to prevent future confusion."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Implement the module discovery logic as a separate asynchronous function `discoverModules(repoPath: string): Promise<string[]>`. This function should: 1. Construct the path to the `contracts` directory inside `repoPath`. 2. Read all file entries in the `contracts` directory. 3. Filter for files ending in `.json`. 4. Map the filenames to module names by removing the `.json` extension. 5. Handle potential errors, such as the `contracts` directory not existing, by throwing a specific error. Hint: Use `fs.promises` for asynchronous file system operations.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "discoverModules",
                "repoPath",
                "contracts"
              ],
              "technology_hints": [
                "fs.promises"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the `Task Runner`'s responsibilities as defined in Section 3.2 of the architecture document. It implements the crucial first action of discovering which modules to process by enumerating files in the `contracts` directory, which is consistent with the `project_file_map`.",
              "sequence_critique": "The internal sequence of operations (construct path, read directory, filter, map to module names) is logical and complete for the function's purpose. This function is a necessary prerequisite before the Task Runner can begin preparing workspaces or spawning jobs.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. The explicit function signature (`discoverModules(repoPath: string): Promise<string[]>`), step-by-step breakdown, specific error handling requirement, and technology hint (`fs.promises`) provide a precise and unambiguous specification."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Integrate the implemented logic into the `run` command's action handler. The handler should now be an `async` function that: 1. Performs the repository path validation. 2. Reads the concurrency setting. 3. Calls `discoverModules` with the validated repository path, wrapping the call in a try/catch block to handle errors gracefully. 4. For now, log the results to the console using `chalk` for clarity (e.g., `Found 15 modules to generate.`, `Concurrency limit set to 4.`). This output confirms the command works and serves as a placeholder for the future job spawning logic.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "run",
                "discoverModules"
              ],
              "technology_hints": [
                "async",
                "chalk",
                "console"
              ]
            },
            "step_critique": {
              "alignment_critique": "Steps are perfectly aligned. They implement the initial, foundational responsibilities of the `generate run` command as defined in the architecture document (Section 3.2), such as validating the repository path, discovering modules, and reading the concurrency setting. This directly contributes to the task's goal.",
              "sequence_critique": "The sequence is logical and correct. Validating the primary input (repository path) must occur first before it can be used by subsequent functions like `discoverModules`. Reading configuration and logging the results are placed appropriately.",
              "clarity_critique": "The instructions are clear and highly actionable for an AI agent. The use of `async`, specific tools (`chalk`), example log messages, and the explicit instruction to use a `try/catch` block provide sufficient detail to implement the step correctly."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Write comprehensive unit tests for the new functionality. Create a test file `src/commands/generate.test.ts` using Jest. Focus on testing the helper functions in isolation. Use a library like `mock-fs` or Jest's built-in mocking to test: 1. The `discoverModules` function with various file system states (empty `contracts` dir, dir with JSON files, dir with mixed files, non-existent dir). 2. The logic for parsing the `IRONCLAD_MAX_PARALLEL` environment variable (test with it set, unset, and set to an invalid value).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Use a library like `mock-fs`"
              ],
              "key_entities_dependencies": [
                "src/commands/generate.test.ts",
                "discoverModules",
                "contracts dir",
                "IRONCLAD_MAX_PARALLEL"
              ],
              "technology_hints": [
                "Jest",
                "mock-fs",
                "Jest's built-in mocking"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It focuses on unit testing core responsibilities of the Task Runner (`discoverModules`, `IRONCLAD_MAX_PARALLEL` parsing), which is the central component of the `ironclad generate run` command and the current development phase, as detailed in the constitution's section 3.2.",
              "sequence_critique": "The logical sequence is sound. Testing helper functions is a standard and necessary part of implementing the larger command feature.",
              "clarity_critique": "The step is mostly clear and actionable. However, it proposes creating `src/commands/generate.test.ts`. The constitution's file map shows that integration tests for commands are placed in a `__tests__` subdirectory (e.g., `src/commands/__tests__/blueprint.integration.test.ts`). To maintain a consistent and clear testing structure within the `commands` module, the path should be specified as `src/commands/__tests__/generate.unit.test.ts` to co-locate it with other command tests and distinguish it as a unit test."
            }
          }
        }
      ],
      "Task 2.2: Develop the Task Runner's module discovery logic to parse `contracts/*.json` files.": [
        {
          "id": "step_1",
          "description": "Create a new directory `src/task-runner` to house the logic for the Task Runner component.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner",
                "Task Runner"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Creating a dedicated directory for the Task Runner component is a logical and necessary organizational step that mirrors the structure of other major components like `src/iba`, consistent with the project's architectural principle of separating concerns.",
              "sequence_critique": "The sequence is correct. Establishing the directory structure is the foundational prerequisite before any source files for the Task Runner can be created.",
              "clarity_critique": "The step is perfectly clear and actionable, specifying the exact directory path to be created."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Inside `src/task-runner`, create a new file named `module-discovery.ts`. This file will contain the logic for discovering module contracts.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner",
                "module-discovery.ts",
                "module contracts"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal. Creating a dedicated `module-discovery.ts` file within a new `src/task-runner` directory is a logical implementation of the 'Task Runner' component defined in the architecture, adhering to the NFR of maintainability.",
              "sequence_critique": "The sequence is logical. Creating the file is the correct prerequisite step before implementing the discovery logic within it.",
              "clarity_critique": "The instruction is clear and actionable, specifying the exact directory and file to be created."
            }
          }
        },
        {
          "id": "step_3",
          "description": "In `module-discovery.ts`, import the necessary Node.js modules: `fs/promises` for asynchronous file system operations and `path` for handling file paths.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "module-discovery.ts",
                "fs/promises",
                "path"
              ],
              "technology_hints": [
                "Node.js",
                "fs/promises",
                "path"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Importing `fs/promises` and `path` is a necessary prerequisite for implementing the module discovery logic, which fundamentally relies on file system interaction and path manipulation as described in the architecture.",
              "sequence_critique": "The step is in the correct logical sequence. Importing required modules is the standard first action when beginning implementation in a new file.",
              "clarity_critique": "The step is clear and unambiguous. It specifies the target file (`module-discovery.ts`) and the exact Node.js modules to be imported, leaving no room for misinterpretation."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Define and export an asynchronous function `discoverModules(contractsDir: string): Promise<string[]>`. Add JSDoc comments explaining that this function scans a directory for module contract files (`*.json`) and returns an array of module names derived from the filenames.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "discoverModules",
                "contractsDir",
                "*.json"
              ],
              "technology_hints": [
                "JSDoc",
                "Promise"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. It directly implements a core responsibility of the Task Runner, as specified in the architecture document (Section 3.2): 'Discover all module names by enumerating `contracts/*.json` files in the initialized repository.' This is a fundamental requirement for the generation pipeline.",
              "sequence_critique": "The sequence is logical. Defining the function to discover modules is a necessary first step before any orchestration or job spawning can occur.",
              "clarity_critique": "The instruction is clear and actionable. It explicitly defines the function signature, its asynchronous nature, and the logic for deriving module names (from `*.json` filenames), which is consistent with the `contracts/<ModuleName>.json` structure established in the Project Constitution."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Implement the happy-path logic for `discoverModules`. Use `fs.promises.readdir` to get all entries in `contractsDir`. Filter this list to include only files ending with `.json`. For each matching file, use `path.basename(file, '.json')` to extract the module name. Return the array of names.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "discoverModules",
                "fs.promises.readdir",
                "contractsDir",
                ".json",
                "path.basename"
              ],
              "technology_hints": [
                "fs.promises",
                "path"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal. It directly implements a core responsibility of the Task Runner as defined in the Project Constitution (Section 3.2), which is to 'Discover all module names by enumerating `contracts/*.json` files in the initialized repository.'",
              "sequence_critique": "The sequence of operations (read directory, filter for JSON files, extract base names) is logical, correct, and represents a standard approach for this task in a Node.js environment. No prerequisite steps are missing for this happy-path implementation.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. By specifying the exact Node.js modules and functions to use (`fs.promises.readdir`, `path.basename`), the step leaves no room for ambiguity and is well-suited for direct implementation."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Enhance `discoverModules` with robust error handling. Wrap the file system logic in a `try...catch` block. If `fs.promises.readdir` throws an error (e.g., directory not found), catch it and re-throw a new, more specific error, like `new Error(\"Failed to discover modules: Could not read contracts directory at '\" + contractsDir + \"'. Original error: \" + error.message)`. This aligns with the system's need for clear failure reporting.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "discoverModules",
                "fs.promises.readdir",
                "contractsDir",
                "Error"
              ],
              "technology_hints": [
                "Node.js fs.promises",
                "try...catch"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task and the Project Constitution. It directly implements the non-functional requirement for 'Reliability (Specific Error Handling)' by creating informative, specific errors for file system failures, which is critical for a robust CLI tool.",
              "sequence_critique": "The internal logic of the step is sound and follows standard best practices for error handling (wrapping a low-level operation in a try/catch block and re-throwing a more specific, high-level error). This is a logical and necessary part of developing the `discoverModules` function.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify the function to modify (`discoverModules`), the exact system call to wrap (`fs.promises.readdir`), the error handling pattern (`try...catch`), and provide a precise, detailed example of the error message to construct. This leaves no ambiguity for implementation."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Create a corresponding test file `src/task-runner/module-discovery.test.ts`. Import the `discoverModules` function and the `fs/promises` module for mocking.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner/module-discovery.test.ts",
                "discoverModules",
                "fs/promises"
              ],
              "technology_hints": [
                "TypeScript",
                "Node.js"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal of developing module discovery logic, as creating a corresponding test file is a fundamental practice for ensuring the correctness and reliability of the implementation, which are core project tenets.",
              "sequence_critique": "The sequence is logical. Creating a test file skeleton is a standard step in a test-driven or test-first development workflow, immediately following or concurrent with the creation of the source file.",
              "clarity_critique": "The step's instructions are clear, but the proposed file path `src/task-runner/module-discovery.test.ts` deviates from the established file structure in the Project Constitution. The constitution places the logic for the `generate` command, which acts as the Task Runner, in `src/commands/generate.ts`. Therefore, the test file should be located at a path like `src/commands/__tests__/generate.unit.test.ts` or a similar path consistent with the established `src/commands/` module structure to avoid architectural drift."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Set up mocking for the `fs/promises` module using `jest.mock('fs/promises')`. This is crucial for isolating the tests from the actual file system, ensuring they are fast and deterministic.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "fs/promises",
                "jest.mock"
              ],
              "technology_hints": [
                "jest",
                "fs/promises"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. The task is to develop module discovery logic, which requires reading the file system. Isolating tests from the actual file system by mocking it is a crucial and standard practice for ensuring test reliability and speed.",
              "sequence_critique": "The step is logically sequenced. Setting up mocks for external dependencies like the file system is a necessary prerequisite before writing the specific test cases that will rely on those mocks.",
              "clarity_critique": "The instruction is clear, but it deviates from an established project pattern. The Project Constitution explicitly lists `mock-fs` as the chosen dependency for mocking the file system in tests (e.g., for the `scaffolder` in Task 1.9). This step proposes using `jest.mock('fs/promises')` instead. While a valid technique, this introduces an inconsistency. For consistency and potentially easier setup of complex virtual directory structures, the established `mock-fs` library should be used."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Write a unit test for the happy path. Mock `fs.promises.readdir` to return an array of filenames including valid contracts (`['UserService.json', 'EmailAdapter.json']`) and other files that should be ignored (`['README.md', '.DS_Store']`). Assert that `discoverModules` returns `['UserService', 'EmailAdapter']`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "fs.promises.readdir",
                "UserService.json",
                "EmailAdapter.json",
                "README.md",
                ".DS_Store",
                "discoverModules",
                "UserService",
                "EmailAdapter"
              ],
              "technology_hints": [
                "fs.promises"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. It defines a unit test for the `discoverModules` function, a core responsibility of the Task Runner as specified in the architecture (Section 3.2), directly contributing to the task's goal.",
              "sequence_critique": "The sequence is logical, representing a standard 'happy path' test case that should precede edge-case testing.",
              "clarity_critique": "The step is exceptionally clear. It specifies the exact function to mock, the mock's return value, and the expected assertion, making it highly actionable."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Write a unit test for the case where the contracts directory is empty. Mock `fs.promises.readdir` to return an empty array. Assert that `discoverModules` returns an empty array.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "contracts directory",
                "fs.promises.readdir",
                "discoverModules"
              ],
              "technology_hints": [
                "Node.js fs.promises API"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It creates a unit test for the `discoverModules` function, a core responsibility of the Task Runner as defined in the Project Constitution (Section 3.2). Testing this key edge case supports the project's Non-Functional Requirement for reliability.",
              "sequence_critique": "The sequence is logical. Testing the 'empty' edge case first is a standard and effective approach in a test-driven development workflow, establishing a baseline before handling more complex cases.",
              "clarity_critique": "The step is clear and actionable. It specifies the exact scenario, the mocking strategy (`fs.promises.readdir`), and the precise assertion to be made, leaving no ambiguity."
            }
          }
        },
        {
          "id": "step_11",
          "description": "Write a unit test for the error handling case. Mock `fs.promises.readdir` to reject with an error (e.g., `new Error('ENOENT')`). Assert that calling `discoverModules` throws the expected custom error.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "fs.promises.readdir",
                "new Error('ENOENT')",
                "discoverModules",
                "custom error"
              ],
              "technology_hints": [
                "fs.promises",
                "Mock",
                "unit test"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task and overall project goals. Testing the error handling for module discovery is critical for fulfilling the 'Reliability (Fail-Fast)' and 'Reliability (Specific Error Handling)' non-functional requirements defined in the constitution.",
              "sequence_critique": "The step is a logical and necessary part of a comprehensive testing suite for the `discoverModules` function. It correctly focuses on edge cases and failure modes, which is essential after establishing the 'happy path' functionality.",
              "clarity_critique": "The step is clear and highly actionable. It specifies the function to mock, the behavior to simulate (rejection), and the expected outcome (throwing an error). The instruction is fully consistent with the project's testing strategy using Jest."
            }
          }
        },
        {
          "id": "step_12",
          "description": "Review the code in `module-discovery.ts` and `module-discovery.test.ts` for clarity, correctness, and adherence to project standards. Ensure all JSDoc comments are complete and accurate.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "module-discovery.ts",
                "module-discovery.test.ts",
                "JSDoc comments"
              ],
              "technology_hints": [
                "TypeScript",
                "JSDoc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Reviewing the module discovery code is a crucial quality assurance measure for fulfilling the task's objective, which is a core responsibility of the Task Runner as defined in the constitution (Section 3.2).",
              "sequence_critique": "The step is logically sequenced. A code review step naturally follows a development step to ensure quality before integration.",
              "clarity_critique": "The prompt is clear and actionable. It specifies the files to review, the criteria for review (clarity, correctness, standards), and a specific focus on JSDoc, which aligns with the project's maintainability goals."
            }
          }
        }
      ],
      "Task 2.3: Implement the Task Runner's workspace manager for creating and populating isolated job directories.": [
        {
          "id": "step_1",
          "description": "Create a new file at `src/types/contract.ts`. Based on the JSON schema in `A.2 Canonical JSON Schemas & Examples` of the architecture document, define and export a TypeScript interface named `ModuleContract`. Ensure all properties like `name`, `dependencies`, `functionSignatures`, and `instructions` are correctly typed.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/types/contract.ts",
                "A.2 Canonical JSON Schemas & Examples",
                "architecture document",
                "ModuleContract",
                "name",
                "dependencies",
                "functionSignatures",
                "instructions"
              ],
              "technology_hints": [
                "TypeScript",
                "JSON schema"
              ]
            },
            "step_critique": {
              "alignment_critique": "The action of defining the `ModuleContract` TypeScript interface is a logical and necessary prerequisite for implementing the workspace manager. The manager will need a strongly-typed representation of the contract data to determine dependencies and populate the workspace correctly. The step is well-aligned with the task's objective.",
              "sequence_critique": "No issues. As a foundational data type definition, this is a logical first step for the task.",
              "clarity_critique": "The instruction to create the file at `src/types/contract.ts` directly contradicts the Project Constitution's `project_file_map`. The constitution explicitly designates `src/core/types.ts` as the location for 'core TypeScript types and interfaces for the system, such as ModuleContract'. The step must be corrected to place the type definition in `src/core/types.ts` to avoid architectural deviation."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a new file at `src/task-runner/workspace-manager.ts`. Import `ModuleContract`. Define and export an asynchronous function `prepareWorkspace`. It should accept `moduleName: string`, `contracts: Map<string, ModuleContract>`, and `repoDir: string` as arguments, and return a `Promise<string>` that resolves with the path to the created workspace. Add detailed JSDoc explaining its purpose, parameters, and return value, referencing section 3.2 of the architecture document.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "referencing section 3.2 of the architecture document"
              ],
              "key_entities_dependencies": [
                "src/task-runner/workspace-manager.ts",
                "ModuleContract",
                "prepareWorkspace",
                "moduleName",
                "contracts",
                "repoDir",
                "section 3.2 of the architecture document"
              ],
              "technology_hints": [
                "TypeScript",
                "JSDoc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal. Creating the `prepareWorkspace` function is the central piece of implementing the workspace manager, directly corresponding to the Task Runner's responsibilities outlined in section 3.2 of the architecture document.",
              "sequence_critique": "The sequence is logical. Defining the function signature and creating its file is the correct foundational step before implementing the detailed logic of directory creation and file population in subsequent steps.",
              "clarity_critique": "The step is exceptionally clear and actionable. It precisely specifies the file path, function name, exact signature (parameters and return type), and documentation requirements, leaving no room for ambiguity."
            }
          }
        },
        {
          "id": "step_3",
          "description": "In `workspace-manager.ts`, implement the directory creation logic within `prepareWorkspace`. Use Node's `path` module for constructing paths and `fs/promises` for file system operations. The function should create the main workspace directory at `<repoDir>/.tmp/ironclad_tasks/<moduleName>` and the necessary subdirectories within it: `contracts`, `idl`, `src`, `src/modules`, and `prompt_templates`. Use the `{ recursive: true }` option to ensure parent directories are created.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "workspace-manager.ts",
                "prepareWorkspace",
                "<repoDir>",
                "<moduleName>"
              ],
              "technology_hints": [
                "Node.js",
                "path",
                "fs/promises"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the Project Constitution. It directly implements the core architectural principle of isolated task workspaces as defined in the 'High-Level Architecture Overview' (Section 2), 'Task Runner Responsibilities' (Section 3.2), and 'Data Model' (Section 4), which are crucial for achieving the non-functional requirements of 'Reliability (Fault Isolation)' and 'Testability (Test Isolation)'.",
              "sequence_critique": "The sequence is logical and correct. Creating the directory structure is the necessary first action within the `prepareWorkspace` function, as it must be done before the directories can be populated with files in subsequent steps.",
              "clarity_critique": "The step is very clear but has a minor omission. The constitution's `project_file_map` and the 'Job' component's responsibilities (Section 3.3) specify that generated tests are written to `src/modules/__tests__/<ModuleName>.test.ts`. To ensure the workspace is fully prepared for the Job process, the list of subdirectories to create should also include `src/modules/__tests__`."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Implement the logic to symlink the primary module's files and the global context files into the newly created workspace. Use `fs/promises.symlink`. Specifically, create symlinks for: \n1. `<repoDir>/contracts/<moduleName>.json` -> `<workspaceDir>/contracts/<moduleName>.json`\n2. `<repoDir>/src/modules/I<moduleName>.ts` -> `<workspaceDir>/src/modules/I<moduleName>.ts`\n3. `<repoDir>/system_context.json` -> `<workspaceDir>/system_context.json`\n4. `<repoDir>/validation_dsl_spec.md` -> `<workspaceDir>/validation_dsl_spec.md`",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "fs/promises.symlink",
                "repoDir",
                "moduleName",
                "workspaceDir",
                "contracts/<moduleName>.json",
                "src/modules/I<moduleName>.ts",
                "system_context.json",
                "validation_dsl_spec.md"
              ],
              "technology_hints": [
                "fs/promises.symlink"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal of populating a job workspace. The action of symlinking files directly contributes to creating an isolated environment for a job, as specified in the constitution's description of the Task Runner.",
              "sequence_critique": "The step is critically incomplete and therefore logically flawed. It omits several file types that are essential for a module generation job, as defined by the Project Constitution (Sections 3.2 and 4). Specifically, it fails to include symlinks for:\n1. Contracts of the module's dependencies (`contracts/<DependencyName>.json`).\n2. Interface stubs of the module's dependencies (`src/modules/I<DependencyName>.ts`).\n3. The entire shared `idl/` directory, which contains essential shared types.\n4. The prompt templates from `prompt_templates/` needed to construct the prompt.\nWithout these required files, downstream validation steps (like `tsc`) and prompt generation would fail.",
              "clarity_critique": "The instructions for the files that *are* listed are very clear, specific, and actionable. The use of path placeholders and the explicit suggestion of the `fs/promises.symlink` function is excellent. The issue is not a lack of clarity in what is asked, but a critical omission of required actions."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Implement the logic to symlink the files for all of the module's dependencies. First, retrieve the module's contract from the `contracts` map to get its `dependencies` array. Then, loop through each dependency name and create symlinks for its contract and interface stub into the workspace's `contracts` and `src/modules` subdirectories, respectively.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "contracts",
                "dependencies",
                "contract",
                "interface stub",
                "workspace",
                "src/modules"
              ],
              "technology_hints": [
                "symlink"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. It directly implements a core responsibility of the Task Runner's workspace manager as defined in the Project Constitution (Sections 3.2 and 4), which is to populate the isolated workspace with dependency artifacts (contracts and interface stubs). This is essential for the subsequent validation steps (e.g., `tsc`) within the Job process.",
              "sequence_critique": "The sequence is logical. It correctly specifies retrieving the list of dependencies for a module before iterating through them to create the necessary symlinks. This step correctly relies on the assumption that the upstream IBA process has already validated the dependency graph, so it doesn't need to handle cases of missing dependency files.",
              "clarity_critique": "The instructions are clear, specific, and actionable. They precisely state what data to retrieve (the `dependencies` array from the module's contract), what actions to perform (create symlinks for both the contract and the interface stub), and where to place the resulting links (the `contracts` and `src/modules` subdirectories within the workspace)."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Implement the logic to symlink the shared `idl` and `prompt_templates` directories from the repository root into the workspace. The destination paths should be `<workspaceDir>/idl` and `<workspaceDir>/prompt_templates`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "idl",
                "prompt_templates",
                "<workspaceDir>"
              ],
              "technology_hints": [
                "symlink"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the Project Constitution. Section 3.2 (Task Runner Responsibilities) and Section 4 (Data Model) both specify that the Task Runner must prepare an isolated workspace by copying or symlinking required files, explicitly including IDLs and templates. This step directly implements that core requirement.",
              "sequence_critique": "The step represents a discrete, logical action within the broader task of populating a workspace. It logically follows the creation of the workspace directory and can be performed in parallel with copying other required files like module contracts.",
              "clarity_critique": "The instruction is clear and actionable. It specifies the exact action (symlink), the source directories (`idl`, `prompt_templates`), their location (repository root), and the explicit destination paths within the workspace. This level of detail is sufficient for unambiguous implementation."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Refactor the file-linking logic to be more robust. Before attempting to create a symlink, use `fs/promises.access` to check if the source file or directory exists. If it does not, throw a specific, informative `Error` stating which file is missing and for which module's workspace it was required. Wrap file operations in `try...catch` blocks for clean error handling.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "file-linking logic",
                "symlink",
                "fs/promises.access",
                "Error",
                "module's workspace"
              ],
              "technology_hints": [
                "fs/promises"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It directly contributes to the task of implementing a robust workspace manager by adding essential error handling and input validation (checking for file existence). This aligns with the project's non-functional requirements for Reliability (Fail-Fast, NFR 9) and Specific Error Handling (NFR 12).",
              "sequence_critique": "The sequence is logical. Adding robustness and specific error handling is a natural next step after establishing the basic file-linking mechanism. The internal logic of checking before linking is a standard, correct pattern.",
              "clarity_critique": "The step is clear and actionable. It specifies the function to use (`fs/promises.access`), the condition to check, the action to take on failure (throw an `Error`), and the required content for the error message, making it straightforward to implement."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Set up the test environment for the workspace manager. Create `src/task-runner/workspace-manager.test.ts`. Install `mock-fs` as a a dev dependency (`npm install --save-dev mock-fs @types/mock-fs`). In the test file, import `mock-fs` and the `prepareWorkspace` function. Use `beforeEach` and `afterEach` hooks to activate and restore the real file system around each test.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install `mock-fs` as a a dev dependency (`npm install --save-dev mock-fs @types/mock-fs`)"
              ],
              "key_entities_dependencies": [
                "src/task-runner/workspace-manager.test.ts",
                "prepareWorkspace",
                "beforeEach",
                "afterEach"
              ],
              "technology_hints": [
                "mock-fs",
                "npm",
                "@types/mock-fs"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are well-aligned with the task of creating a test environment for a file-system-intensive component. The use of 'mock-fs' is consistent with the project constitution's established testing patterns (NFR: 'Maintainability (Tooling Consistency)') and dependencies, reinforcing architectural coherence.",
              "sequence_critique": "The sequence is logical: create the test file, install its specific dependencies, and then set up the necessary test hooks. This is a standard and correct workflow.",
              "clarity_critique": "The instructions are clear, explicit, and actionable for an AI agent, specifying the exact file to create, the precise packages to install with the correct command, and the standard testing pattern ('beforeEach'/'afterEach') to implement."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Write a unit test for `prepareWorkspace` that covers a module with **no dependencies**. Use `mock-fs` to construct a virtual repository containing a single module's contract, its interface stub, a `system_context.json`, a `validation_dsl_spec.md`, and an `idl` directory. Call `prepareWorkspace` and assert that all expected directories and symlinks are created correctly in the virtual file system.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "prepareWorkspace",
                "module's contract",
                "interface stub",
                "system_context.json",
                "validation_dsl_spec.md",
                "idl",
                "symlinks"
              ],
              "technology_hints": [
                "mock-fs"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It directly contributes to implementing the Task Runner's workspace manager by specifying a unit test for its core function, `prepareWorkspace`. The use of `mock-fs` and the focus on a simple, no-dependency module are consistent with the project's established testing strategy and architectural principles.",
              "sequence_critique": "The sequence is logical. Testing the base case of a module with no dependencies is the correct foundational step before testing more complex scenarios, such as modules with dependencies or error conditions.",
              "clarity_critique": "The step is clear and actionable, but has a minor omission. The Project Constitution (Sections 3.2 and 4) indicates that prompt templates (e.g., from `prompt_templates/`) are also copied or linked into the workspace. To be fully comprehensive, the test's virtual repository setup should also include a `prompt_templates` directory, and the assertions should verify its contents are correctly linked into the workspace."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Write a unit test for a module **with dependencies**. Expand the mock file system to include contracts and interface stubs for two dependency modules. Call `prepareWorkspace` for the main module. Assert that the workspace correctly contains symlinks for the main module's files as well as the contract and interface files for both of its dependencies.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "mock file system",
                "contracts",
                "interface stubs",
                "prepareWorkspace",
                "symlinks"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal of implementing the workspace manager. It directly tests a core responsibility of the Task Runner\u2014populating a workspace with all necessary dependency artifacts (contracts, interface stubs)\u2014as explicitly required by Section 3.2 of the Project Constitution. This is critical for the system's reliability and fault isolation.",
              "sequence_critique": "The sequence is logical. It correctly follows the Arrange-Act-Assert pattern for a unit test. This step represents a natural increase in complexity from a likely preceding test for a module without dependencies, which is a sound testing progression.",
              "clarity_critique": "The step is exceptionally clear and actionable. It specifies the exact scenario (module with dependencies), the required test setup (mock contracts and interface stubs), and the precise assertions to be made (verifying the creation of symlinks for the module's own files and its dependencies' files). The instruction is unambiguous."
            }
          }
        },
        {
          "id": "step_11",
          "description": "Write a unit test for **error handling**. Configure the mock file system so that a required file is missing (e.g., the interface stub for a dependency). Call `prepareWorkspace` and use `expect(...).rejects.toThrow()` to assert that the function correctly throws an error with a message that identifies the missing file.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "prepareWorkspace",
                "mock file system",
                "interface stub",
                "expect(...).rejects.toThrow()"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task and overall project goals. Testing for missing dependency files is a critical error handling scenario for the `prepareWorkspace` function, directly supporting the Non-Functional Requirements for 'Reliability (Fail-Fast)' and 'Reliability (Specific Error Handling)' defined in the constitution.",
              "sequence_critique": "The step is logically placed. Testing failure paths, such as this one, is a standard and necessary part of unit testing a function, typically following the implementation of the success-path test.",
              "clarity_critique": "The step is exceptionally clear and actionable. It precisely defines the test scenario (missing dependency interface stub), the expected behavior (throw an error), the assertion mechanism (`.rejects.toThrow()`), and the required quality of the error message (identifying the specific missing file). No ambiguity is present."
            }
          }
        }
      ],
      "Task 2.4: Implement the prompt constructor to generate `prompt.txt` and `prompt.hash` from templates and context files.": [
        {
          "id": "step_1",
          "description": "Install the `handlebars` library, which will be used for populating prompt templates. Add it as a production dependency.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install the `handlebars` library"
              ],
              "key_entities_dependencies": [
                "prompt templates"
              ],
              "technology_hints": [
                "handlebars"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. The project's architecture document (Section A.3) shows a prompt template using Handlebars syntax, making the installation of the `handlebars` library a direct and necessary action for implementing the prompt constructor.",
              "sequence_critique": "The sequence is logical. Installing a required dependency is the correct first step before implementing code that relies on it.",
              "clarity_critique": "The instruction is clear, unambiguous, and directly actionable. It correctly specifies that `handlebars` should be a production dependency."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a new file `src/prompt-constructor.ts` to encapsulate all logic related to prompt generation. Also create its corresponding test file `src/prompt-constructor.test.ts`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/prompt-constructor.ts",
                "src/prompt-constructor.test.ts"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step directly supports the task's goal. However, the proposed file path `src/prompt-constructor.ts` is architecturally inconsistent. According to the Project Constitution's file map and component design (Section 3.2), prompt construction is a responsibility of the Task Runner. Therefore, to maintain modularity, this file should be located at `src/task-runner/prompt-constructor.ts`.",
              "sequence_critique": "The sequence is logical. Creating the source file and its corresponding test file is a standard and correct first step for implementing a new, testable module.",
              "clarity_critique": "The instructions are clear and actionable, specifying the exact filenames for the new module and its test suite."
            }
          }
        },
        {
          "id": "step_3",
          "description": "In `src/prompt-constructor.ts`, define the necessary interfaces for type safety. Create `GeneratePromptOptions` which will include `workspaceDir`, `templatePath`, `moduleName`, and an optional `failureReportPath`. Also create `GeneratePromptResult` which will contain `promptContent: string` and `promptHash: string`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/prompt-constructor.ts",
                "GeneratePromptOptions",
                "workspaceDir",
                "templatePath",
                "moduleName",
                "failureReportPath",
                "GeneratePromptResult",
                "promptContent",
                "promptHash"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Defining interfaces for the prompt constructor's inputs (`GeneratePromptOptions`) and outputs (`GeneratePromptResult`) is a foundational step that directly supports the implementation. The specified properties correctly reflect the requirements outlined in the constitution (Sections 3.2 and 3.3), including the optional failure report for the retry loop's prompt amendment.",
              "sequence_critique": "The sequence is logical. Defining interfaces before implementing the function logic is a standard best practice in TypeScript development that ensures type safety from the start.",
              "clarity_critique": "The step is highly clear and actionable. However, it specifies creating the file at `src/prompt-constructor.ts`. For better architectural consistency with the `project_file_map`, which establishes a `src/task-runner` directory for Task Runner logic, placing this new file at `src/task-runner/prompt-constructor.ts` would be preferable to co-locate related components."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Implement the main exported function `generatePrompt(options: GeneratePromptOptions): Promise<GeneratePromptResult>`. This function will orchestrate reading all necessary files, constructing the prompt, and calculating the hash.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "generatePrompt",
                "GeneratePromptOptions",
                "GeneratePromptResult"
              ],
              "technology_hints": [
                "Promise",
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Implementing the `generatePrompt` orchestrator function is the central action required to fulfill the task's goal of creating `prompt.txt` and `prompt.hash`, consistent with the Task Runner's responsibilities outlined in the architecture (Section 3.2).",
              "sequence_critique": "The implied sequence of operations within the function (reading files, constructing the prompt, then calculating the hash) is logical and correct.",
              "clarity_critique": "The step is clear and actionable, providing a specific function signature and purpose. However, it implicitly relies on a templating engine (as suggested by the syntax in Appendix A.3 of the architecture doc), but no such library is listed as a dependency in the Project Constitution. A more complete instruction set would either specify the library to use or include a prerequisite step to add one to the project."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Inside `prompt-constructor.ts`, implement the data gathering logic. This involves asynchronously reading the module's contract file (`<workspaceDir>/contracts/<moduleName>.json`), the system context (`<workspaceDir>/system_context.json`), all IDL files (`<workspaceDir>/idl/*.ts`), and the optional failure report from `failureReportPath`. Consolidate the content of all IDL files into a single string. Handle file-not-found errors gracefully by throwing informative exceptions.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "prompt-constructor.ts",
                "<workspaceDir>/contracts/<moduleName>.json",
                "<workspaceDir>/system_context.json",
                "<workspaceDir>/idl/*.ts",
                "failureReportPath"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "Steps are perfectly aligned. The data gathering logic directly sources the components (`contract`, `systemContext`, `idlSnippets`, `previousFailure`) required by the `prompt_templates/module_prompt.tmpl` defined in the architecture document (Appendix A.3).",
              "sequence_critique": "The sequence is logical. Gathering all necessary data is the correct prerequisite step before attempting to populate the template in a subsequent step.",
              "clarity_critique": "The instruction to 'Consolidate the content of all IDL files into a single string' is slightly ambiguous. For better determinism and to help the IMG parse the input, it should specify *how* to consolidate them, for example, by adding a file header comment like `// --- File: <filename> ---` before the content of each IDL file."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Implement the Handlebars rendering logic. Read the template file from `options.templatePath`. Register a custom Handlebars helper named `toJsonPretty` that takes a JavaScript object and returns its pretty-printed JSON string representation (`JSON.stringify(obj, null, 2)`). Compile the template and render it with the gathered data.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "options.templatePath",
                "toJsonPretty",
                "JSON.stringify(obj, null, 2)"
              ],
              "technology_hints": [
                "Handlebars",
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. The Project Constitution, specifically Appendix A.3, shows a sample prompt template that explicitly requires a `toJsonPretty` helper, which this step directs to be created. This directly contributes to the core goal of generating a structured prompt for the IMG.",
              "sequence_critique": "The sequence of actions described within the step (read template, register helper, compile, render) is logical and correct for using a templating engine like Handlebars. It correctly assumes the necessary data has already been gathered as a prerequisite.",
              "clarity_critique": "The step is exceptionally clear and actionable. It specifies the technology (Handlebars), the helper function's name (`toJsonPretty`), and the exact implementation logic for the helper (`JSON.stringify(obj, null, 2)`), leaving no ambiguity for an AI coding agent."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Implement the hashing logic within the `generatePrompt` function. After rendering the prompt content, use the Node.js `crypto` module to calculate its SHA-256 hash. The final function should return an object matching the `GeneratePromptResult` interface.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "generatePrompt",
                "GeneratePromptResult"
              ],
              "technology_hints": [
                "Node.js crypto module",
                "SHA-256"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal and the project's core architectural requirements. It directly implements the `prompt.hash` generation responsibility assigned to the Task Runner in section 3.2 of the architecture, which is crucial for the system's determinism and auditability NFRs (specifically 'Auditability (Prompt Integrity)').",
              "sequence_critique": "The logical sequence is correct. Hashing the prompt content is the natural final action within a function designed to generate both the content and its integrity hash, occurring immediately after the content itself is rendered.",
              "clarity_critique": "The step is clear and highly actionable. It specifies the function scope (`generatePrompt`), the technology (`Node.js crypto` module), the algorithm (`SHA-256`), and the expected return contract (`GeneratePromptResult` interface), leaving little room for ambiguity for an AI coding agent."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Now, switch to `src/prompt-constructor.test.ts`. Install `mock-fs` as a dev dependency to simulate the file system structure required for testing.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install `mock-fs` as a dev dependency"
              ],
              "key_entities_dependencies": [
                "src/prompt-constructor.test.ts"
              ],
              "technology_hints": [
                "mock-fs"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Installing `mock-fs` is a necessary prerequisite for testing the prompt constructor, which is a file-I/O-intensive component. This aligns with the constitution's emphasis on testing and its specific approval of `mock-fs` for this purpose.",
              "sequence_critique": "The specified file path `src/prompt-constructor.test.ts` is architecturally inconsistent. According to the constitution (Section 3.2), the prompt constructor is a responsibility of the Task Runner. Therefore, its source and test files should be located within the `src/task-runner` directory (e.g., `src/task-runner/prompt-constructor.test.ts`) to maintain the established project structure.",
              "clarity_critique": "The instructions are clear, specific, and actionable for an AI agent."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Write a setup block for your tests using `mock-fs`. Create a mock directory structure representing a typical Job workspace. Include a sample `contracts/TestModule.json`, `idl/types.ts`, `system_context.json`, and a `prompt_templates/module_prompt.tmpl` that uses the variables and helpers specified in the architecture document (section A.3).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "Job workspace",
                "contracts/TestModule.json",
                "idl/types.ts",
                "system_context.json",
                "prompt_templates/module_prompt.tmpl",
                "architecture document (section A.3)",
                "variables and helpers"
              ],
              "technology_hints": [
                "mock-fs"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It establishes the necessary test preconditions (mock file system with required inputs) for implementing the prompt constructor, which is the core goal of Task 2.4. Using `mock-fs` is consistent with the project's established testing strategy.",
              "sequence_critique": "The logical sequence is correct. Creating test fixtures and a mock environment is a standard and necessary prerequisite before writing the implementation or the test assertions for a function that performs file I/O.",
              "clarity_critique": "The step is highly actionable but could be slightly more precise. While it correctly lists the required input files, it could explicitly state the expected directory structure of the mock workspace (e.g., `/.tmp/ironclad_tasks/TestModule/`) to perfectly mirror the layout defined in the Project Constitution's file map (Section 4), removing any potential ambiguity for the AI agent."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Write a unit test for the happy path. Call `generatePrompt` with options pointing to your mock files (without a `failureReportPath`). Assert that the returned `promptContent` contains the correctly substituted data from all mock files. Manually calculate the expected SHA-256 hash of the output and assert that the returned `promptHash` matches it.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "generatePrompt",
                "failureReportPath",
                "promptContent",
                "promptHash"
              ],
              "technology_hints": [
                "SHA-256"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It directly supports the task of implementing the prompt constructor by defining a test for its core 'happy path' functionality, including content substitution and hashing, which are both required by the Project Constitution's description of the Task Runner.",
              "sequence_critique": "The logical sequence is correct. Writing a unit test is a standard and appropriate step. The internal logic of the test (call function, assert content, assert hash) is also sound.",
              "clarity_critique": "The step is mostly clear, but the phrase 'Manually calculate the expected SHA-256 hash' is ambiguous for an AI agent. It should be rephrased to be more explicit, for example: 'Independently construct the expected prompt content string within the test, calculate its SHA-256 hash, and assert that the returned `promptHash` matches the independently calculated hash.' This clarifies that the test itself must perform the hash calculation for verification, rather than using a pre-computed value."
            }
          }
        },
        {
          "id": "step_11",
          "description": "Write a unit test for the retry/amendment path. Create a mock `failures.json` file in your virtual file system. Call `generatePrompt` with the `failureReportPath` option pointing to this file. Assert that the `===LAST_ERROR===` section and the pretty-printed JSON from `failures.json` are present in the `promptContent`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "failures.json",
                "generatePrompt",
                "failureReportPath",
                "===LAST_ERROR===",
                "promptContent"
              ],
              "technology_hints": [
                "virtual file system",
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Testing the prompt amendment logic for retries is a critical requirement explicitly defined in the Project Constitution (Section 3.3, Job Responsibilities and Appendix A.3, Prompt-Template Reference), and this step directly verifies its implementation.",
              "sequence_critique": "The sequence described for the unit test (Arrange: create mock file; Act: call function; Assert: check output) is logical and standard practice. No issues found.",
              "clarity_critique": "The step is exceptionally clear and actionable. It specifies the function to call (`generatePrompt`), the option to use (`failureReportPath`), the mock file to create (`failures.json`), and the exact content to assert (`===LAST_ERROR===` and pretty-printed JSON), which directly corresponds to the prompt template defined in the constitution."
            }
          }
        },
        {
          "id": "step_12",
          "description": "Write unit tests for error conditions. Test what happens when a required file (like the module contract) is missing. Ensure that your `generatePrompt` function throws a specific and helpful error. Test with an empty `idl` directory to ensure it handles that case without crashing.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "generatePrompt",
                "module contract",
                "idl"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task and overall project goals. Focusing on error conditions and edge cases directly implements the non-functional requirements for 'Reliability (Specific Error Handling)' and 'Reliability (Input Integrity)' as defined in the Project Constitution.",
              "sequence_critique": "The step describes a logical and necessary set of tests for the function. It correctly identifies a critical failure case (a missing required input file) and a common edge case (an empty optional input directory) that must be handled by the prompt constructor.",
              "clarity_critique": "The instructions are highly clear and actionable. The step specifies not just *what* to test (missing contract, empty IDL directory) but also the *expected behavior* (throw a specific error vs. handle without crashing), which is crucial for guiding implementation and ensuring robustness."
            }
          }
        },
        {
          "id": "step_13",
          "description": "Review the completed `prompt-constructor.ts` module. Add JSDoc comments to the exported function and interfaces, explaining their purpose, parameters, and return values. Ensure the code is clean, readable, and adheres to the project's coding standards.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "prompt-constructor.ts"
              ],
              "technology_hints": [
                "JSDoc",
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Reviewing, documenting (with JSDoc), and ensuring code quality are essential final actions for completing an implementation task. This directly supports the project's constitutional NFRs for 'Maintainability' and 'Maintainability (Code Style Consistency)'.",
              "sequence_critique": "The step is logically sequenced. It correctly assumes the initial implementation of `prompt-constructor.ts` is complete and positions itself as a final review and documentation step before the task is considered done.",
              "clarity_critique": "The instructions are clear and actionable. It specifies the target file, the type of documentation (JSDoc), the specific elements to document (exported function, interfaces), and the required content for the comments (purpose, parameters, return values), leaving no room for ambiguity."
            }
          }
        }
      ],
      "Task 2.5: Develop the concurrency manager in the Task Runner to respect the `IRONCLAD_MAX_PARALLEL` environment variable.": [
        {
          "id": "step_1",
          "description": "In the `TaskRunner` component, implement the logic to read and parse the `IRONCLAD_MAX_PARALLEL` environment variable. Create a utility function for this. The function should parse the value as a positive integer. According to the architecture spec (section 3.2.2), if the variable is not set, is not a valid number, or is less than 1, it must default to `1`. Add a log message at the start of the Task Runner's execution to state the determined concurrency limit. Write focused unit tests for this parsing logic, covering valid, invalid, and missing inputs.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "TaskRunner",
                "IRONCLAD_MAX_PARALLEL"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. It directly implements the `Scalability` NFR which mandates configuration via `IRONCLAD_MAX_PARALLEL` and the `Reliability (Configuration Graceful Degradation)` NFR which requires gracefully handling invalid inputs for such variables.",
              "sequence_critique": "The sequence is logical. Creating a focused utility function, logging its result, and writing unit tests for it is a sound, self-contained unit of work.",
              "clarity_critique": "The step is clear, but could be more precise by explicitly requiring a `WARN` level log message when the environment variable is invalid and a fallback is used, in addition to the standard `INFO` log stating the final determined limit. This would better fulfill the 'Reliability (Configuration Graceful Degradation)' NFR."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a new generic and reusable concurrency handler in a new file, e.g., `src/lib/ConcurrencyManager.ts`. This class will manage the worker pool. Its constructor should accept a concurrency limit, an array of task items, and an asynchronous `taskProcessor` function `(task: T) => Promise<V>`. It should have a single public method `run(): Promise<V[]>`. \n\n**Hint:** A robust implementation pattern is to have a pool of async 'worker' functions. Each worker runs in a loop, atomically consuming tasks from the shared task array (e.g., using `tasks.pop()`) and executing the `taskProcessor` on them until no tasks are left. The `run` method should create `concurrencyLimit` worker promises and use `Promise.allSettled` to await their completion, which ensures all tasks are attempted even if some fail, fulfilling the 'Fault Isolation' NFR. The `run` method should then return the results.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/lib/ConcurrencyManager.ts",
                "ConcurrencyManager",
                "taskProcessor",
                "run",
                "concurrencyLimit",
                "tasks.pop()",
                "'Fault Isolation' NFR"
              ],
              "technology_hints": [
                "Promise.allSettled"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Creating a generic concurrency manager directly implements the task's goal and supports the project's non-functional requirements for Scalability, Maintainability, and Fault Isolation. However, it proposes creating a new `src/lib/` directory, which is not defined in the Project Constitution's file map. While a reasonable architectural pattern, this represents a deviation that should be formally adopted.",
              "sequence_critique": "The step describes a self-contained, logical unit of work. The sequence is sound.",
              "clarity_critique": "The step has a minor but important ambiguity. It specifies the `run()` method should return `Promise<V[]>` but suggests using `Promise.allSettled`, which actually returns `Promise<PromiseSettledResult<V>[]>`. To fully align with the 'Fault Isolation' NFR mentioned, the method should return the settled results, and the signature in the prompt should be clarified to `Promise<PromiseSettledResult<V>[]>`."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Refactor the `TaskRunner` to integrate the new `ConcurrencyManager`. First, encapsulate all the logic for processing a single module (preparing workspace, constructing prompt, spawning the Job, and awaiting its result) into a new private async method, e.g., `private async processModule(moduleName: string): Promise<JobResult>`. Then, in the main execution flow of the Task Runner, instantiate the `ConcurrencyManager`, passing it the list of discovered module names, the concurrency limit, and a bound reference to your new `processModule` method as the `taskProcessor`. Finally, `await` the `run()` method of the manager and handle the array of settled promises to collect successes and failures.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "TaskRunner",
                "ConcurrencyManager",
                "processModule",
                "JobResult",
                "taskProcessor",
                "run"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. It directly implements the `TaskRunner`'s core concurrency responsibility as defined in the architecture (Section 3.2) and fulfills a key non-functional requirement for `Scalability`. The proposed refactoring into a `processModule` method and using a separate `ConcurrencyManager` also strongly supports the `Maintainability` NFR by promoting a clear separation of concerns, which is a core project principle.",
              "sequence_critique": "The sequence is logical and correct. Encapsulating the single-module processing logic into a dedicated method is the necessary prerequisite before integrating it with the new concurrency manager.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. The step specifies method names, signatures, and the exact parameters for the `ConcurrencyManager` constructor. The mention of using a 'bound reference' for the `processModule` method is particularly precise, correctly anticipating potential `this` context issues and making the instruction highly effective for an AI coding agent."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Add detailed logging around the task execution to improve UX and observability, as specified in the architecture. Log when a module task is starting and when it has completed (successfully or with failure). For example: `[Task Runner] Starting job for 'UserService'...` and `[Task Runner] Job for 'UserService' completed with status: SUCCESS`. This provides a clear, real-time view of the concurrent process.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "Task Runner",
                "IRONCLAD_MAX_PARALLEL",
                "UserService"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It directly supports the task of managing concurrent processes by making their lifecycle observable. It also directly implements the user experience (UX) and logging requirements detailed in Sections 6 and 11 of the Project Constitution, matching the specified CLI output mockups.",
              "sequence_critique": "The sequence is logical. Logging the start and completion of a task is a natural part of implementing the task management loop itself.",
              "clarity_critique": "The prompt is clear but could be more precise to ensure full constitutional compliance. While it correctly specifies the human-readable console output for UX (Section 6), it omits the constitution's core requirement for structured, machine-readable (JSON) logging for observability (Section 11). The prompt should be refined to instruct the implementation of a logging utility that supports both formats, ensuring the system meets both its UX and its more rigorous observability NFRs."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Create a comprehensive suite of unit tests for the `ConcurrencyManager` in `src/lib/ConcurrencyManager.test.ts`. Use Jest's fake timers (`jest.useFakeTimers()`) to control the flow of time and a mock `taskProcessor` that simulates work with a delay. Your tests must verify:\n1. **Throttling:** With more tasks than workers (e.g., 4 tasks, concurrency 2), ensure only `concurrency` tasks start at once.\n2. **Full Parallelism:** With fewer tasks than workers (e.g., 2 tasks, concurrency 4), ensure all tasks start at once.\n3. **Fault Isolation:** If a mocked task processor throws an error, ensure other concurrent and queued tasks are still executed.\n4. **Result Aggregation:** Ensure the `run()` method correctly returns an array of `PromiseSettledResult` objects reflecting the outcome of each task.\n5. **Edge Cases:** Test with zero tasks and a concurrency limit of 1.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "ConcurrencyManager",
                "src/lib/ConcurrencyManager.test.ts",
                "jest.useFakeTimers()",
                "taskProcessor",
                "run()",
                "PromiseSettledResult"
              ],
              "technology_hints": [
                "Jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the project's goals. The specified tests for throttling, full parallelism, and fault isolation directly verify the 'Scalability' and 'Reliability (Fault Isolation)' non-functional requirements defined in the Project Constitution. Ensuring the concurrency manager is robust is critical to the Task Runner's function.",
              "sequence_critique": "The step is logically sound. It presumes the `ConcurrencyManager` component has already been created and now requires a comprehensive test suite, which is a standard development sequence. The list of test cases is comprehensive and does not have any ordering dependencies.",
              "clarity_critique": "The instructions are very clear and actionable. However, the proposed file path `src/lib/ConcurrencyManager.test.ts` implies the creation of a `src/lib` directory, which is not defined in the Project Constitution's `project_file_map`. To maintain architectural consistency, the component should be located within an established directory, such as `src/task-runner` or `src/utils`."
            }
          }
        }
      ],
      "Task 2.6: Implement the core Job worker process script, including reading its workspace configuration.": [
        {
          "id": "step_1",
          "description": "Create a new file `src/core/types.ts` to define shared data structures. Based on the `ModuleContract` schema in the architecture document (section A.2), define a TypeScript interface named `ModuleContract`. Ensure all properties like `name`, `purpose`, `dependencies`, `functionSignatures`, and `instructions` are included with appropriate types.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/core/types.ts",
                "ModuleContract",
                "architecture document (section A.2)",
                "name",
                "purpose",
                "dependencies",
                "functionSignatures",
                "instructions"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It defines the `ModuleContract` TypeScript interface, a core data structure that is a fundamental prerequisite for the task of implementing the Job worker process which consumes it. The specified file path, `src/core/types.ts`, is consistent with the Project Constitution's designated location for core type definitions.",
              "sequence_critique": "The sequence is logical. Defining the primary input data structure (`ModuleContract`) is a necessary first step before implementing the logic that will operate on that data. This step correctly establishes this foundation for the rest of the task.",
              "clarity_critique": "The instructions are clear and actionable. The step specifies the exact file to create, the interface to define, and correctly points to the architecture document's appendix (A.2) as the definitive source for the interface's properties and types, ensuring the implementation will match the formal specification."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create the main Job worker script file at `src/core/job.ts`. This file will contain the entry point and logic for a single module generation job.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/core/job.ts"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step correctly initiates the implementation of the Job worker process, directly contributing to the task's goal.",
              "sequence_critique": "The sequence is logical. Creating the file is the necessary first step before any logic can be added.",
              "clarity_critique": "The instruction is clear, but the specified file path `src/core/job.ts` is architecturally inconsistent with the Project Constitution. The constitution establishes a pattern of organizing major, distinct components into their own top-level directories within `src/` (e.g., `src/iba`, `src/task-runner`). The 'Job' is a major component, not a generic core utility. Placing it in `src/core/` conflates a primary application component with foundational logic. A more consistent path, following the established architectural paradigm, would be `src/job/job.ts` or `src/job/index.ts`."
            }
          }
        },
        {
          "id": "step_3",
          "description": "In `src/core/job.ts`, implement the initial setup. Import necessary modules like `fs/promises`, `path`, and `crypto`. Also, import the `ModuleContract` type from `./types.ts`. Define a main `async function runJob()` which will serve as the primary execution function.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/core/job.ts",
                "fs/promises",
                "path",
                "crypto",
                "ModuleContract",
                "./types.ts",
                "runJob"
              ],
              "technology_hints": [
                "fs/promises",
                "path",
                "crypto"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal of implementing the Job worker process. Creating the entry file, defining the main function, and importing foundational modules are necessary first steps.",
              "sequence_critique": "The sequence is logical. It establishes the necessary file and function structure before any implementation logic is added.",
              "clarity_critique": "The instruction is clear, but the proposed file path `src/core/job.ts` creates a minor architectural inconsistency. The project constitution shows a pattern of placing major, distinct components (like `iba` and `task-runner`) in their own dedicated directories (`src/iba`, `src/task-runner`). To maintain this pattern for the 'Job' component, a path like `src/job-worker/index.ts` would be more consistent and maintainable than placing its main script in the generic `src/core` directory."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Implement workspace path handling within `runJob`. The script must accept a single command-line argument: the path to its isolated workspace. Read this path from `process.argv`. If the argument is not provided, throw an error and exit. From the workspace path, derive the `moduleName` (e.g., from `/path/to/.tmp/ironclad_tasks/UserService`, the module name is `UserService`).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runJob",
                "process.argv",
                "moduleName",
                ".tmp/ironclad_tasks/UserService"
              ],
              "technology_hints": [
                "Node.js"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. A Job process is fundamentally defined by its isolated workspace, and acquiring this path as its primary input is the necessary first action. Deriving the module name from this path is also a core requirement that directly supports the component's responsibilities as outlined in the constitution (Section 3.3).",
              "sequence_critique": "The sequence is logical. Validating the presence of the command-line argument must happen before any other processing. There are no missing prerequisite steps.",
              "clarity_critique": "The step is clear but lacks the required specificity for a meticulous implementation. It should be enhanced to: 1. Mandate not just checking for the argument's presence, but also validating that the path exists and is a directory, aligning with the NFR for 'Reliability (Input Path Validation)'. 2. Instead of a generic 'throw an error and exit', it must require exiting with a specific non-zero code from the project's 'Exit-Code Taxonomy' (Appendix A.6), ensuring 'Interoperability (CLI Exit Codes)'."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Create an `async` function `loadWorkspaceConfig(workspacePath: string, moduleName: string)`. This function should read the following files from the workspace and return them as an object: `prompt.txt` (as a string), `prompt.hash` (as a string), and `contracts/<moduleName>.json` (parsed as a `ModuleContract` object). Use `path.join` to construct file paths robustly.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "loadWorkspaceConfig",
                "workspacePath",
                "moduleName",
                "prompt.txt",
                "prompt.hash",
                "contracts/<moduleName>.json",
                "ModuleContract",
                "path.join"
              ],
              "technology_hints": [
                "async",
                "path.join"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. It implements the first logical action of the `Job` worker process as defined in the architecture document (Section 3.3), which is to load its configuration from the dedicated workspace before it can proceed with any generation or validation tasks.",
              "sequence_critique": "The sequence is logical. Loading configuration is a necessary prerequisite for all subsequent steps within the Job worker's lifecycle. This step correctly establishes that foundation.",
              "clarity_critique": "The step is clear and actionable. However, it could be slightly enhanced by explicitly defining the shape of the return object (e.g., `{ prompt: string, promptHash: string, contract: ModuleContract }`) and specifying the expected error handling behavior (i.e., throwing an informative error if a file is not found), which aligns with the 'Reliability (Fail-Fast)' non-functional requirement."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Create a function `verifyPromptIntegrity(promptContent: string, expectedHash: string)`. This function will use the Node.js `crypto` module to calculate the SHA-256 hash of the `promptContent`. It must then compare the calculated hash with the `expectedHash`. If they do not match, it should throw an error. Log a success message if they match. Hint: `crypto.createHash('sha256').update(data).digest('hex')`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "verifyPromptIntegrity",
                "promptContent",
                "expectedHash",
                "crypto"
              ],
              "technology_hints": [
                "Node.js",
                "crypto",
                "SHA-256"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. It directly implements a core responsibility of the 'Job' process as defined in the architecture document (Section 3.3): 'Verify the hash matches the content [of the prompt]'. This supports the critical non-functional requirements of 'Auditability (Prompt Integrity)' and 'Reliability (Determinism)' by ensuring the input to the IMG is exactly what the Task Runner intended.",
              "sequence_critique": "The step is logically sound. Creating this verification utility is a foundational prerequisite for the main Job process logic, which will call this function at the very beginning of its execution, after reading the prompt file and its expected hash from the workspace.",
              "clarity_critique": "The step is very clear and actionable, but could be slightly improved for robustness. The instruction to 'Log a success message' is generic; to better align with the 'Observability (Structured Logging)' NFR, it should specify emitting a structured log (e.g., at the INFO level). Similarly, the instruction to 'throw an error' could be enhanced by requiring the error message to include both the expected and the actual hashes to facilitate faster debugging."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Assemble the `runJob` function. It should orchestrate the calls in order: parse the workspace path and module name, log the start of the job, call `loadWorkspaceConfig`, and then call `verifyPromptIntegrity` with the loaded data. Add console logs to indicate progress at each major step.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runJob",
                "loadWorkspaceConfig",
                "verifyPromptIntegrity"
              ],
              "technology_hints": [
                "console.log"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal. Assembling the `runJob` function to orchestrate the initial loading and verification sequence is a fundamental part of implementing the Job worker process described in the constitution (Section 3.3).",
              "sequence_critique": "The logical sequence is correct. The Job must first determine its context (workspace path, module name), then load the configuration from that path, and finally verify the integrity of the loaded artifacts. The order of operations is sound.",
              "clarity_critique": "The instruction to 'Add console logs' is slightly misaligned with the project's non-functional requirement for structured logging. The constitution (Section 11) specifies structured JSON logs for observability. The step should be more precise, guiding the agent to use a structured logger that includes context like `component` and `module`, rather than using generic `console.log` statements."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Implement robust error handling. Wrap the entire body of `runJob` in a `try...catch` block. In the `catch` block, log the error to `console.error` and use `process.exit(1)` to terminate the process with a failure code. Call `runJob()` at the end of the file to execute it.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runJob",
                "console.error",
                "process.exit"
              ],
              "technology_hints": [
                "Node.js"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is partially aligned. While adding a top-level error handler is crucial for the Job's resilience, the prescribed action to simply exit does not fully align with the system's failure reporting mechanism. The architecture (Sections 3.2 & 3.3) specifies that the Task Runner primarily identifies failed jobs by the presence of a `final_failure.json` file. A Job that crashes due to an unexpected error without creating this file violates its contract with the Task Runner.",
              "sequence_critique": "Steps appear well-aligned and logical.",
              "clarity_critique": "The instructions are clear but incomplete and inconsistent with two key constitutional requirements: 1. **Failure Reporting:** It fails to instruct the agent to create a `final_failure.json` file within the `catch` block. 2. **Logging:** It specifies a generic `console.error`, whereas the 'Observability (Structured Logging)' NFR mandates structured JSON logging. 3. **Exit Codes:** It prescribes a generic `exit(1)`, which deviates from the specific `Exit-Code Taxonomy` defined in the constitution's Appendix A.6."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Create a unit test file `src/core/job.test.ts`. Use Jest and its mocking capabilities (`jest.mock`) to test the `job.ts` script without actual file system access. You will need to mock the `fs/promises`, `path`, and `crypto` modules, as well as `process.argv` and `process.exit`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/core/job.test.ts",
                "job.ts",
                "process.argv",
                "process.exit"
              ],
              "technology_hints": [
                "Jest",
                "jest.mock",
                "fs/promises",
                "path",
                "crypto"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's objective to create unit tests for the Job worker is aligned with the task. However, the proposed file path, `src/core/job.test.ts`, is inconsistent with the project's established architectural pattern. The `project_file_map` shows that other major components like `iba`, `task-runner`, and `commands` reside in their own dedicated directories. The Job is a major component and should logically be placed in a new `src/job/` directory to maintain architectural consistency, rather than in the generic `src/core/` directory.",
              "sequence_critique": "The sequence is logical. Creating the unit test file before the implementation follows a standard Test-Driven Development (TDD) workflow, which is a sound approach for developing a critical and complex component like the Job worker.",
              "clarity_critique": "The instructions on what to test and which modules to mock are clear and actionable. However, the clarity is diminished by the architectural ambiguity of the specified file path. The instruction should explicitly state whether a new directory for the Job component should be created, which would align with the established project structure and remove ambiguity for the AI agent."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Write unit tests in `src/core/job.test.ts` for the following scenarios: 1) A successful run where all files are present and the prompt hash is valid. 2) A failure case where the workspace path argument is missing. 3) A failure case where a required file (e.g., `prompt.txt`) is missing. 4) A failure case where the prompt hash does not match the content of `prompt.txt`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/core/job.test.ts",
                "prompt.txt",
                "prompt hash",
                "workspace path argument"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The test scenarios described are perfectly aligned with the core responsibilities of the `Job` worker process as defined in the architecture (Section 3.3). They correctly focus on validating the initial state of the workspace, including file presence and prompt integrity, which is a critical first step for the `Job`.",
              "sequence_critique": "The sequence is logical. Defining tests for the component's initialization and input validation logic before or alongside its implementation is a sound, test-driven approach. The order of test cases (success path, then various failure paths) is standard and effective.",
              "clarity_critique": "The test scenarios are clear and actionable. However, the proposed file path `src/core/job.test.ts` creates a minor architectural inconsistency. The project file map shows a pattern of creating dedicated directories for major components (`src/iba`, `src/task-runner`). Placing the Job logic in `src/core` deviates from this pattern. A more consistent location would be within the `src/task-runner` directory (e.g., `src/task-runner/job.test.ts`) or in a new dedicated `src/job` directory."
            }
          }
        }
      ],
      "Task 2.7: Create the client for the external Ironclad Module Generator (IMG) API, including secure handling of `IMG_API_KEY`.": [
        {
          "id": "step_1",
          "description": "Create a new directory `src/img` to house all components related to the Ironclad Module Generator (IMG) client. Also, install the necessary dependencies for making HTTP requests and managing local environment variables. Hint: Run `npm install axios dotenv`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Run `npm install axios dotenv`"
              ],
              "key_entities_dependencies": [
                "src/img",
                "Ironclad Module Generator (IMG) client"
              ],
              "technology_hints": [
                "axios",
                "dotenv",
                "npm"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the task. Creating a dedicated `src/img` directory supports the 'Maintainability' NFR by modularizing the external API client. Installing `axios` is essential for HTTP communication, and `dotenv` directly addresses the constitutional requirement for secure local handling of `IMG_API_KEY` via environment variables.",
              "sequence_critique": "The sequence is logical. Creating the directory structure and installing dependencies are the correct prerequisite steps before implementing the client logic.",
              "clarity_critique": "The instructions are clear, specific, and directly actionable, including an explicit command hint that leaves no room for ambiguity."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Inside `src/img`, create a file `img.types.ts`. Based on the architecture document (sections 3.4, 5, A.2, A.3), define the necessary TypeScript interfaces: `ImgPromptRequest` for the request body (`{ prompt: string }`), `ImgSuccessResponse` for the successful response body (`{ implementationCode: string, testCode: string }`), and a configuration interface `ImgClientConfig` (`{ apiUrl: string, apiKey: string }`).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/img",
                "img.types.ts",
                "architecture document (sections 3.4, 5, A.2, A.3)",
                "ImgPromptRequest",
                "ImgSuccessResponse",
                "ImgClientConfig"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task. Defining the TypeScript interfaces for the request, response, and configuration is a logical and necessary prerequisite for building the IMG API client. The specified interfaces directly reflect the API contracts detailed in the architecture document.",
              "sequence_critique": "The sequence is correct. Defining data types and contracts before implementing the logic that uses them is a standard and sound practice.",
              "clarity_critique": "The step is clear and actionable. However, it proposes creating a new directory `src/img/` which is not defined in the `project_file_map` of the constitution. While creating a dedicated directory for the IMG client is a reasonable architectural choice that promotes modularity, it deviates from the established pattern of placing core type definitions in `src/core/types.ts`. This represents a minor, unstated refinement to the project's file structure."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Create a file `src/img/img.errors.ts`. Define custom error classes that extend `Error` to provide more specific failure details. Create `ImgConfigError` for missing configuration and `ImgApiError` for API-level failures (e.g., non-200 status codes). The `ImgApiError` should store the HTTP status code and any response data.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/img/img.errors.ts",
                "Error",
                "ImgConfigError",
                "ImgApiError",
                "IMG_API_KEY"
              ],
              "technology_hints": [
                "TypeScript",
                "HTTP"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Creating specific, custom error classes (`ImgConfigError`, `ImgApiError`) is a fundamental practice for building a robust API client, directly supporting the project's non-functional requirements for 'Reliability (Specific Error Handling)' and 'Maintainability'. The proposed file path `src/img/img.errors.ts`, while introducing a new directory, is a logical architectural choice for encapsulating all IMG client-related code, consistent with the project's modular structure.",
              "sequence_critique": "The sequence is logical. Defining the error contracts for the client module upfront is a sound engineering practice that clarifies the failure modes before implementing the primary logic that will use them. No prerequisite steps are missing.",
              "clarity_critique": "The step is exceptionally clear and actionable. It precisely specifies the file to be created, the base class to extend, the names of the new error classes, their distinct purposes, and the required properties for the `ImgApiError` class (status code and response data). This level of detail is ideal for an AI coding agent."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Create a file `src/img/img.config.ts`. Implement a function `getImgConfig(): ImgClientConfig`. This function must read `IMG_API_URL` and `IMG_API_KEY` from `process.env`. If either variable is missing or empty, it must throw an `ImgConfigError`. This ensures the application fails fast with a clear message if not configured properly. Use `dotenv.config()` at the top of your application's entry point (or for testing) to load a `.env` file for local development.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Use `dotenv.config()` at the top of your application's entry point (or for testing) to load a `.env` file for local development."
              ],
              "key_entities_dependencies": [
                "src/img/img.config.ts",
                "getImgConfig",
                "ImgClientConfig",
                "IMG_API_URL",
                "IMG_API_KEY",
                "process.env",
                "ImgConfigError",
                "dotenv.config()",
                ".env"
              ],
              "technology_hints": [
                "dotenv"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goals\u2014securely loading configuration and failing fast\u2014are well-aligned with the Project Constitution's security and reliability requirements. However, the proposed file path `src/img/img.config.ts` introduces a new top-level component directory (`img`) that is not defined in the constitution's `project_file_map`. A more consistent location would be within the `src/lib` directory (e.g., `src/lib/img-client/img.config.ts`), which is designated for generic, reusable library code like an external API client.",
              "sequence_critique": "The sequence is logical. Establishing the configuration retrieval and validation logic is a correct first step before implementing the API client that will use it.",
              "clarity_critique": "The instructions are clear, specific, and highly actionable. They define the function, its behavior, error conditions, and source of data unambiguously."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Create the main client file `src/img/img.client.ts`. Implement an async function `callImg(prompt: string): Promise<ImgSuccessResponse>`. This function will: 1. Call `getImgConfig` to retrieve the API URL and key. 2. Use `axios` to make a POST request to the `apiUrl`. 3. Set the `Content-Type` to `application/json` and the `Authorization` header to `Bearer <apiKey>`. 4. Send the prompt in the request body, conforming to the `ImgPromptRequest` interface. 5. Handle responses: if the status is 200, validate the response body structure and return it. If the status is not 2xx, throw an `ImgApiError` with the status and response data. Catch any network or other `axios` errors and re-throw them as a generic `ImgApiError`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/img/img.client.ts",
                "callImg",
                "ImgSuccessResponse",
                "getImgConfig",
                "apiUrl",
                "apiKey",
                "ImgPromptRequest",
                "ImgApiError"
              ],
              "technology_hints": [
                "axios"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step aligns with the task's goal, but the proposed file path `src/img/img.client.ts` introduces a new `src/img` directory not defined in the Project Constitution's `project_file_map`. This logic would be better placed in a pre-defined location like `src/lib` for reusable components or `src/core` as it directly supports the `Job` process.",
              "sequence_critique": "The step is missing prerequisite actions. Before implementing the client, the project plan should include steps to: 1. Add the chosen HTTP client library (`axios`) to the project's dependencies, as it is not listed in the constitution. 2. Formally define the necessary data structures (`ImgSuccessResponse`, `ImgPromptRequest`, `ImgApiError`) in a shared types file like `src/core/types.ts`.",
              "clarity_critique": "The step is clear and actionable. For improved debugging alignment with the constitution's 'Auditability' and 'Reliability' NFRs, it could explicitly state that the custom `ImgApiError` should capture and expose the HTTP status code and response body from the failed API call."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Install development dependencies for testing. Hint: Run `npm install -D nock @types/nock`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Run `npm install -D nock @types/nock`"
              ],
              "key_entities_dependencies": [],
              "technology_hints": [
                "npm",
                "nock",
                "@types/nock"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. The task is to create an API client, and testing is a critical part of that. Installing `nock` directly supports the project's testing strategy (Section 12), which requires mocking external dependencies like the IMG API to ensure reliable and isolated tests.",
              "sequence_critique": "The sequence is logical. Installing testing dependencies is a necessary prerequisite step that should happen before or alongside the implementation of the client and its corresponding tests.",
              "clarity_critique": "The step is exceptionally clear and actionable. It states the purpose, identifies the specific packages needed (`nock`, `@types/nock`), and provides the exact, correct `npm` command for an AI agent to execute."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Create a test file `src/img/img.client.test.ts`. Use `nock` to mock the IMG API endpoint. Write a test case for the successful path: mock a 200 OK response with a valid `ImgSuccessResponse` payload. Assert that `callImg` returns the expected object and that the `Authorization` header was sent correctly. Ensure you clean up nock mocks after each test.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/img/img.client.test.ts",
                "IMG API endpoint",
                "ImgSuccessResponse",
                "callImg",
                "Authorization"
              ],
              "technology_hints": [
                "nock"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal of testing the IMG client is well-aligned with the task's objective of creating a reliable and secure client. However, it deviates from the Project Constitution in two ways: 1. It proposes creating a file in a new `src/img/` directory, which is not defined in the `project_file_map`. The logic for the `Job` process, which calls the IMG, is located in `src/core/`. 2. It introduces a new dependency, `nock`, which is not listed in the `global_dependencies_and_interfaces` section of the constitution.",
              "sequence_critique": "The sequence is logical if following a Test-Driven Development (TDD) pattern, where a test is written before the implementation. However, it assumes the `nock` dependency has been installed. A preceding step should have explicitly added `nock` and `@types/nock` to the project's development dependencies in `package.json`.",
              "clarity_critique": "The step is mostly clear but contains ambiguity that conflicts with the constitution. It refers to a data structure named `ImgSuccessResponse`, which is not defined in the `key_data_structures` list. The constitution describes the IMG response payload in Section 5, but does not give it this specific type name, which could lead to confusion. The proposed file path also conflicts with the established `project_file_map`."
            }
          }
        },
        {
          "id": "step_8",
          "description": "In `src/img/img.client.test.ts`, add test cases for configuration errors. Unset the `IMG_API_KEY` and `IMG_API_URL` environment variables (one at a time) and assert that `callImg` throws the `ImgConfigError`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/img/img.client.test.ts",
                "IMG_API_KEY",
                "IMG_API_URL",
                "callImg",
                "ImgConfigError"
              ],
              "technology_hints": [
                "environment variables"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal of testing for missing configuration aligns perfectly with the project's Non-Functional Requirements for security (secure credential handling) and reliability (fail-fast on misconfiguration). However, it introduces a new file path (`src/img/img.client.test.ts`) and implies a new error type (`ImgConfigError`) that are not defined in the Project Constitution's `project_file_map` or `key_data_structures`. While this represents a reasonable refactoring for separation of concerns, it is an undocumented deviation from the established architecture where IMG interaction is described as a responsibility of the `Job` component.",
              "sequence_critique": "The step is logically sound. Testing for configuration failure cases is a standard and necessary part of developing a robust client.",
              "clarity_critique": "The step is clear, specific, and highly actionable. It specifies the file, the exact conditions to test (unsetting environment variables), and the expected outcome (throwing a specific error)."
            }
          }
        },
        {
          "id": "step_9",
          "description": "In `src/img/img.client.test.ts`, add test cases for various API error responses. Use `nock` to simulate: 1. A 401 Unauthorized error. 2. A 429 Too Many Requests error. 3. A 500 Internal Server Error. For each case, assert that `callImg` throws an `ImgApiError` containing the correct HTTP status code.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/img/img.client.test.ts",
                "callImg",
                "ImgApiError"
              ],
              "technology_hints": [
                "nock"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal of testing API error responses is well-aligned with creating a resilient IMG client, a core requirement from the architecture document (Sections 9 and 14). However, the proposed file path `src/img/img.client.test.ts` introduces a new `src/img` directory, which is not defined in the `project_file_map` of the constitution. The IMG client is a direct dependency of the `Job` worker process, whose logic is defined in `src/core/job.ts`. To maintain architectural consistency, this new client logic and its tests should be placed within an established directory, such as `src/core` or the new generic `src/lib`.",
              "sequence_critique": "The sequence is logical, as testing error handling naturally follows the creation of the basic client. However, the plan omits a critical prerequisite: adding the `nock` library to the project's development dependencies. The `nock` library is not listed in the constitution's `global_dependencies_and_interfaces`, and its installation should be an explicit preceding step.",
              "clarity_critique": "The step is exceptionally clear and actionable. It precisely defines the testing tool to use (`nock`), the specific HTTP error codes to simulate (401, 429, 500), and the exact assertion to make (throwing a specific `ImgApiError` with the correct status code)."
            }
          }
        },
        {
          "id": "step_10",
          "description": "In `src/img/img.client.test.ts`, add test cases for malformed or unexpected responses. Simulate a 200 OK response with an invalid JSON payload (e.g., missing the `testCode` property or not being JSON at all). Assert that the client throws an appropriate error.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/img/img.client.test.ts",
                "testCode"
              ],
              "technology_hints": [
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Testing how the API client handles malformed payloads, even on successful HTTP status codes, is critical for achieving the project's core non-functional requirements of Reliability and Fault Isolation. It ensures the Job process can gracefully handle contract violations from the external IMG service without crashing.",
              "sequence_critique": "The step represents a logical progression in a comprehensive testing suite for an API client, typically following 'happy path' tests and basic HTTP error tests. No ordering issues are apparent.",
              "clarity_critique": "The step is clear and actionable. However, it introduces a new file path `src/img/img.client.test.ts` which implies a new `src/img/` directory. This directory is not defined in the Project Constitution's `project_file_map`. Logic for the Job process's interaction with the IMG is described in the architecture under `src/core/job.ts` or `src/task-runner/`. This represents a minor architectural deviation that should be reconciled with the constitution."
            }
          }
        },
        {
          "id": "step_11",
          "description": "Create an `index.ts` file in `src/img`. Export the `callImg` function and all relevant types (`ImgSuccessResponse`, `ImgConfigError`, `ImgApiError`) to create a clean public interface for this module. Add JSDoc comments to all exported members explaining their purpose, parameters, and return values, adhering to the project's documentation standards.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "index.ts",
                "src/img",
                "callImg",
                "ImgSuccessResponse",
                "ImgConfigError",
                "ImgApiError"
              ],
              "technology_hints": [
                "JSDoc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Creating a barrel file (`index.ts`) with documented exports for the IMG client is a standard practice that supports the project's non-functional requirements for Maintainability (clear interfaces) and Reliability (specific error types). The creation of a new `src/img` directory is a logical extension of the established component-based file structure.",
              "sequence_critique": "The sequence is logical. This step correctly acts as the final part of creating the `src/img` module, exposing a clean public interface after the core logic and types have presumably been defined in preceding steps of the task.",
              "clarity_critique": "The step is clear and actionable. It specifies the exact file to create, the members to export, and the documentation requirements. To be pedantically precise, it could explicitly state that the `ImgSuccessResponse` type should correspond to the `{ implementationCode, testCode }` payload defined in the architecture document (Section 3.4), but this is a minor clarification."
            }
          }
        }
      ],
      "Task 2.8: Implement the Job's 3-attempt retry loop with deterministic prompt amendment for failures.": [
        {
          "id": "step_1",
          "description": "First, let's establish the core structure for the retry loop. In the main execution function of the Job process, define a constant `MAX_ATTEMPTS = 3`. Then, create a `for` loop that iterates from `attempt = 1` to `MAX_ATTEMPTS`. All subsequent logic for IMG calls and validation will be placed inside this loop.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "main execution function of the Job process",
                "Job process",
                "MAX_ATTEMPTS",
                "attempt",
                "IMG calls",
                "validation"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. It directly establishes the 3-attempt retry loop, which is a core responsibility of the 'Job' process as defined in the Project Constitution (Section 3.3 and NFR Section 14).",
              "sequence_critique": "The sequence is logical. Creating the fundamental loop structure is the correct first step before adding the logic that will execute within it (IMG calls, validation, etc.).",
              "clarity_critique": "The instructions are clear, specific, and highly actionable. Defining a constant `MAX_ATTEMPTS = 3` and a `for` loop with a named iterator `attempt` provides unambiguous guidance for implementation."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Define the TypeScript interfaces for structured failure reporting within the Job's scope. Based on the architecture spec (Section 4), create interfaces for `ValidationFailure` (with properties like `validator`, `message`, `details`) and `FailureReport` (which will be an array of `ValidationFailure` objects). Also, define an interface for `FinalFailureReport` (with `module`, `attempts`, `lastPromptHash`, `errorSummary`). Use a library like `zod` to create schemas for these interfaces to ensure type safety when reading/writing JSON files.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "Job",
                "architecture spec (Section 4)",
                "ValidationFailure",
                "FailureReport",
                "FinalFailureReport"
              ],
              "technology_hints": [
                "TypeScript",
                "zod",
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Defining the data structures for failure reporting is a necessary prerequisite for implementing the retry loop logic that consumes and produces these structures. The proposed interfaces map directly to the `FailureEntry` and `FinalFailureReport` data structures outlined in the Project Constitution.",
              "sequence_critique": "The logical sequence is correct. Defining types and schemas is the foundational first step before implementing the logic that operates on that data within the retry loop.",
              "clarity_critique": "The step is clear but could be more precise to prevent architectural drift. It should explicitly use the type name `FailureEntry` as defined in the Project Constitution, instead of introducing a new name `ValidationFailure`. It should also mandate that the `validator` property in `FailureEntry` uses the existing `ValidatorType` string literal union for consistency."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Implement a helper function `recordFailure(workspacePath: string, failure: ValidationFailure): Promise<void>`. This function will read the `failures.json` file in the workspace (or create an empty array if it doesn't exist), append the new `failure` object, and write the updated array back to `failures.json`. Use `fs/promises` for asynchronous file I/O and ensure the JSON is pretty-printed for readability.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "recordFailure",
                "workspacePath",
                "ValidationFailure",
                "failures.json"
              ],
              "technology_hints": [
                "fs/promises"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal. Recording individual failures in `failures.json` is a direct and necessary prerequisite for both amending the prompt during the retry loop and generating the `final_failure.json` report, as specified in the Job component's responsibilities (Constitution \u00a73.3).",
              "sequence_critique": "The sequence is logical. Implementing this atomic helper function is a foundational building block that should be created before the main retry loop logic that will call it.",
              "clarity_critique": "The step is mostly clear but has one significant ambiguity. It uses the type `ValidationFailure` without defining it. To ensure consistency with the established data model, it must explicitly state that this type should be an implementation of the `FailureEntry` data structure already defined in the Project Constitution. This ensures the data written to `failures.json` is valid from the start."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Create the deterministic prompt amendment function: `amendPromptForRetry(workspacePath: string, lastFailure: ValidationFailure): Promise<void>`. This function must perform the following actions: 1. Read the current `prompt.txt`. 2. Format the `lastFailure` object into a JSON string. 3. Append a section to the prompt content, exactly as specified in the architecture spec's template: `\\n===LAST_ERROR===\\n<formatted_failure_json>`. 4. Overwrite `prompt.txt` with the new, amended content. 5. Recalculate the SHA-256 hash of the new prompt content and overwrite the `prompt.hash` file.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "amendPromptForRetry",
                "workspacePath",
                "lastFailure",
                "ValidationFailure",
                "prompt.txt",
                "prompt.hash",
                "===LAST_ERROR==="
              ],
              "technology_hints": [
                "TypeScript",
                "JSON",
                "SHA-256"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are perfectly aligned with the task of implementing a deterministic prompt amendment function, which is a core responsibility of the 'Job' process as defined in the Project Constitution (Section 3.3). This directly supports the project's non-functional requirements for Reliability and Auditability.",
              "sequence_critique": "The sequence of operations is logical and correct. It correctly reads existing content, modifies it in memory, writes the full new content back, and only then recalculates the integrity hash based on the final written content. No steps are missing or out of order.",
              "clarity_critique": "The steps are clear and actionable. For perfect clarity and to ensure deterministic output, step 2 ('Format the `lastFailure` object into a JSON string') should explicitly state that the JSON must be 'pretty-printed' (e.g., with 2-space indentation), consistent with the `toJsonPretty` helper shown in the architecture's prompt template example (Appendix A.3)."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Implement the function `writeFinalFailureReport(workspacePath: string, moduleName: string, totalAttempts: number): Promise<void>`. This function will be called when all retries are exhausted. It should: 1. Read the full `failures.json` file. 2. Read the `prompt.hash` file. 3. Create a summary of the last error. 4. Construct a `FinalFailureReport` object. 5. Write this object to `final_failure.json` in the workspace.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "writeFinalFailureReport",
                "workspacePath",
                "moduleName",
                "totalAttempts",
                "Promise<void>",
                "failures.json",
                "prompt.hash",
                "FinalFailureReport",
                "final_failure.json"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal of implementing the retry loop's terminal failure state. It directly implements the `FinalFailureReport` data structure and the auditability requirements defined in the Project Constitution.",
              "sequence_critique": "The sequence is logical and correct. All necessary data (`failures.json`, `prompt.hash`) is read before the final report object is constructed and written to disk.",
              "clarity_critique": "The instruction in step 3, \"Create a summary of the last error,\" is slightly ambiguous. To ensure deterministic and consistent output, it should specify how to derive the summary. For example, it could instruct the agent to use the `validator` and `message` fields from the most recent entry in the `failures.json` array."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Integrate the new functions into the main retry loop. Inside the loop, after a validation failure: 1. Call `recordFailure` with the details of the validation error. 2. Check if `attempt < MAX_ATTEMPTS`. If so, call `amendPromptForRetry` and log that a retry is being initiated. 3. If the validation succeeds, set a `success` flag to `true` and `break` the loop.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "the new functions",
                "the main retry loop",
                "recordFailure",
                "validation error",
                "attempt",
                "MAX_ATTEMPTS",
                "amendPromptForRetry",
                "success"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The steps align with implementing the retry mechanism but are incomplete. They critically omit the requirement to handle the final, permanent failure after all retries are exhausted. The Project Constitution (Section 3.3) explicitly requires the Job to write a `final_failure.json` file in this scenario, and this step fails to account for that terminal state.",
              "sequence_critique": "The sequence for a successful retry attempt is logical (record failure, amend prompt). However, the overall loop control flow is flawed because it lacks the branch to handle the case where `attempt < MAX_ATTEMPTS` is false. The logic should be an explicit `if/else` structure to differentiate between preparing for a retry and finalizing a permanent failure.",
              "clarity_critique": "The instructions are clear for the actions they describe but are critically unclear by omission. They fail to instruct the agent on the required action when the retry limit is reached, which is a primary responsibility of this loop. The step should be amended to include an `else` block for the `attempt < MAX_ATTEMPTS` check, which would handle the creation of the `final_failure.json` report before breaking the loop."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Add the concluding logic immediately after the retry loop. Check the `success` flag. If `true`, proceed with the success path (writing final code, exiting with code 0). If `false`, it means all attempts failed. In this case, call `writeFinalFailureReport` and then terminate the process with a non-zero exit code to signify failure to the Task Runner.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "retry loop",
                "success",
                "writeFinalFailureReport",
                "Task Runner"
              ],
              "technology_hints": [
                "process exit codes"
              ]
            },
            "step_critique": {
              "alignment_critique": "Steps are well-aligned. They directly implement the Job's terminal states (success or permanent failure) as defined in the architecture (Section 3.3), ensuring the Job correctly signals its outcome to the orchestrating Task Runner via an exit code and the presence or absence of a `final_failure.json` file.",
              "sequence_critique": "The sequence is logical. Placing this concluding logic immediately after the retry loop is the correct approach to handle the two possible outcomes of the loop: a successful break or the exhaustion of all attempts.",
              "clarity_critique": "The instructions are clear and actionable. The use of a `success` flag to drive a conditional block for the two terminal paths (success vs. failure) is unambiguous and provides a straightforward implementation path for an AI coding agent."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Write comprehensive unit tests for the retry logic. Use a mocking library (like Jest's built-in mocking) and a file system mocking library (like `mock-fs`) to test: 1. The `amendPromptForRetry` function to ensure it modifies `prompt.txt` and `prompt.hash` deterministically and correctly. 2. The `recordFailure` function to verify it correctly appends to the `failures.json` array. 3. The overall loop control flow: ensure it breaks on the first success, and that it runs exactly 3 times before creating a `final_failure.json` on persistent failure.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "amendPromptForRetry",
                "prompt.txt",
                "prompt.hash",
                "recordFailure",
                "failures.json",
                "final_failure.json"
              ],
              "technology_hints": [
                "Jest",
                "mock-fs"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Writing comprehensive unit tests for the retry loop, prompt amendment, and failure reporting directly supports the task's goal and reinforces core constitutional requirements for Reliability, Determinism, and Auditability.",
              "sequence_critique": "The sequence is logical. It follows a standard bottom-up testing approach by specifying tests for individual helper functions (`amendPromptForRetry`, `recordFailure`) before testing the integrated loop control flow.",
              "clarity_critique": "The step is very clear and actionable. However, it could be slightly improved by explicitly stating that the test for the persistent failure case should not only verify the *creation* of `final_failure.json`, but also validate its *content* against the `FinalFailureReport` data structure defined in the constitution (e.g., checking `attempts` is 3, and the `lastPromptHash` is correct)."
            }
          }
        }
      ],
      "Task 2.9: Develop the TypeScript compiler (`tsc`) validation engine to check generated code against interface stubs.": [
        {
          "id": "step_1",
          "description": "Create the directory structure and type definitions for the TSC validation engine. Create a new directory `src/validation/engines`. Inside, create a file `tscValidator.ts`. Also create `src/validation/types.ts` to hold shared validation types. In `src/validation/types.ts`, define two interfaces: `ValidationResult { success: boolean; errors?: ValidationError[]; }` and `ValidationError { file?: string; line?: number; column?: number; message: string; }`. In `tscValidator.ts`, import these types.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/validation/engines",
                "tscValidator.ts",
                "src/validation/types.ts",
                "ValidationResult",
                "ValidationError"
              ],
              "technology_hints": [
                "tsc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are well-aligned with building a validation engine. However, creating a new `src/validation/types.ts` file introduces a potential architectural inconsistency. The proposed `ValidationResult` and `ValidationError` interfaces represent a fundamental data contract for all validation engines and would be more consistent with the established Project Constitution if defined in the central `src/core/types.ts` to avoid fragmenting core type definitions across the project. The creation of `src/validation/engines` for the engine logic itself is a reasonable architectural refinement.",
              "sequence_critique": "The sequence is logical. Creating the directory structure, then the type definition file, and finally the engine file that imports from it is the correct and necessary order of operations.",
              "clarity_critique": "The instructions are clear, specific, and fully actionable. The file paths and interface definitions are explicitly stated, leaving no room for ambiguity."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `tscValidator.ts`, implement a private helper function `createValidationTsConfig(workspacePath: string): Promise<string>`. This function will create a `tsconfig.validation.json` file inside the specified workspace. The tsconfig should be configured for validation purposes with `compilerOptions` including `\"noEmit\": true`, `\"strict\": true`, `\"moduleResolution\": \"node\"`, `\"target\": \"ES2022\"`, `\"module\": \"commonjs\"`, and `\"baseUrl\": \".\"`. The `include` property should be `[\"**/*.ts\"]`. The function should return the path to the created tsconfig file. Use Node's `fs/promises` and `path` modules.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "tscValidator.ts",
                "createValidationTsConfig",
                "workspacePath",
                "tsconfig.validation.json"
              ],
              "technology_hints": [
                "fs/promises",
                "path",
                "TypeScript",
                "tsconfig.json"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Creating a dedicated, dynamically generated `tsconfig.json` within the isolated job workspace is the correct architectural approach for running a scoped `tsc` validation. This directly supports the core responsibility of the 'Job' worker as defined in the Project Constitution (Section 3.3).",
              "sequence_critique": "The step is logically sound. Creating the tsconfig file is a necessary prerequisite before the TypeScript compiler can be executed. Implementing this helper function is a foundational step for the `tsc` validator.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify the exact function signature, the file to be created (`tsconfig.validation.json`), the precise compiler options, and the `include` paths. This level of detail is ideal for an AI agent."
            }
          }
        },
        {
          "id": "step_3",
          "description": "In `tscValidator.ts`, implement a function `parseTscOutput(output: string): ValidationError[]`. This function will take the raw string output from a failed `tsc` command and parse it into an array of `ValidationError` objects. Use a regular expression to capture the file path, line number, column number, and error message from each line of the `tsc` output. Example line to parse: `src/modules/MyModule.ts(10,5): error TS2322: Type 'string' is not assignable to type 'number'.`",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "tscValidator.ts",
                "parseTscOutput",
                "ValidationError"
              ],
              "technology_hints": [
                "tsc",
                "regular expression"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is directly aligned with the task of creating a `tsc` validation engine. Parsing the compiler's output into a structured format is a fundamental requirement for reporting errors in a machine-readable way, which is consistent with the project's goal of producing structured `FailureReport`s as defined in the constitution.",
              "sequence_critique": "The step is logically sound as a self-contained function implementation. It correctly assumes that the raw `tsc` output string is available as an input. This function would naturally be called after a preceding step that executes the `tsc` command and captures its output. No sequence issues are identified.",
              "clarity_critique": "The instruction is clear and actionable, but it directs the implementation to use a regular expression on human-readable `tsc` output. While feasible, this approach is inherently brittle and could break if the compiler's output format changes. A more robust solution, aligned with the 'Maintainability' NFR, would be to use the TypeScript Compiler API programmatically, which avoids parsing potentially unstable string formats. The current approach, while functional, introduces minor technical debt."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Implement the main validation function `runTscValidation(workspacePath: string): Promise<ValidationResult>` in `tscValidator.ts`. This function will orchestrate the validation process: 1. Call `createValidationTsConfig` to generate the tsconfig. 2. Use Node's `child_process.spawn` to execute the TypeScript compiler: `npx tsc --project <path_to_tsconfig.validation.json>`. Ensure the command is run with the `cwd` set to `workspacePath`. 3. Capture all `stdout` and `stderr` data. 4. When the process exits, if the exit code is 0, return `{ success: true }`. 5. If the exit code is non-zero, use `parseTscOutput` on the captured output to generate the errors and return `{ success: false, errors: [...] }`. 6. Use a `try...finally` block to ensure the temporary `tsconfig.validation.json` is deleted after the process completes, regardless of outcome.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runTscValidation",
                "ValidationResult",
                "tscValidator.ts",
                "createValidationTsConfig",
                "workspacePath",
                "tsconfig.validation.json",
                "parseTscOutput"
              ],
              "technology_hints": [
                "Node.js",
                "child_process.spawn",
                "TypeScript compiler (tsc)",
                "npx",
                "Promise",
                "try...finally"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are well-aligned with the task's goal and the project constitution. Implementing a `tsc` validation engine that runs in an isolated workspace is a core responsibility of the 'Job' worker process as defined in the architecture (Section 3.3).",
              "sequence_critique": "The sequence is logical. It correctly orders the creation of a temporary config, execution of the compiler, capturing results, and ensuring cleanup with a `try...finally` block.",
              "clarity_critique": "The instructions are mostly clear but have two minor gaps: 1. The command `npx tsc --project ...` omits the `--noEmit` flag, which is explicitly specified in the architecture document (Section 3.3) to prevent the validation step from producing unwanted output files. 2. The error handling logic focuses on the `tsc` process's non-zero exit code but does not specify how to handle exceptions thrown by `child_process.spawn` itself (e.g., if the `tsc` command cannot be found), which would be a different failure mode."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Set up the testing environment for the `tscValidator`. Create a new test file `src/validation/engines/tscValidator.test.ts`. Use Jest for testing. You will need to mock the `child_process` and `fs/promises` modules to simulate `tsc` execution and file system operations without actually running them. Prepare mock data: a valid `tsc` output (empty string), and a multi-line invalid `tsc` error output string.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "tscValidator",
                "src/validation/engines/tscValidator.test.ts",
                "child_process",
                "fs/promises"
              ],
              "technology_hints": [
                "Jest",
                "tsc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Creating a test environment is a foundational and necessary action for developing the `tsc` validation engine, directly contributing to the task's goal and adhering to the project's testing strategy.",
              "sequence_critique": "The sequence is logical. It correctly prioritizes creating the test file, identifying the external dependencies (`child_process`, `fs/promises`) that need to be mocked, and preparing the mock data required for subsequent test cases.",
              "clarity_critique": "The proposed file path `src/validation/engines/tscValidator.test.ts` introduces a new directory structure (`src/validation/engines/`) that is not defined in the Project Constitution's `project_file_map`. While this might be a logical location, it represents an architectural deviation. For consistency, the plan should either justify this new directory or place the validator in a location consistent with existing patterns, such as a new `validators` subdirectory within `src/core/`."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Write unit tests in `tscValidator.test.ts` for the `runTscValidation` function. Create at least two test cases: 1. A 'success' case where the mocked `spawn` process returns exit code 0. Assert that the function returns `{ success: true }`. 2. A 'failure' case where the mocked `spawn` process returns exit code 1 and writes the mock error string to `stdout`. Assert that the function returns `{ success: false, ... }` and that the `errors` array is correctly parsed and matches the expected structure.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "tscValidator.test.ts",
                "runTscValidation",
                "spawn",
                "stdout",
                "errors"
              ],
              "technology_hints": [
                "tsc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. The Project Constitution specifies that the TypeScript Compiler (`tsc`) is a primary validation engine (Section 2, 3.5), and the overall testing strategy (Section 12) mandates unit and component testing for such critical parts of the system. Testing the success and failure paths of the `tsc` wrapper function directly contributes to the task's goal and the project's reliability requirements.",
              "sequence_critique": "The sequence is logical. This step presumes the existence of the `runTscValidation` function signature and its basic implementation which invokes a child process. Writing unit tests is the immediate and correct next step to verify and harden that implementation, consistent with Test-Driven Development (TDD) or standard development practices.",
              "clarity_critique": "The step is clear and actionable. A minor improvement would be to specify a realistic, multi-line example of a mock `tsc` error output string. This would ensure the test not only checks for failure but also validates that the parsing logic correctly handles the specific format and structure of `tsc`'s error messages, which is a critical part of the function's responsibility."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Finalize the `tscValidator` module. Add comprehensive JSDoc comments to all exported functions and types, explaining their purpose, parameters, and return values. Review the code for clarity, robustness, and adherence to project conventions. Ensure all dependencies like `child_process`, `fs/promises`, and `path` are correctly imported.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "tscValidator"
              ],
              "technology_hints": [
                "JSDoc",
                "child_process",
                "fs/promises",
                "path"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of creating a validation engine. Focusing on documentation, code quality, and final checks directly supports the project's core NFRs for Maintainability and Reliability, as defined in the constitution.",
              "sequence_critique": "The step is logically sequenced as a finalization activity, correctly assuming that the core implementation of the validator has already been completed in prior steps of this task.",
              "clarity_critique": "The instructions are generally clear. However, the 'review for robustness' instruction could be more specific. It fails to explicitly require that the validator's failure output must conform to the `FailureEntry` data structure defined in the Project Constitution, specifically using the `ValidatorType` of 'tsc'. This is a critical detail for ensuring the component integrates correctly into the Job's validation and reporting sequence."
            }
          }
        }
      ],
      "Task 2.10: Develop the test runner validation engine to execute generated tests (e.g., using Jest/Mocha).": [
        {
          "id": "step_1",
          "description": "Set up the testing environment. Install Jest and its TypeScript dependencies by running `npm install --save-dev jest ts-jest @types/jest`. These will be used to programmatically execute the generated tests.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install Jest and its TypeScript dependencies by running `npm install --save-dev jest ts-jest @types/jest`"
              ],
              "key_entities_dependencies": [
                "generated tests"
              ],
              "technology_hints": [
                "npm",
                "jest",
                "ts-jest",
                "@types/jest",
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. Installing `jest`, `ts-jest`, and `@types/jest` is a direct prerequisite for building the test runner validation engine, and these specific tools are explicitly listed in the Project Constitution's technology stack and global dependencies.",
              "sequence_critique": "The sequence is logical. Installing the necessary dependencies is the correct first step before writing any code that will use them to execute tests.",
              "clarity_critique": "The instruction is clear and actionable. It provides the exact `npm` command to execute and correctly identifies the packages as development dependencies."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Define the shared data structure for validation results. Create a new file at `src/validation/types.ts` and define an exported type `ValidationResult` as follows: `export interface ValidationResult { validator: string; message: string; details?: any; }`. This will ensure consistent error reporting across all validation engines.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/validation/types.ts",
                "ValidationResult",
                "validator",
                "message",
                "details",
                "validation engines"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is critically misaligned with the Project Constitution. The constitution already defines `ValidationResult` and `ValidationError` in Task 2.9, which are more robust and specific (`{ success: boolean, errors?: ValidationError[] }`). This step attempts to overwrite this with a conflicting, less specific, and ambiguous data structure. The goal should be to *use* the existing types, not introduce a conflicting one.",
              "sequence_critique": "The step is logically out of sequence. Task 2.9 is documented as having already established the validation data structures and the `src/validation/types.ts` file. This step incorrectly instructs the creation of a file that should already exist as a prerequisite, indicating a failure to acknowledge the output of previous tasks.",
              "clarity_critique": "While the instruction is literally clear, it is semantically flawed and promotes poor design. The proposed `ValidationResult` structure is ambiguous, and the use of `details?: any` introduces a type-safety anti-pattern that directly contradicts the project's core mission of ensuring 'strict adherence to contracts' and its choice of TypeScript. The instruction should be to import and use the existing, superior types."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Create the test runner engine module. Create a new file at `src/validation/testRunnerEngine.ts`. Import the `ValidationResult` type. Define and export an asynchronous function signature: `export async function runTests(testFilePath: string, workspaceDir: string): Promise<ValidationResult[]>`. This function will be the main entry point for this validation step.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/validation/testRunnerEngine.ts",
                "ValidationResult",
                "runTests",
                "testFilePath",
                "workspaceDir"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of creating a test runner engine. However, it introduces two minor deviations from the established architecture. First, the proposed file path `src/validation/testRunnerEngine.ts` should be `src/validation/engines/testRunnerEngine.ts` to maintain consistency with the directory structure defined for other validation engines (e.g., `tscValidator.ts`). Second, the return type `Promise<ValidationResult[]>` is inconsistent with the architectural pattern where a single validation run on a file produces a single `ValidationResult` object, not an array of them.",
              "sequence_critique": "The sequence is logical. Creating the file and defining the primary function signature is the correct first step for this module.",
              "clarity_critique": "The instructions are clear but should be corrected to align with the project constitution. The file path should be `src/validation/engines/testRunnerEngine.ts`, and the function's return type should be `Promise<ValidationResult>` (singular) to reflect a single, consolidated outcome from the test runner execution."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Implement the core test execution logic using Jest's programmatic API. In `testRunnerEngine.ts`, import `runCLI` from the `jest` package. Inside the `runTests` function, construct a Jest configuration object. This configuration must specify `rootDir: workspaceDir`, `testMatch: [testFilePath]`, `transform: { '^.+\\.ts$': 'ts-jest' }`, `silent: true`, and `passWithNoTests: true`. Then, call `await runCLI(config, [workspaceDir])` to execute the tests.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "testRunnerEngine.ts",
                "runCLI",
                "runTests",
                "workspaceDir",
                "testFilePath",
                "config"
              ],
              "technology_hints": [
                "Jest",
                "Jest's programmatic API",
                "ts-jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is mostly aligned, but the configuration `passWithNoTests: true` is a critical flaw. The project's core mission is to generate 'test-covered' software. Allowing a validation step to succeed when the IMG fails to generate any actual tests directly contradicts this goal. This configuration should be `false` to ensure that an empty test suite is correctly flagged as a failure.",
              "sequence_critique": "The step is logically incomplete. It describes how to *execute* the tests but omits the crucial subsequent step: capturing and interpreting the results from Jest. The `runCLI` function returns a results object. The implementation must process this object (e.g., check `results.success`, `results.numFailedTests`) to determine the outcome and format it into the system's `ValidationResult` interface. This is a missing prerequisite for the engine to be useful.",
              "clarity_critique": "The instruction is clear about invoking Jest but lacks clarity on handling the output. It should explicitly state that the return value of `await runCLI(...)` must be captured. Furthermore, it should instruct the agent to parse the detailed test results from this return object to populate the `ValidationError` array with specific failure messages from Jest in the event of a test failure."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Process the results from the Jest execution. The `runCLI` function returns a results object. Check the `results.success` property. If it's `true`, return an empty array `[]` signifying success. If `false`, iterate through `results.testResults` and their `assertionResults` to find items with `status === 'failed'`. For each failed assertion, extract its `failureMessages` array, format it into a clear error string, and create a `ValidationResult` object with `validator: 'test-runner'` and the formatted message. Collect all such results into an array and return it.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runCLI",
                "results",
                "results.success",
                "results.testResults",
                "assertionResults",
                "status",
                "failureMessages",
                "ValidationResult",
                "validator: 'test-runner'"
              ],
              "technology_hints": [
                "Jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of creating a test runner validation engine and the overall project goal of producing validated, test-covered code.",
              "sequence_critique": "The logical sequence is correct: check for overall success first, and only parse detailed errors on failure. This is an efficient approach.",
              "clarity_critique": "The step is critically inconsistent with the project's established data structures. It instructs the creation of an array of `ValidationResult` objects, but the constitution defines `ValidationResult` as a single object `{ success: boolean, errors?: ValidationError[] }` returned by an engine. The step should be revised to return a single `ValidationResult` object, populating its `errors` array with `ValidationError` objects derived from Jest's `assertionResults` on failure. The `validator` type ('test-runner') belongs in the higher-level `FailureEntry` structure, not here."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Implement robust error handling for the test runner itself. Wrap the entire `runCLI` call and result processing logic in a `try...catch` block. If any exception occurs during Jest's execution (e.g., a configuration error), catch the error and return an array containing a single `ValidationResult` object. This object should have `validator: 'test-runner-execution-error'` and a `message` that includes the exception's details.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runCLI",
                "ValidationResult",
                "validator",
                "message",
                "'test-runner-execution-error'"
              ],
              "technology_hints": [
                "Jest",
                "try...catch"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal of adding robust error handling is perfectly aligned with the project's non-functional requirements for reliability. However, the proposed implementation detail of creating a new validator type, 'test-runner-execution-error', directly conflicts with the established `ValidatorType` enum in the Project Constitution. This represents architectural drift. The failure should be attributed to the existing 'test-runner' validator, with the error message clarifying that it was an execution error, not a test failure.",
              "sequence_critique": "The step is logically sound. Implementing error handling is a fundamental part of developing a robust function that interacts with external tools like a test runner.",
              "clarity_critique": "The instruction is clear but its specificity leads to a constitution violation. It should be rephrased to instruct the creation of a single `ValidationResult` object with `success: false` and an array of `ValidationError` objects containing the exception details. This aligns with the `ValidationResult` data structure and avoids creating an invalid `ValidatorType`."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Create comprehensive unit tests for the `testRunnerEngine`. Create a test file at `src/validation/__tests__/testRunnerEngine.test.ts`. Use `jest.mock('jest', ...)` to mock the `runCLI` function. Write tests for three scenarios: 1) `runCLI` returns a success object, assert the function returns an empty array. 2) `runCLI` returns a failure object with sample error messages, assert the function returns a correctly formatted `ValidationResult[]`. 3) `runCLI` throws an error, assert the function returns a single `ValidationResult` capturing the exception.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "testRunnerEngine",
                "src/validation/__tests__/testRunnerEngine.test.ts",
                "jest.mock",
                "runCLI",
                "ValidationResult[]",
                "ValidationResult"
              ],
              "technology_hints": [
                "jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of testing the test runner engine. However, its specification of the function's return value is inconsistent with the project constitution. The constitution defines that validation engines return a single `ValidationResult` object (`{ success: boolean, errors?: ValidationError[] }`), but the step incorrectly asks to assert for an `empty array` on success or a `ValidationResult[]` (an array of results) on failure. This must be corrected to align with the established `ValidationResult` data structure.",
              "sequence_critique": "The sequence is logical. It correctly follows the standard unit testing pattern: set up mocks, test the success path, test the failure path, and test the exception handling path.",
              "clarity_critique": "The step is mostly clear, but the incorrect description of the expected return type is a significant clarity issue. It should be rephrased to specify asserting for a single `ValidationResult` object in all scenarios, for example: 1) On success, assert the result is `{ success: true }`. 2) On failure, assert the result is `{ success: false, errors: [...] }`. 3) On an exception, assert the result is `{ success: false, errors: [ ... ] }` containing a single error that captures the exception message."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Finalize the module with documentation and code quality checks. Add JSDoc comments to the `runTests` function in `testRunnerEngine.ts`, explaining its purpose, parameters, return value, and the fact that it uses Jest programmatically. Run the project's linter (e.g., ESLint, Prettier) to ensure the new code conforms to the established style guide.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Run the project's linter (e.g., ESLint, Prettier)"
              ],
              "key_entities_dependencies": [
                "runTests",
                "testRunnerEngine.ts"
              ],
              "technology_hints": [
                "JSDoc",
                "Jest",
                "ESLint",
                "Prettier"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. It directly supports the Non-Functional Requirements for 'Maintainability' specified in the constitution, which mandate comprehensive documentation (code comments) and adherence to coding standards (ESLint, Prettier) for the tool's own codebase.",
              "sequence_critique": "The sequence is logical. Placing documentation and code style checks as a finalization step for the `testRunnerEngine.ts` module is appropriate, as it should occur after the core functionality has been implemented and tested.",
              "clarity_critique": "The instructions are clear and specific. The request for JSDoc details the target function and the exact points to cover. The instruction to run the project's linter is an unambiguous, actionable command for an AI agent."
            }
          }
        }
      ],
      "Task 2.11: Implement the basic 'Edge Case String Check' validator for generated test files.": [
        {
          "id": "step_1",
          "description": "Create a new file at `src/validation/edgeCaseValidator.ts`. This file will contain the logic for the edge case string check validator. Also, create a shared types file `src/validation/types.ts` if it doesn't exist, to define common validation result structures.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/validation/edgeCaseValidator.ts",
                "edge case string check validator",
                "src/validation/types.ts",
                "common validation result structures"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of creating the edge case validator. However, the proposed file path `src/validation/edgeCaseValidator.ts` deviates from the established constitutional pattern. According to the project file map (derived from Task 2.9), specific validation engine implementations should reside in `src/validation/engines/`. Therefore, the file should be created at `src/validation/engines/edgeCaseValidator.ts` to maintain architectural consistency.",
              "sequence_critique": "The sequence is logical. Creating the necessary source files, including the shared types file, is a correct prerequisite before proceeding to implement the validator's logic in subsequent steps.",
              "clarity_critique": "The instructions are clear and actionable. To enhance clarity and prevent architectural drift, the instruction should explicitly reference the established `src/validation/engines/` directory for housing the new validator file."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `src/validation/types.ts`, define and export a `ValidationResult` interface. According to the architecture spec (section 4), it should have at least these properties: `validator: string`, `message: string`, and `details: object`. Also, define and export a simplified `ModuleContract` interface that includes `instructions?: { edgeCases?: string[] }`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/validation/types.ts",
                "ValidationResult",
                "architecture spec (section 4)",
                "validator",
                "message",
                "details",
                "ModuleContract",
                "instructions",
                "edgeCases"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is misaligned with the Project Constitution. It directs the implementation of a `ValidationResult` interface (`{ validator: string; message: string; details: object }`) that the constitution explicitly flags as a 'Conflicting Definition'. The established, canonical interface from Task 2.9 is `{ success: boolean; errors?: ValidationError[] }`. Adhering to the conflicting definition introduces architectural inconsistency and ignores a previously resolved design decision.",
              "sequence_critique": "The sequence is logical. Defining types before implementing the validator logic that consumes them is the correct order.",
              "clarity_critique": "The instruction is clear but directs the implementation of an incorrect data structure. The justification provided ('according to the architecture spec (section 4)') is misleading; section 4 describes the attributes for a `FailureReport`/`FailureEntry`, not the return type for a validation engine, which is what `ValidationResult` represents. The step should reference the established `ValidationResult` from Task 2.9."
            }
          }
        },
        {
          "id": "step_3",
          "description": "In `src/validation/edgeCaseValidator.ts`, import the types from `../validation/types.ts`. Then, define the function signature for the validator: `export function validateEdgeCases(testCode: string, contract: ModuleContract): ValidationResult[]`. This function will take the generated test code and the module's contract as input.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/validation/edgeCaseValidator.ts",
                "../validation/types.ts",
                "validateEdgeCases",
                "testCode",
                "contract",
                "ModuleContract",
                "ValidationResult"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step aligns with the task's goal. However, the proposed file path `src/validation/edgeCaseValidator.ts` is inconsistent with the established project file map, which specifies that individual validation engines should reside in `src/validation/engines/`. The correct path should be `src/validation/engines/edgeCaseValidator.ts`.",
              "sequence_critique": "The sequence of creating the file, importing types, and then defining the function signature is logical and requires no changes.",
              "clarity_critique": "The specified return type `ValidationResult[]` is inconsistent with the primary constitutional definition of `ValidationResult`, which represents the outcome of a *single* validation engine's run. A single run should produce one `ValidationResult` object, which in turn can contain an array of `ValidationError` objects for each failed check. The proposed signature should be `...: ValidationResult` to maintain consistency with the data model."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Implement the core logic for `validateEdgeCases`. First, safely access the `edgeCases` array from the contract using optional chaining (`contract.instructions?.edgeCases`). If the array is null, undefined, or empty, the function should immediately return an empty array `[]`, indicating success.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "validateEdgeCases",
                "edgeCases",
                "contract",
                "instructions"
              ],
              "technology_hints": [
                "optional chaining"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. It implements the base case for the 'Edge Case String Check' validator, a component explicitly defined in the architecture document (Section 3.3), directly contributing to the task's goal.",
              "sequence_critique": "The sequence is logical. Handling the 'no-op' case (no edge cases to check) is the correct first step in implementing the validation function before tackling the more complex iteration and checking logic.",
              "clarity_critique": "The step is exceptionally clear and actionable. It specifies the exact access pattern (optional chaining), the conditions for early return, and the precise return value, leaving no ambiguity for an AI agent."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Continue the implementation. If the `edgeCases` array is valid and contains strings, iterate through each `edgeCase` string. For each one, check if it is present as a literal substring in the `testCode` string. Hint: `testCode.includes(edgeCase)` is sufficient.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "edgeCases",
                "edgeCase",
                "testCode",
                "testCode.includes"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the project's architecture document (Section 3.3, Job Responsibilities) and constitution. It directly implements the 'Edge Case String Check' using the `ModuleContract.instructions.edgeCases` field, which is a specified feature designed to improve the quality of generated tests.",
              "sequence_critique": "The step's placement is logical. It correctly assumes that the `testCode` and the `edgeCases` array have already been loaded and validated in prior steps, and it focuses on the core iteration and checking logic.",
              "clarity_critique": "The instruction is clear about the check itself (`testCode.includes(edgeCase)`). However, it lacks guidance on how to report failures. To align with the `ValidationResult` and `ValidationError` data structures defined in the constitution, the step should specify that the implementation must collect all missing edge cases and format them into an array of `ValidationError` objects, rather than just performing a series of boolean checks and potentially exiting on the first failure."
            }
          }
        },
        {
          "id": "step_6",
          "description": "For every `edgeCase` string that is *not* found in the `testCode`, create a `ValidationResult` object and add it to a results array. The object must conform to the `ValidationResult` interface, with `validator: 'edge-case'`, a descriptive `message` (e.g., 'Required edge case not found in test code.'), and `details: { missingEdgeCase: string }`. Return the array of failures at the end of the function.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "edgeCase",
                "testCode",
                "ValidationResult",
                "validator",
                "message",
                "details",
                "missingEdgeCase"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step's intent aligns with the task's goal of checking for edge cases. However, the data structure it instructs to create (`{ validator: 'edge-case', message: '...', details: ... }`) directly conflicts with the primary `ValidationResult` interface defined in the Project Constitution (`{ success: boolean; errors?: ValidationError[]; }`). The step promotes the use of a structure explicitly marked as `ValidationResult (Conflicting Definition)`, which is a significant deviation from the established architectural contract.",
              "sequence_critique": "The logical step of creating a result for each failure is sound, assuming it's part of a loop iterating over the required edge cases. No issues with the sequence.",
              "clarity_critique": "The instruction is clear about what to do but is critically flawed because it directs the implementation to produce an object that violates the project's core `ValidationResult` type. A correct implementation should produce a single `ValidationResult` object where `success` is false and the `errors` array contains a `ValidationError` object for each missing edge case. The step must be rewritten to conform to the established `ValidationResult` and `ValidationError` interfaces."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Create a unit test file at `src/validation/edgeCaseValidator.test.ts`. Import the `validateEdgeCases` function and the necessary types. Use Jest for the test suite.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/validation/edgeCaseValidator.test.ts",
                "validateEdgeCases",
                "necessary types"
              ],
              "technology_hints": [
                "Jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step to create a unit test file is directly aligned with the task of implementing a validator, as testing is an integral part of the implementation process.",
              "sequence_critique": "The step logically follows the creation of the implementation file. Creating a test file after the function to be tested has been defined is a standard and correct sequence.",
              "clarity_critique": "The specified file path `src/validation/edgeCaseValidator.test.ts` is inconsistent with the project's established file structure. The Project Constitution's `project_file_map` indicates that specific validation engines should reside in `src/validation/engines/` (e.g., `tscValidator.ts`). To adhere to the dominant co-location pattern for tests, the test file should be created at `src/validation/engines/edgeCaseValidator.test.ts`, alongside its corresponding implementation file."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Write unit tests for the success scenarios in `edgeCaseValidator.test.ts`. Cover the following cases: 1) A test where all required edge cases are present. 2) A test where the `edgeCases` array in the contract is empty. 3) A test where the `instructions` object is missing from the contract. 4) A test where the `edgeCases` property is `null`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "edgeCaseValidator.test.ts",
                "edgeCases",
                "contract",
                "instructions"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "Steps are perfectly aligned with the task goal. They cover the primary success path and essential edge cases (empty or non-existent requirements), which are critical for creating a robust validator as per the architecture defined in Section 3.3 of the architecture document.",
              "sequence_critique": "The sequence is logical. Testing success scenarios is a foundational step for implementing the validator, establishing the baseline for correct behavior before tackling failure conditions.",
              "clarity_critique": "The instructions are clear and actionable, with explicitly defined test cases. To be hyper-precise, the prompt could specify the expected `ValidationResult` structure (e.g., `{ success: true, errors: [] }`) for each success case, aligning with the data structures defined in the constitution (Task 2.9). However, this is a minor refinement, and the current clarity is high."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Write unit tests for the failure scenarios. Cover these cases: 1) A test where one of two required edge cases is missing, expecting one failure result. 2) A test where both required edge cases are missing, expecting two failure results. 3) A test to confirm the check is case-sensitive (e.g., the edge case is 'Timeout' but the code contains 'timeout').",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "Steps are well-aligned with the task of creating a robust validator. Testing failure scenarios, including partial failures and case sensitivity, directly supports the project's core mission of ensuring strict adherence to contracts.",
              "sequence_critique": "The sequence is logical. It focuses on testing failure scenarios, which is a critical part of ensuring the validator's correctness and reliability.",
              "clarity_critique": "The instructions are clear but could be more precise by referencing the established data structures from the Project Constitution. For example, instead of 'expecting one failure result', it should specify 'expecting a `ValidationResult` object where the `errors` array contains one `ValidationError` object' to ensure the output conforms to the project's `ValidationResult` and `ValidationError` interfaces."
            }
          }
        }
      ],
      "Task 2.12: Implement the validation orchestrator within the Job to run the sequence of checks on IMG output.": [
        {
          "id": "step_1",
          "description": "Create a new directory `src/job/validation` to house all validation-related logic. Inside this directory, create a file named `types.ts`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/job/validation",
                "types.ts"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is misaligned with the Project Constitution. The constitution already establishes a dedicated top-level directory `src/validation/` for all validation-related logic, including `src/validation/engines/` for implementations and `src/validation/types.ts` for shared types. Creating a new, redundant directory at `src/job/validation/` fragments the architecture, violates the established file map, and incorrectly co-locates validation logic within the Job's specific implementation folder. The Job process is a *consumer* of the validation engines, which are a distinct, reusable component.",
              "sequence_critique": "The step is logically flawed as it proposes creating a directory structure that conflicts with the project's established architecture. It should not be executed. The correct approach is to use the existing `src/validation/` directory for any new validation-related components.",
              "clarity_critique": "The instruction is clear in its intent to create a directory and file, but the specified path `src/job/validation/` is incorrect and directly contradicts the architectural blueprint defined in the Project Constitution."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `src/job/validation/types.ts`, define and export the core data structures for validation. Create a `ValidationFailure` interface with fields: `validator` (string literal type for each validator, e.g., 'json-schema' | 'tsc' | ...), `message` (string), and `details` (string | object). Also, define a minimal `ModuleContract` interface containing only the fields relevant to validation for now: `name` (string) and `instructions.edgeCases` (optional string array).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/job/validation/types.ts",
                "ValidationFailure",
                "ModuleContract",
                "instructions.edgeCases",
                "'json-schema'",
                "'tsc'"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal of defining data structures for validation is aligned with the task. However, the implementation is misaligned with the Project Constitution. It directs the creation of a new `ValidationFailure` interface which is identical to the `ValidationFailure (Simplified)` structure already identified in the constitution as conflicting with the canonical `FailureEntry` type. Furthermore, creating a new minimal `ModuleContract` interface is redundant and introduces inconsistency, as a comprehensive `ModuleContract` type is already a core, well-defined data structure.",
              "sequence_critique": "Defining types before implementing the logic that uses them is a correct sequence. However, the proposed file path `src/job/validation/types.ts` is inconsistent with the established project file map. The constitution designates `src/validation/types.ts` for shared validation types, and creating a separate, job-specific validation type file introduces architectural fragmentation.",
              "clarity_critique": "The instructions are clear but flawed. They explicitly direct the agent to create types that contradict the established architecture. A more constitutionally-aligned instruction would direct the agent to import and use the existing canonical types, such as `FailureEntry` and `ValidatorType`, from their defined locations (e.g., `src/core/types.ts`, `src/validation/types.ts`) rather than creating new, conflicting definitions."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Create a new file `src/job/validation/orchestrator.ts`. Define an asynchronous function `runValidationSequence`. It should accept an object containing: `imgResponse` (any), `contract` (ModuleContract), and `workspacePath` (string). It should return a `Promise<ValidationFailure[]>`. An empty array signifies success.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/job/validation/orchestrator.ts",
                "runValidationSequence",
                "imgResponse",
                "contract",
                "ModuleContract",
                "workspacePath",
                "ValidationFailure"
              ],
              "technology_hints": [
                "TypeScript",
                "Promise"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal is aligned, but its implementation details deviate from the constitution. 1) The proposed file path 'src/job/validation/orchestrator.ts' creates a new top-level 'src/job/' directory, which is inconsistent with the established `project_file_map` that places job logic in 'src/core/job.ts' and validation logic in 'src/validation/'. 2) The proposed return type 'Promise<ValidationFailure[]>' introduces a new, undefined type that conflicts with the canonical 'ValidationResult' and 'ValidationError' structures, perpetuating a known data structure conflict instead of resolving it.",
              "sequence_critique": "The step is logically sequenced as the initial creation of the orchestrator's file and function signature.",
              "clarity_critique": "The step lacks clarity in its type definitions. 1) The 'imgResponse' parameter is typed as 'any', but the constitution defines a specific 'ImgSuccessResponse' interface which should be used. 2) The return type 'ValidationFailure[]' is ambiguous as 'ValidationFailure' is not defined; the step should use the canonical 'ValidationError[]' type from the constitution to represent a list of failures."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Create stub files for each individual validation check within `src/job/validation/`: `schemaValidator.ts`, `tscValidator.ts`, `dslValidator.ts`, `edgeCaseValidator.ts`, and `testRunnerValidator.ts`. Each file should export a function with the appropriate signature that currently returns a resolved promise with an empty array (e.g., `export const validateTsc = async (...): Promise<ValidationFailure[]> => [];`).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/job/validation/",
                "schemaValidator.ts",
                "tscValidator.ts",
                "dslValidator.ts",
                "edgeCaseValidator.ts",
                "testRunnerValidator.ts",
                "validateTsc",
                "ValidationFailure[]"
              ],
              "technology_hints": [
                "Promise"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's intent to create stubs for validation engines is aligned with the task. However, the implementation details conflict with the Project Constitution. Specifically: 1. The proposed file path `src/job/validation/` is inconsistent with the established constitutional path `src/validation/engines/`. 2. The proposed return type `Promise<ValidationFailure[]>` conflicts with the canonical `ValidationResult` interface (`{ success: boolean; errors?: ValidationError[] }`) which is the established data structure for validation outcomes.",
              "sequence_critique": "The sequence is logical. Creating stubs for the individual validation engines is a correct prerequisite for implementing the orchestrator that will call them.",
              "clarity_critique": "The action to create stub files is clear, but the step lacks specificity in two key areas: 1. The function signature is underspecified as `(...)`. It should define the expected input parameters for each validator function (e.g., workspace path, generated code, module contract) to ensure a consistent interface. 2. The use of the non-standard `ValidationFailure[]` type introduces ambiguity and conflicts with the clearly defined `ValidationResult` type in the constitution."
            }
          }
        },
        {
          "id": "step_5",
          "description": "In `src/job/validation/orchestrator.ts`, implement the orchestration logic inside `runValidationSequence`. Import and call each of the five validation functions from the stub files in the correct order as defined in the architecture (Schema, TSC, DSL, Edge Case, Test Runner). Aggregate all failures from each step into a single array and return it. Ensure the implementation handles both synchronous and asynchronous validators.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/job/validation/orchestrator.ts",
                "runValidationSequence",
                "Schema",
                "TSC",
                "DSL",
                "Edge Case",
                "Test Runner"
              ],
              "technology_hints": [
                "asynchronous validators"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of implementing the validation orchestrator as described in the architecture document (Section 3.3). It directly addresses the core responsibility of sequencing the validation checks.",
              "sequence_critique": "The specified order of validators (Schema, TSC, DSL, Edge Case, Test Runner) is correct and consistent with the architecture. However, the instruction to 'Aggregate all failures from each step' implies that all validators run regardless of earlier failures. This is logically flawed. A critical failure, such as a TSC compilation error, should prevent subsequent validators (like the Test Runner) from executing. The orchestrator must implement a 'fail-fast' or 'short-circuit' logic, stopping and reporting failure as soon as one validation step fails.",
              "clarity_critique": "The instruction is mostly clear but contains a critical ambiguity. The phrase 'Aggregate all failures from each step' is misleading. It should be rephrased to explicitly require a short-circuiting behavior: 'Execute validators sequentially. If any validator reports a failure, the sequence must halt immediately and return the failures from that step. Do not execute subsequent validators.'"
            }
          }
        },
        {
          "id": "step_6",
          "description": "Implement the `schemaValidator.ts`. The exported function should validate that the `imgResponse` object has the structure `{ implementationCode: string, testCode: string }`. Use a library like `zod` for robust schema validation. If validation fails, return an array with a single `ValidationFailure` object containing the details. Hint: `npm install zod`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "npm install zod"
              ],
              "key_entities_dependencies": [
                "schemaValidator.ts",
                "imgResponse",
                "ValidationFailure"
              ],
              "technology_hints": [
                "zod",
                "npm"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of creating the validation orchestrator and directly implements the first required validation check (IMG response schema validation) as defined in the architecture.",
              "sequence_critique": "The step is logically sound. Implementing the schema validator is a correct and necessary prerequisite for the subsequent validation steps within the Job process.",
              "clarity_critique": "The step's instruction to return a `ValidationFailure` object conflicts with the canonical `ValidationResult` and `ValidationError` interfaces established in the Project Constitution. To maintain consistency, the step should instruct the function to return a `Promise<ValidationResult>`, where the `errors` array would contain `ValidationError` objects on failure."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Implement the `edgeCaseValidator.ts`. The function should take the `testCode` string and the `contract` object. It should check if every string literal from `contract.instructions.edgeCases` is present in the `testCode`. For each missing edge case, generate a `ValidationFailure` object. Return an array of all such failures.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "edgeCaseValidator.ts",
                "testCode",
                "contract",
                "contract.instructions.edgeCases",
                "ValidationFailure"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step's core logic is well-aligned with the architecture's requirement for an 'Edge Case String Check' as defined in Section 3.3 of the architecture document. However, the specified output data structure, `ValidationFailure`, conflicts with the canonical `ValidationResult` and `ValidationError` types established in the Project Constitution (derived from Task 2.9). To maintain consistency and ensure the validation orchestrator can handle all validator outputs uniformly, this step must be updated to produce a `ValidationResult` object containing an array of `ValidationError` objects.",
              "sequence_critique": "The step is logically sequenced. Implementing individual validators like this one is a necessary prerequisite before implementing the orchestrator that calls them.",
              "clarity_critique": "The prompt should be more precise to align with the project's established patterns. It should specify the canonical return type as `Promise<ValidationResult>` and clarify that each missing edge case should generate a `ValidationError` object. Additionally, to prevent architectural drift, it should specify the file's location as `src/validation/engines/edgeCaseValidator.ts`, which is the established pattern for validation engines, rather than the conflicting path `src/validation/edgeCaseValidator.ts` noted in the constitution."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Implement the `tscValidator.ts`. This function will be asynchronous. It needs to: 1. Use `fs/promises` to write the `implementationCode` to a temporary file `<ModuleName>.ts` inside the `workspacePath`. 2. Use Node.js `child_process.exec` to run the TypeScript compiler (`tsc --noEmit --strict --target es2020 --module commonjs <path-to-temp-file>`) against the temporary file. 3. Capture `stdout` and `stderr`. If the process exits with a non-zero code, parse the output to create a detailed `ValidationFailure` object. 4. Ensure the temporary file is deleted after the check, regardless of success or failure.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "tscValidator.ts",
                "implementationCode",
                "ModuleName",
                "workspacePath",
                "ValidationFailure"
              ],
              "technology_hints": [
                "fs/promises",
                "Node.js",
                "child_process.exec",
                "TypeScript compiler (tsc)"
              ]
            },
            "step_critique": {
              "alignment_critique": "The steps are well-aligned with the task of creating a `tsc` validation engine, which is a core requirement of the project architecture as defined in the constitution and architecture document.",
              "sequence_critique": "The instruction in step 4 to delete the temporary file 'regardless of success or failure' is a critical flaw. If `tsc` validation succeeds, the generated file (`<ModuleName>.ts`) must persist within the workspace because it is a required input for subsequent validation steps, such as the test runner engine. Deleting it would break the validation sequence.",
              "clarity_critique": "The instructions are inconsistent with the established Project Constitution in two key areas. First, step 2 prescribes hardcoding `tsc` compiler flags in the `exec` command, which directly contradicts the architectural decision (derived from Task 2.9) to use a dynamically generated `tsconfig.validation.json` for more robust and maintainable configuration. Second, step 3 refers to a `ValidationFailure` object, which is an undefined and non-canonical data structure. The instruction should specify the creation of a `ValidationResult` object containing `ValidationError` entries, as defined in the constitution."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Implement the `testRunnerValidator.ts`. This is similar to the TSC validator. It needs to: 1. Write both `implementationCode` and `testCode` to their respective files (`<ModuleName>.ts` and `__tests__/<ModuleName>.test.ts`) inside the workspace. 2. Use `child_process.exec` to run the test runner (e.g., `jest --silent --json --outputFile=jest-output.json <path-to-test-file>`). 3. If the test runner fails, parse the output (e.g., the `jest-output.json` file) to extract failure details and create `ValidationFailure` objects. 4. Ensure all temporary files are cleaned up.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "testRunnerValidator.ts",
                "implementationCode",
                "testCode",
                "<ModuleName>.ts",
                "__tests__/<ModuleName>.test.ts",
                "jest-output.json",
                "ValidationFailure"
              ],
              "technology_hints": [
                "child_process.exec",
                "jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the project's goal of producing test-covered modules and the architecture's requirement for a test-runner validation engine. However, Step 3's use of the term `ValidationFailure` conflicts with the canonical `ValidationError` data structure established in the constitution and should be reconciled to maintain architectural consistency.",
              "sequence_critique": "The sequence is logical. However, it omits a critical configuration detail mandated by the non-functional requirement `Reliability (Test Coverage Enforcement)`. The test runner command in Step 2 must be configured to fail if the generated test file contains no tests (e.g., using `--passWithNoTests=false` for Jest).",
              "clarity_critique": "Step 4, \"Ensure all temporary files are cleaned up,\" is ambiguous. It should specify that only files created *by this validator* (e.g., `jest-output.json`) should be cleaned up. The primary generated files (`<ModuleName>.ts`, `__tests__/<ModuleName>.test.ts`) must persist for subsequent validation steps within the same Job's validation sequence."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Implement the `dslValidator.ts` as a placeholder. As per the architecture, a full DSL implementation is a future enhancement. The function should log a `WARN` level message to the console stating 'DSL validation is not yet implemented' and return an empty array, effectively passing the validation step for now.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "dslValidator.ts"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Creating a placeholder for the DSL validator is consistent with the architecture document (Section 13), which explicitly defers a full implementation to a future phase. This allows the primary task of building the validation orchestrator to proceed without being blocked.",
              "sequence_critique": "The step is a self-contained unit of work. Its sequence within the larger task is logical, as it provides a necessary (though stubbed) component for the main validation orchestrator.",
              "clarity_critique": "The instruction 'return an empty array' is slightly ambiguous. To ensure consistency with the project's data structures, it should explicitly state that the function must return a `ValidationResult` object indicating success, as defined in the constitution (e.g., `{ success: true, errors: [] }`)."
            }
          }
        },
        {
          "id": "step_11",
          "description": "Create a new file `src/job/validation/orchestrator.test.ts`. Write unit tests for the `runValidationSequence` function. Use a mocking library (e.g., Jest's `jest.mock`) to mock the individual validator modules. Test that the orchestrator calls validators in the correct sequence and correctly aggregates failures from multiple sources.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/job/validation/orchestrator.test.ts",
                "runValidationSequence",
                "validator modules"
              ],
              "technology_hints": [
                "Jest",
                "jest.mock"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal of creating unit tests for the validation orchestrator is well-aligned with the task. However, the proposed file path `src/job/validation/orchestrator.test.ts` is inconsistent with the Project Constitution. The constitution places the job worker's logic in `src/core/job.ts`. The validation orchestrator is a core part of the job's logic, not a separate validation engine. Therefore, its source and test files should be located within the `src/core/` directory structure (e.g., in a new `src/core/job/` subdirectory) to maintain architectural consistency, rather than creating a new top-level `src/job/` directory.",
              "sequence_critique": "The step is logically sequenced, following a standard test-driven or test-concurrent development pattern where tests are written to verify the behavior of a core function.",
              "clarity_critique": "The instructions are clear and actionable. They specify the function to test (`runValidationSequence`), the required technique (`jest.mock`), and the essential test cases (correct sequence of calls and aggregation of failures)."
            }
          }
        },
        {
          "id": "step_12",
          "description": "Write unit tests for the synchronous validators: `schemaValidator.ts` and `edgeCaseValidator.ts`. For `schemaValidator`, test with valid, invalid, and incomplete `imgResponse` objects. For `edgeCaseValidator`, test scenarios where all, some, and no edge cases are found in the test code.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "schemaValidator.ts",
                "edgeCaseValidator.ts",
                "imgResponse"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. The main task is to build a validation orchestrator. Testing the individual validator components that the orchestrator will call is a direct and necessary prerequisite. This directly supports the Job's responsibilities as defined in Section 3.3 of the architecture document.",
              "sequence_critique": "The sequence is logical. Unit testing individual, low-level components before assembling them into a higher-level orchestrator is a standard and robust development practice. No issues found.",
              "clarity_critique": "The prompt is clear and actionable, providing specific positive and negative test scenarios. To enhance clarity and enforce architectural consistency, it could explicitly state the full, constitution-compliant file paths for the validators being tested (e.g., `src/validation/engines/schemaValidator.ts`) and their corresponding test files, resolving any potential path conflicts noted in the constitution."
            }
          }
        },
        {
          "id": "step_13",
          "description": "Write unit tests for the asynchronous validators that use `child_process`: `tscValidator.ts` and `testRunnerValidator.ts`. This is complex and requires mocking `child_process.exec` and `fs/promises`. Test the logic for parsing successful and failed command outputs. For example, mock `exec` to return a non-zero exit code and specific stderr content, then assert that your validator creates the correct `ValidationFailure` object.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "tscValidator.ts",
                "testRunnerValidator.ts",
                "child_process.exec",
                "fs/promises",
                "exec",
                "ValidationFailure"
              ],
              "technology_hints": [
                "child_process",
                "fs/promises"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step correctly identifies the need to test core validation components, which is essential for the reliability of the overall validation orchestrator. However, it critically deviates from the Project Constitution by instructing the agent to assert the creation of a `ValidationFailure` object. The constitution defines a more robust, canonical structure (`ValidationResult` containing `ValidationError` objects, derived from Task 2.9) and explicitly flags `ValidationFailure (Simplified)` as a conflicting definition to be reconciled. The tests must align with the canonical data structures to ensure architectural consistency.",
              "sequence_critique": "The sequence is logical. Writing comprehensive unit tests for individual validation engines is a necessary prerequisite before or concurrent with implementing the orchestrator that depends on them. This follows standard test-first principles.",
              "clarity_critique": "The instruction is mostly clear regarding the required mocking and test scenarios. However, it is critically flawed by its reference to the incorrect data structure. To be actionable and consistent with the constitution, the instruction should be rephrased to: \"...assert that your validator creates the correct `ValidationResult` object, populating the `errors` array with structured `ValidationError` objects based on the mocked `stderr`.\""
            }
          }
        },
        {
          "id": "step_14",
          "description": "Create an `index.ts` file in `src/job/validation/` that exports the main `runValidationSequence` function and the `ValidationFailure` type. This will provide a clean public interface for the validation module to be used by the main Job process logic.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "index.ts",
                "src/job/validation/",
                "runValidationSequence",
                "ValidationFailure"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is misaligned with the Project Constitution in two key areas. First, the proposed file path `src/job/validation/` conflicts with the established top-level `src/validation/` directory, which is designated for all validation engines and their related types. This introduces architectural inconsistency. Second, the step refers to a `ValidationFailure` type, which is not a canonical data structure in the constitution. It conflicts with several established types like `FailureEntry`, `ValidationError`, and the explicitly noted-as-conflicting `ValidationFailure (Simplified)`. Introducing another undefined failure type deviates from the goal of having a strict, well-defined data model.",
              "sequence_critique": "The step of creating a barrel file (`index.ts`) is a logical concluding action for a module. However, its correctness depends on prior steps having defined the functions and types to be exported, which is not specified here.",
              "clarity_critique": "The instruction is not sufficiently clear for a meticulous agent. It is ambiguous because it references the non-canonical `ValidationFailure` type. To be actionable and consistent, it must specify which of the constitution's defined types (e.g., `ValidationError`, `FailureEntry`) should be exported."
            }
          }
        }
      ],
      "Task 2.13: Implement the Job's failure reporting logic to write `failures.json` and `final_failure.json`.": [
        {
          "id": "step_1",
          "description": "Create a new file at `src/job/failure-types.ts` to define the data structures for failure reporting, strictly adhering to the architecture specification. Define two exported TypeScript interfaces: `ValidationFailure` and `FinalFailureReport`. The `ValidationFailure` interface should include fields like `attempt`, `timestamp`, `validator`, `message`, and `details`. The `FinalFailureReport` should include `module`, `attempts`, `lastPromptHash`, `errorSummary`, and an array of `ValidationFailure` objects, as described in Section 4 of the architecture document.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/job/failure-types.ts",
                "ValidationFailure",
                "FinalFailureReport",
                "architecture specification",
                "Section 4 of the architecture document"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step has significant alignment issues. 1. It proposes a new `ValidationFailure` interface, which directly conflicts with the canonical `FailureEntry` data structure already defined in the constitution for this purpose. The constitution explicitly warns against a similar conflicting definition and calls for reconciliation. 2. The proposed structure for `FinalFailureReport` (including an array of all failures) contradicts the constitution's definition and the example schema in the architecture document (Appendix A.2), which specify it as a concise summary for the `final_failure.json` file. The detailed history belongs in the `IntermediateFailureReport` (`failures.json`). 3. The proposed file path `src/job/failure-types.ts` deviates from the established project structure, which centralizes core types in `src/core/types.ts`.",
              "sequence_critique": "The sequence of defining types before implementing logic that uses them is correct. However, the step is misplaced; it should modify the existing, canonical type definition file (`src/core/types.ts`) rather than creating a new, architecturally inconsistent file.",
              "clarity_critique": "The instructions are clear but direct the agent to implement structures and file paths that violate the Project Constitution. The agent would likely execute the step as written, leading to architectural inconsistencies and data structure conflicts."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a new file at `src/job/FailureReporter.ts`. Implement a `FailureReporter` class responsible for all file-based failure logging. The constructor should accept the `workspaceDir` string. The class should have two public async methods: `logValidationFailure(failure: ValidationFailure): Promise<void>` and `writeFinalReport(moduleName: string, attemptCount: number, lastPromptHash: string): Promise<void>`. Import the types from `failure-types.ts`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/job/FailureReporter.ts",
                "FailureReporter",
                "workspaceDir",
                "logValidationFailure",
                "failure",
                "ValidationFailure",
                "writeFinalReport",
                "moduleName",
                "attemptCount",
                "lastPromptHash",
                "failure-types.ts"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The creation of a dedicated `FailureReporter` class aligns well with the task's goal of encapsulating failure reporting. However, the step deviates from the Project Constitution. It specifies a method parameter `ValidationFailure` which conflicts with the canonical `FailureEntry` data structure. Additionally, the `writeFinalReport` method signature is incomplete, as it omits the `errorSummary` field required by the `FinalFailureReport` data model.",
              "sequence_critique": "The sequence is logical. Creating the class to encapsulate the reporting logic is the correct foundational step for this task.",
              "clarity_critique": "The instructions are procedurally clear but create semantic conflicts with the constitution. To avoid ambiguity, the step should explicitly reference the canonical `FailureEntry` and `FinalFailureReport` types. It also refers to a non-existent `failure-types.ts` file; according to the file map, these types should be imported from `src/core/types.ts`."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Implement the `logValidationFailure` method in `FailureReporter.ts`. This method must be resilient. It should read the existing `failures.json` from the workspace, parse it as an array of `ValidationFailure` objects, append the new failure, and write the updated array back to the file. If `failures.json` does not exist, it should start with a new array containing just the new failure. Use the `fs/promises` module for all file operations and wrap them in try/catch blocks to handle potential file system errors.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "logValidationFailure method",
                "FailureReporter.ts",
                "failures.json",
                "ValidationFailure object"
              ],
              "technology_hints": [
                "fs/promises"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task goal. Implementing a resilient method to append validation failures to `failures.json` is a core part of the Job's failure reporting logic as defined in the architecture.",
              "sequence_critique": "The sequence of operations described within the step (read, parse, append, write) is logical for managing an intermediate log file that grows with each failure within a single Job's retry loop.",
              "clarity_critique": "The step's clarity is compromised by its reference to `ValidationFailure` objects. The Project Constitution explicitly identifies a conflict between a simplified `ValidationFailure` type and the canonical `FailureEntry` data structure. To ensure consistency and prevent implementation of a conflicting type, the instruction must be revised to specify that the method should append the canonical `FailureEntry` object to an array of type `IntermediateFailureReport` (which is an alias for `FailureEntry[]`)."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Implement the `writeFinalReport` method in `FailureReporter.ts`. This method will be called when a Job has exhausted all its retries. It should first read the accumulated failures from `failures.json`. Then, it should construct a `FinalFailureReport` object containing the `moduleName`, `attemptCount`, `lastPromptHash`, a concise `errorSummary` (e.g., from the last failure), and the full array of `ValidationFailure` objects. Finally, it must write this object as a formatted JSON string to `final_failure.json` in the workspace.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "writeFinalReport",
                "FailureReporter.ts",
                "failures.json",
                "FinalFailureReport",
                "moduleName",
                "attemptCount",
                "lastPromptHash",
                "errorSummary",
                "ValidationFailure",
                "final_failure.json"
              ],
              "technology_hints": [
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is misaligned with the Project Constitution. The instruction to include \"the full array of ValidationFailure objects\" in the `FinalFailureReport` directly contradicts the constitution's definition and the example schema in Appendix A.2. The constitution specifies `FinalFailureReport` as a concise summary containing only `moduleName`, `attempts`, `lastPromptHash`, and `errorSummary`. The full history of failures is meant to reside in the intermediate `failures.json` file within the job workspace, not in the final summary report.",
              "sequence_critique": "The sequence of operations described within the step (read intermediate failures, construct final report, write final report) is logical.",
              "clarity_critique": "The step has two clarity issues. First, it uses the term `ValidationFailure`, which is listed in the constitution as a simplified, conflicting data structure. The canonical type for an individual failure record is `FailureEntry`. Second, the contradiction with the constitution regarding the contents of `FinalFailureReport` creates a critical ambiguity for the implementing agent, forcing it to choose between the step's instructions and the project's foundational rules."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Create a new unit test file `src/job/FailureReporter.test.ts`. Use Jest to write tests for the `FailureReporter` class. You will need to mock the `fs/promises` module to simulate file system interactions without writing to disk. Use `jest.mock('fs/promises')` and provide mock implementations for `readFile` and `writeFile`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/job/FailureReporter.test.ts",
                "FailureReporter",
                "readFile",
                "writeFile"
              ],
              "technology_hints": [
                "Jest",
                "fs/promises",
                "jest.mock('fs/promises')"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal of implementing failure reporting. Writing unit tests for the reporting component is a crucial part of the implementation process, ensuring its reliability.",
              "sequence_critique": "The sequence is logical. Creating unit tests for a class is a standard and necessary step in its implementation, whether following Test-Driven Development or a more traditional approach.",
              "clarity_critique": "The step introduces a significant architectural inconsistency. The proposed file path `src/job/FailureReporter.test.ts` creates a new `src/job/` directory, which contradicts the Project Constitution's `project_file_map` that places job-related logic in `src/core/job.ts` and its corresponding test in `src/core/job.test.ts`. The path should be reconciled with the established architecture, likely placing the new test file at `src/core/failureReporter.test.ts` to be co-located with other core job logic."
            }
          }
        },
        {
          "id": "step_6",
          "description": "In `FailureReporter.test.ts`, write unit tests for the `logValidationFailure` method. Create two test cases: 1) When `failures.json` does not exist, it should be created with a single failure entry. 2) When `failures.json` already exists, the new failure should be appended to the existing list. Use `mocked(fs.writeFile).mock.calls` to assert that the correct data is being written to the correct file path.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "FailureReporter.test.ts",
                "logValidationFailure",
                "failures.json",
                "fs.writeFile",
                "mocked(fs.writeFile).mock.calls"
              ],
              "technology_hints": [
                "Jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with testing the `failures.json` logic, which is a core part of the task's goal. However, the task also mentions writing `final_failure.json`, which this step does not address. This is acceptable assuming a subsequent step will cover testing the logic for the final report.",
              "sequence_critique": "The sequence of the two test cases described (file does not exist, then file exists) is logical and correctly covers the two primary states for an append-or-create file operation.",
              "clarity_critique": "The step is mostly clear but has a significant omission. For the 'append' test case, the implementation must first read the existing file. The step only instructs to assert on `fs.writeFile`, but fails to mention that the test setup must also mock the pre-existing `failures.json` file content so it can be read by the method under test. This ambiguity could lead to an incomplete or incorrect test implementation."
            }
          }
        },
        {
          "id": "step_7",
          "description": "In `FailureReporter.test.ts`, write a comprehensive unit test for the `writeFinalReport` method. Simulate an existing `failures.json` file with several failure entries. Call `writeFinalReport` and assert that `fs/promises.writeFile` is called for `final_failure.json`. Verify that the content written is a valid JSON string and correctly matches the `FinalFailureReport` structure, including all the details from the simulated `failures.json`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "FailureReporter.test.ts",
                "writeFinalReport",
                "failures.json",
                "failure entries",
                "fs/promises.writeFile",
                "final_failure.json",
                "FinalFailureReport"
              ],
              "technology_hints": [
                "TypeScript",
                "fs/promises",
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is misaligned with the Project Constitution. It instructs to test that `writeFinalReport` includes 'all the details from the simulated `failures.json`' in the final report. However, the constitution defines `FinalFailureReport` (the structure for `final_failure.json`) as a concise summary containing `module`, `attempts`, `lastPromptHash`, and `errorSummary`, not a full list of `FailureEntry` objects. The test described would validate an implementation that violates the established data model.",
              "sequence_critique": "The sequence of actions within the proposed unit test (simulate, call, assert file write, assert content) is logical and follows standard testing patterns.",
              "clarity_critique": "The step is contradictory and therefore lacks clarity. It asks to verify that the output both matches the `FinalFailureReport` structure (a summary) and includes 'all the details' from the intermediate `failures.json` (a list). An AI agent cannot satisfy both requirements simultaneously, making the instruction ambiguous and non-actionable without correction."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Review and refine the `FailureReporter.ts` implementation. Add comprehensive JSDoc comments to the class, its constructor, and its public methods, explaining their purpose, parameters, and behavior. Ensure all file paths are correctly constructed using `path.join` to be OS-agnostic. Verify that JSON output is human-readable by using `JSON.stringify(data, null, 2)`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "FailureReporter.ts",
                "path.join",
                "JSON.stringify"
              ],
              "technology_hints": [
                "JSDoc",
                "path.join",
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal of implementing robust failure reporting. The focus on JSDoc, OS-agnostic path construction, and human-readable JSON output directly supports the project's constitutional requirements for Maintainability, cross-platform support, and Developer Experience (UX) for debugging.",
              "sequence_critique": "The step is a logical refinement and review action. It correctly assumes a base implementation exists and focuses on improving its quality and correctness. No sequencing issues are present.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. Providing specific implementation details like `path.join` and the exact `JSON.stringify(data, null, 2)` signature makes the requirements unambiguous for an AI agent."
            }
          }
        }
      ],
      "Task 2.14: Implement the Task Runner's logic for spawning, monitoring, and awaiting Job process completion.": [
        {
          "id": "step_1",
          "description": "Create a new file `src/task-runner/job-runner.ts`. This module will be responsible for managing the lifecycle of all module generation jobs. Also, create a corresponding test file `src/task-runner/job-runner.test.ts`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner/job-runner.ts",
                "module generation jobs",
                "src/task-runner/job-runner.test.ts"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Creating a dedicated `job-runner.ts` file within the `src/task-runner/` directory is a logical and necessary action to house the logic for spawning, monitoring, and managing Job processes, which is the core objective of the task.",
              "sequence_critique": "The sequence is correct. Creating the source and test files is the foundational first step before any implementation can occur.",
              "clarity_critique": "The step is clear and actionable. It specifies the exact file paths and provides a concise, accurate description of the module's responsibility, which aligns with the Task Runner's design in the architecture document."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `src/task-runner/job-runner.ts`, define the necessary data structures. Create a `JobTask` interface representing the input for a single job (e.g., `{ moduleName: string, workspacePath: string, jobScriptPath: string }`). Also, create a `JobResult` interface for the output of a completed job (e.g., `{ moduleName: string, exitCode: number, stdout: string, stderr: string }`).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner/job-runner.ts",
                "JobTask",
                "JobResult"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step correctly identifies the need for data structures to manage job inputs and outputs, which is aligned with the task. However, the proposed `JobResult` interface (`{ moduleName, exitCode, stdout, stderr }`) directly conflicts with the canonical `JobResult` already defined in the Project Constitution. The constitution's `JobResult` is a higher-level structure representing the final outcome, containing a status ('SUCCESS', 'FAILURE') and the `FinalFailureReport`. The structure proposed in this step represents raw process completion details, not the final interpreted outcome of the job. This inconsistency must be reconciled to maintain architectural integrity.",
              "sequence_critique": "The sequence of defining data structures before implementing the logic that uses them is correct and logical.",
              "clarity_critique": "The step is actionable but will cause a direct conflict with the established project constitution due to the naming of `JobResult`. The instruction should be revised to avoid this ambiguity. It should either specify a different name for the raw process output (e.g., `JobProcessOutput`) or clarify that this low-level structure is an intermediate result used to construct the canonical `JobResult` by inspecting the job's workspace for success artifacts or a `final_failure.json` file."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Implement a helper function `spawnJobProcess(task: JobTask): Promise<JobResult>`. This function will wrap the creation of a single child process in a Promise. Use Node.js's `child_process.fork(task.jobScriptPath, [task.workspacePath])` to spawn the job. Capture all `stdout` and `stderr` data. The promise should resolve with a `JobResult` object when the process emits the 'exit' event. It should reject if the process emits an 'error' event.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "spawnJobProcess",
                "JobTask",
                "JobResult",
                "child_process.fork",
                "task.jobScriptPath",
                "task.workspacePath",
                "stdout",
                "stderr",
                "'exit' event",
                "'error' event"
              ],
              "technology_hints": [
                "Node.js",
                "child_process",
                "Promise"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task, phase, and overall architectural goals. It directly implements a core responsibility of the Task Runner\u2014spawning and monitoring isolated Job processes\u2014as defined in Section 3.2 of the architecture document.",
              "sequence_critique": "The step is logically sound as a self-contained unit of work. Implementing this foundational helper function is a correct prerequisite before building the higher-level orchestration logic that will manage a pool of these jobs.",
              "clarity_critique": "The step is clear but conflates two responsibilities: raw process execution and interpreting the outcome into a domain-specific `JobResult`. A better design would have `spawnJobProcess` resolve with a raw execution result (e.g., `{ exitCode: number, stdout: string, stderr: string }`), leaving the calling orchestrator to perform the subsequent business logic of inspecting the workspace (for `final_failure.json`) and constructing the final `JobResult`. This improves separation of concerns and reusability. The step should also specify that captured `stdout` and `stderr` should be logged for observability."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Implement the main orchestration function `runAllJobs(tasks: JobTask[]): Promise<JobResult[]>`. This function will manage the concurrent execution of all jobs. Start by reading the `IRONCLAD_MAX_PARALLEL` environment variable, defaulting to a sensible value like `1` or the number of CPU cores if the variable is not set. Use a library like `os` to get CPU core count.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runAllJobs",
                "JobTask",
                "JobResult",
                "IRONCLAD_MAX_PARALLEL"
              ],
              "technology_hints": [
                "os",
                "Promise"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal and directly implements a core responsibility of the Task Runner as defined in the Project Constitution (Sections 3.2, 5, 9, and 14).",
              "sequence_critique": "The sequence is logical. Implementing the main orchestration function that manages concurrency is a correct foundational step for the Task Runner's job management logic.",
              "clarity_critique": "The step is mostly clear, but could be improved. It should specify the exact fallback logic for concurrency (e.g., 'default to the number of CPU cores, with a minimum of 1') instead of the ambiguous '1 or the number of CPU cores'. Additionally, it should explicitly mention the need to handle invalid or non-numeric values for `IRONCLAD_MAX_PARALLEL` by parsing the string and falling back to the default, consistent with the 'Reliability (Configuration Graceful Degradation)' NFR."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Inside `runAllJobs`, implement the concurrency management logic. A robust pattern is to use a queue of tasks and a pool of active workers. Iterate through the `tasks` array, spawning jobs up to the `maxConcurrency` limit. Use `Promise.all` to wait for the entire batch of jobs to complete. For a more efficient approach that processes tasks as workers become free, manage a running pool of promises and use `Promise.race` to add new tasks as old ones complete. Hint: A simpler, effective approach is to use a library like `p-limit` which handles this concurrency pattern elegantly. Install it if you choose this path: `npm install p-limit`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install it if you choose this path: `npm install p-limit`"
              ],
              "key_entities_dependencies": [
                "runAllJobs",
                "tasks",
                "maxConcurrency"
              ],
              "technology_hints": [
                "Promise.all",
                "Promise.race",
                "p-limit",
                "npm"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal and the project constitution. It directly addresses the core responsibility of the Task Runner to manage a concurrent pool of Job processes, as specified in the architecture (Section 3.2) and the non-functional requirement for scalability (Section 14).",
              "sequence_critique": "The sequence is logical. It presents a series of implementation options from a basic manual approach to a more robust library-based solution, which is a sound way to guide development.",
              "clarity_critique": "The step is clear and highly actionable, particularly the hint to use `p-limit`. The description of the `Promise.all` approach is slightly ambiguous and could lead to an inefficient batching implementation rather than a true concurrent pool where workers are replenished as they become free. To improve architectural consistency, the step could also suggest using the project's own `ConcurrencyManager` class (defined in Task 2.5) as an alternative to a third-party library."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Integrate structured logging into `runAllJobs` and `spawnJobProcess`. Log key events: when the job runner starts, the concurrency limit, when each job is spawned (including its module name and PID), and when each job completes (including its exit code and module name). On job failure (non-zero exit code), log the `stderr` to provide immediate diagnostic information.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runAllJobs",
                "spawnJobProcess",
                "concurrency limit",
                "module name",
                "PID",
                "exit code",
                "stderr"
              ],
              "technology_hints": [
                "structured logging"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. It directly implements the 'Observability (Structured Logging)' non-functional requirement and the detailed logging strategy described in Section 11 of the architecture document. Logging these specific events (spawn, completion, failure) is essential for the system's auditability, reliability, and maintainability.",
              "sequence_critique": "The sequence is logical. Integrating logging is a natural and necessary part of implementing the core job spawning and monitoring functions. No prerequisite steps are missing.",
              "clarity_critique": "The step is clear and actionable. To enhance precision, it could explicitly require the structured logs to be in JSON format and adhere to the contextual fields (`component`, `module`, `taskId`, etc.) defined in Section 11 of the Project Constitution, ensuring perfect consistency with the established architectural standard."
            }
          }
        },
        {
          "id": "step_7",
          "description": "In `src/task-runner/job-runner.test.ts`, write comprehensive unit tests for the `runAllJobs` function. Use `jest.mock('child_process')` to mock the `fork` function. Your mock should allow you to control the behavior of the simulated child processes, including their exit codes and stdout/stderr streams.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner/job-runner.test.ts",
                "runAllJobs",
                "fork",
                "child_process",
                "exit codes",
                "stdout",
                "stderr"
              ],
              "technology_hints": [
                "jest",
                "child_process"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Unit testing the `runAllJobs` function is a critical part of implementing the Task Runner's core responsibility of spawning and monitoring jobs, as defined in Task 2.14 and Section 3.2 of the architecture. The use of mocking for `child_process` aligns with the project's unit testing strategy.",
              "sequence_critique": "The step's placement is logical, as it is the testing counterpart to the implementation of the `runAllJobs` function within the same task. No sequencing issues are apparent.",
              "clarity_critique": "The step is exceptionally clear and actionable. It specifies the exact test file, the function under test, the precise module and function to mock (`child_process.fork`), and the necessary capabilities of the mock (controlling exit codes and streams), which is sufficient for an AI agent to proceed without ambiguity."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Create test cases in `job-runner.test.ts` to cover the following scenarios: 1) A list of jobs that is smaller than the concurrency limit. 2) A list of jobs larger than the concurrency limit, ensuring the limit is respected. 3) A mix of successful (exit code 0) and failed (non-zero exit code) jobs, verifying that all results are collected correctly. 4) A case where `fork` itself throws an error. 5) An empty list of tasks, ensuring the function returns an empty array immediately.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "job-runner.test.ts",
                "concurrency limit",
                "fork"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The test scenarios are well-aligned with the task of implementing a robust job runner. They directly address key non-functional requirements from the constitution, such as 'Reliability (Fault Isolation)' (test case #3) and 'Scalability' (test case #2).",
              "sequence_critique": "The set of test cases is logical, covering core functionality, edge cases, and failure modes. However, it could be made more comprehensive by adding a scenario for a job process that terminates due to a signal (e.g., SIGKILL), which is a distinct failure mode from a non-zero exit code and is handled differently by Node.js's `child_process` module.",
              "clarity_critique": "The descriptions are clear and actionable. For maximum precision, test case #3 ('verifying that all results are collected correctly') should explicitly mention asserting against the structure of the returned `JobResult[]` array, ensuring it aligns with the `JobResult` data structure defined in the project constitution."
            }
          }
        }
      ],
      "Task 2.15: Implement the Task Runner's results aggregation to merge successful code and collate failures.": [
        {
          "id": "step_1",
          "description": "First, define the necessary data structures for handling job results. In a shared types file (e.g., `src/types.ts`), define a `JobResult` type. This type should encapsulate the outcome of a single job process, including `moduleName` (string), `workspacePath` (string), and `exitCode` (number).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/types.ts",
                "JobResult",
                "moduleName",
                "workspacePath",
                "exitCode"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is misaligned with the Project Constitution. It proposes defining a `JobResult` as `{ moduleName, workspacePath, exitCode }`, which represents raw process output. This conflicts with the established, higher-level `JobResult` data structure which includes a `status` ('SUCCESS'/'FAILURE') and the `FinalFailureReport`. The proposed structure is insufficient for the task of aggregation, as the Task Runner needs the interpreted outcome, not just a raw exit code, to decide whether to merge code or collate a failure report.",
              "sequence_critique": "The sequence is logical. Defining data structures as a first step before implementing the logic that uses them is a correct approach.",
              "clarity_critique": "The instruction is clear and actionable, but it directs the agent to implement a data structure that is inconsistent with the project's established architecture. Additionally, it suggests a file path `src/types.ts` which conflicts with the canonical `src/core/types.ts` defined in the constitution's file map."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Create a new module to handle the aggregation logic. Create a file named `src/task-runner/results-aggregator.ts`. This module will be responsible for processing the `JobResult` array.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner/results-aggregator.ts",
                "JobResult"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned. Creating a dedicated `results-aggregator.ts` module within the `src/task-runner/` directory directly supports the task's goal and adheres to the constitutional principle of maintainability by separating concerns.",
              "sequence_critique": "The sequence is logical. Creating the file to house the aggregation logic is the correct foundational first step before implementing the functions within it.",
              "clarity_critique": "The step is clear and actionable. It specifies the exact file path and its high-level responsibility, making it unambiguous for an AI agent."
            }
          }
        },
        {
          "id": "step_3",
          "description": "In `src/task-runner/results-aggregator.ts`, create an async function `processJobResults`. This function will take an array of `JobResult` objects, the repository root path, and the base path for workspaces as arguments. It should iterate over each `JobResult` and determine if the job succeeded or failed. For now, just log whether each job is a 'success' or 'failure'. A job is considered a failure if a `final_failure.json` file exists in its workspace, and a success otherwise. The function should return an object containing two arrays: `successfulModules` and `failedModuleWorkspaces`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner/results-aggregator.ts",
                "processJobResults",
                "JobResult",
                "final_failure.json",
                "successfulModules",
                "failedModuleWorkspaces"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step's goal to classify job outcomes is aligned with the task. However, the prescribed method for determining success/failure is inconsistent with the established `JobResult` data structure in the constitution. The constitution defines `JobResult` as containing a `status` property, making it the source of truth. This step instructs the agent to re-derive the status by checking for the existence of `final_failure.json` on the file system, which is a redundant and less robust approach that ignores the established data contract.",
              "sequence_critique": "The sequence is logical. Classifying job results is a necessary prerequisite before taking action on them (merging successful code, collating failure reports).",
              "clarity_critique": "The instructions are clear but lead to a flawed implementation due to the issue raised in the alignment critique. The prompt should be revised to explicitly state that the success or failure of a job must be determined by inspecting the `status` property of the `JobResult` object, not by performing new file system checks."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Now, implement the logic for successful jobs within `processJobResults`. If a job is successful, copy the generated implementation and test files from its workspace to their final destination in the main repository. The source paths are `<workspacePath>/<ModuleName>.ts` and `<workspacePath>/__tests__/<ModuleName>.test.ts`. The destination paths are `<repoDir>/src/modules/<ModuleName>.ts` and `<repoDir>/src/modules/__tests__/<ModuleName>.test.ts`. Use the `fs-extra` library for robust file copying and directory creation. Ensure you log each successful merge.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "processJobResults",
                "workspacePath",
                "ModuleName",
                "<workspacePath>/<ModuleName>.ts",
                "<workspacePath>/__tests__/<ModuleName>.test.ts",
                "repoDir",
                "<repoDir>/src/modules/<ModuleName>.ts",
                "<repoDir>/src/modules/__tests__/<ModuleName>.test.ts"
              ],
              "technology_hints": [
                "fs-extra"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the Task Runner's responsibilities outlined in the Project Constitution (Section 3.2), correctly identifying the source and destination paths for generated code and tests.",
              "sequence_critique": "The step is missing a critical prerequisite check. The Project Constitution's Exit-Code Taxonomy (Appendix A.6) defines exit code 65 for an 'Attempt to overwrite existing src file'. The logic must first check if the destination implementation file already exists before attempting to copy. This check is a crucial part of the sequence and is currently omitted.",
              "clarity_critique": "The instruction is clear about the source/destination paths and the tool to use (`fs-extra`). However, its clarity is undermined by the omission of the file existence check. It should be explicitly stated that the system must verify the destination path is empty or contains only a stub before copying, and how to behave otherwise, to prevent accidental overwrites and align with the defined exit codes."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Next, implement the logic for collating failures. Create a new async function `collateFailureReports` in `src/task-runner/results-aggregator.ts`. This function will take the list of failed module workspace paths and the repository root path. It should read the `final_failure.json` from each failed workspace, combine them into a single array, and write this array to a new file named `.ironclad_failures.json` in the repository root. If there are no failures, this file should not be created.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "collateFailureReports",
                "src/task-runner/results-aggregator.ts",
                "failed module workspace paths",
                "repository root path",
                "final_failure.json",
                ".ironclad_failures.json"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the Task Runner's responsibilities as defined in the architecture document (Section 3.2), which explicitly requires collating individual `final_failure.json` reports into a single top-level `.ironclad_failures.json`. It directly supports the NFRs for Auditability and Reliability (Consistent Failure Reporting).",
              "sequence_critique": "The sequence is logical. This step correctly follows the completion of all jobs and precedes the final determination of the CLI exit code.",
              "clarity_critique": "The instructions are clear, but could be enhanced by explicitly mentioning that the data read from each `final_failure.json` should be parsed as a `FinalFailureReport` and the combined array written to `.ironclad_failures.json` should conform to the `AggregatedFailureReport` type alias. This ensures type-safe implementation and adherence to the project's established data model."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Integrate the new aggregation logic into the main Task Runner entry point. After awaiting all job promises, call `processJobResults`. Then, if there are any failed modules, call `collateFailureReports` with the list of failed workspaces. Ensure the final exit code of the Task Runner will be non-zero if the `.ironclad_failures.json` file was created.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "Task Runner entry point",
                "processJobResults",
                "collateFailureReports",
                "failed workspaces",
                "job promises",
                ".ironclad_failures.json"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of aggregating job results and collating failures, which is a core responsibility of the Task Runner as defined in Section 3.2 of the constitution.",
              "sequence_critique": "A critical step is missing from the sequence. According to the constitution (Section 3.2), after merging successful job outputs, the Task Runner must perform 'global validation' (e.g., full project type-check, integration tests). The current step omits this. The final exit code should be determined *after* both job aggregation and global validation are complete, as either can result in a build failure.",
              "clarity_critique": "The instruction to 'Ensure the final exit code... will be non-zero' is too generic. The constitution (Appendix A.6) defines a specific `Exit-Code Taxonomy`. The step should be more precise, instructing the agent to use the specific exit code for this condition (e.g., code `100` for job failures) and to account for other potential failure codes from the (missing) global validation step."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Create a unit test file for the new logic: `src/task-runner/results-aggregator.test.ts`. Install and use the `mock-fs` library to simulate the file system, including the main repository structure and temporary workspaces.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install and use the `mock-fs` library"
              ],
              "key_entities_dependencies": [
                "src/task-runner/results-aggregator.test.ts",
                "main repository structure",
                "temporary workspaces"
              ],
              "technology_hints": [
                "mock-fs"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Implementing and verifying results aggregation, which is a file-system-intensive operation involving reading from temporary workspaces and writing to the main repository, necessitates robust unit tests. Using `mock-fs` is the correct approach for isolating this logic from the actual disk, which is consistent with the established testing strategy and the `Maintainability (Tooling Consistency)` NFR in the constitution.",
              "sequence_critique": "The step is logically sequenced. Creating a test file, especially one that sets up a complex mocked environment, is a standard and crucial part of the development workflow for the corresponding implementation file.",
              "clarity_critique": "The instructions are clear, specific, and actionable. The file path is explicit, and the directive to use `mock-fs` to simulate both the main repository and temporary workspaces provides sufficient detail for an AI agent to set up the necessary test scaffolding."
            }
          }
        },
        {
          "id": "step_8",
          "description": "Write a unit test for the 'all successful' scenario. In your test, use `mock-fs` to set up a mock repository and several job workspaces. Each workspace should contain mock `ModuleName.ts` and `__tests__/ModuleName.test.ts` files, but NO `final_failure.json`. Run your aggregation logic and assert that all files were copied to the correct locations in `src/modules/` and that `.ironclad_failures.json` was NOT created.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "ModuleName.ts",
                "__tests__/ModuleName.test.ts",
                "final_failure.json",
                "aggregation logic",
                "src/modules/",
                ".ironclad_failures.json"
              ],
              "technology_hints": [
                "mock-fs"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal. Testing the 'all successful' scenario is a crucial part of implementing the results aggregation logic, specifically covering the responsibility of merging successfully generated code into the main repository.",
              "sequence_critique": "The step is logically sequenced. Writing a test for the primary success case ('happy path') first is a standard and effective approach in test-driven development, as it defines the core expected behavior of the component.",
              "clarity_critique": "The step is clear, but could be more precise about the inputs to the function being tested. It should explicitly state that the test should create mock `JobResult` objects (with a 'SUCCESS' status) as the direct input to the aggregation function. The aggregation function would then use the information in these objects (e.g., module name, workspace path) to perform the file copy operations on the mocked file system. This makes the unit test more focused and less reliant on implicit file system discovery."
            }
          }
        },
        {
          "id": "step_9",
          "description": "Write a unit test for the 'all failed' scenario. Use `mock-fs` to set up workspaces, each containing a unique `final_failure.json` file. Run the aggregation logic and assert that no files were copied to `src/modules/` and that `.ironclad_failures.json` was created at the repository root and contains the aggregated content from all `final_failure.json` files.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "final_failure.json",
                "aggregation logic",
                "src/modules/",
                ".ironclad_failures.json"
              ],
              "technology_hints": [
                "mock-fs"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned. It directly tests the Task Runner's failure aggregation responsibility, which is a core requirement for auditability as defined by the `.ironclad_failures.json` and `AggregatedFailureReport` data structures in the constitution.",
              "sequence_critique": "The sequence is logical. It correctly outlines a fundamental test case ('all failed') necessary for building robust result aggregation logic. The internal order of setup, execution, and assertion is standard and correct.",
              "clarity_critique": "The step is exceptionally clear and actionable. It specifies the test scenario, tooling (`mock-fs`), file setup, and the precise assertions needed to verify both the positive outcome (correct failure report) and the negative outcome (no code copied), consistent with the defined behavior."
            }
          }
        },
        {
          "id": "step_10",
          "description": "Write a unit test for a mixed scenario with both successful and failed jobs. Set up mock workspaces accordingly. Run the logic and assert that only the successful module's files were copied and that `.ironclad_failures.json` contains only the reports from the failed modules.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "mock workspaces",
                "successful module's files",
                ".ironclad_failures.json",
                "failed modules"
              ],
              "technology_hints": [
                ".json"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task. Testing a mixed scenario with both successes and failures is a critical validation case for the results aggregation logic, directly verifying the system's ability to handle fault isolation and collate failure reports as required by the project's core NFRs and high-level architecture.",
              "sequence_critique": "The logical sequence within the step (setup, execute, assert) is standard and correct for a test case. It correctly focuses on the outcome of the aggregation logic, which is the final part of the Task Runner's per-module workflow.",
              "clarity_critique": "The step is clear and actionable. It explicitly states the required test scenario (mixed success/failure) and the expected assertions (selective file copy, correct failure report collation). The instructions rely on concepts like 'mock workspaces' and the structure of `.ironclad_failures.json`, which are well-defined in the Project Constitution, making the request unambiguous."
            }
          }
        },
        {
          "id": "step_11",
          "description": "Refactor and add documentation. Add JSDoc comments to the new functions (`processJobResults`, `collateFailureReports`) explaining their purpose, parameters, and return values. Ensure logging is clear and consistent. Clean up any temporary code and ensure all tests pass.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "processJobResults",
                "collateFailureReports"
              ],
              "technology_hints": [
                "JSDoc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal. Refactoring, documenting, and ensuring test coverage are crucial final actions for implementing a core feature like results aggregation, directly supporting the project's non-functional requirements for Maintainability and Reliability.",
              "sequence_critique": "The step is logically sequenced as a final cleanup and quality assurance action for Task 2.15. It correctly assumes that the primary implementation of `processJobResults` and `collateFailureReports` has occurred in previous steps.",
              "clarity_critique": "The step is largely clear. However, the instruction to 'Ensure logging is clear and consistent' could be made more actionable by explicitly requiring adherence to the project's established logging strategy, as detailed in Section 11 of the architecture document and the 'Observability (Structured Logging)' non-functional requirement. This would ensure the logging is not just subjectively clear but constitutionally compliant."
            }
          }
        }
      ],
      "Task 2.16: Implement the final collation step to create the top-level `.ironclad_failures.json` report.": [
        {
          "id": "step_1",
          "description": "Create a new file at `src/utils/report-collator.ts`. In this file, define and export an asynchronous function named `collateFailureReports`. This function should accept two arguments: `failureReportPaths` (an array of strings representing the full paths to individual `final_failure.json` files) and `outputDir` (a string representing the root directory of the repository).",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/utils/report-collator.ts",
                "collateFailureReports",
                "failureReportPaths",
                "final_failure.json",
                "outputDir"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal of creating a `collateFailureReports` function is aligned with the task. However, the proposed file location at `src/utils/report-collator.ts` is inconsistent with the Project Constitution. The constitution explicitly assigns the responsibility of collating failures to the Task Runner component, specifically within the `src/task-runner/results-aggregator.ts` file (derived from Task 2.15). Placing this logic in a generic `utils` directory violates the established component architecture.",
              "sequence_critique": "The step is a logical starting point for the task, as it defines the function signature and file structure before implementation.",
              "clarity_critique": "The step is clear and actionable, with specific instructions for the file path, function name, and function arguments."
            }
          }
        },
        {
          "id": "step_2",
          "description": "Implement the logic for `collateFailureReports`. The function should initialize an empty array to hold the report data. It should then iterate over the `failureReportPaths` array. For each path, use a `try...catch` block to read the file content asynchronously, parse it as JSON, and push the resulting object into your data array. If reading or parsing fails for a specific file, log a warning to the console (e.g., `console.warn`) with the file path and error, then continue to the next file. Use the `fs/promises` module for file operations.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "collateFailureReports",
                "failureReportPaths",
                "console.warn"
              ],
              "technology_hints": [
                "fs/promises",
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task of creating the top-level `.ironclad_failures.json` report. It directly addresses the core logic of collecting and parsing individual `final_failure.json` files from failed jobs, which is a key responsibility of the Task Runner as defined in the Project Constitution (Section 3.2). The fault-tolerant approach of logging a warning and continuing on a parsing error aligns with the project's resilience requirements.",
              "sequence_critique": "The logical sequence described within the step (initialize array, loop, try-catch read/parse, push to array) is correct and standard for an aggregation task. No issues found.",
              "clarity_critique": "The step is clear but could be more precise to ensure consistency with project standards. It should explicitly state that the parsed JSON object must be validated against the `FinalFailureReportSchema (Zod)` to ensure structural correctness, not just syntactic validity. This aligns with the established pattern of using Zod for robust parsing (e.g., `ModuleContractSchema`). Additionally, it should specify that the resulting aggregated data structure is of type `AggregatedFailureReport`."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Finalize the `collateFailureReports` function. After the loop, check if the collected data array is not empty. If it contains one or more report objects, construct the full path for the output file (`.ironclad_failures.json` inside `outputDir` using `path.join`). Then, write the array to this file as a pretty-printed JSON string (use `JSON.stringify(data, null, 2)`). If the data array is empty, the function should do nothing and simply complete.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "collateFailureReports",
                "data",
                ".ironclad_failures.json",
                "outputDir",
                "path.join",
                "JSON.stringify"
              ],
              "technology_hints": [
                "JSON",
                "Node.js"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal of creating the `.ironclad_failures.json` report. The conditional logic to only write the file if failures exist is crucial and correctly implements the system's behavior as defined in the architecture (Section 3.2) and the `Reliability (Consistent Failure Reporting)` non-functional requirement.",
              "sequence_critique": "The logical sequence is correct: check for data, then construct the path, then write the file. This prevents unnecessary file system operations on successful runs and is the most efficient order of operations.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify the exact output filename, the method for path construction (`path.join`), and the precise format for the JSON output (`JSON.stringify(data, null, 2)`), leaving no room for ambiguity for the AI agent."
            }
          }
        },
        {
          "id": "step_4",
          "description": "Create a corresponding unit test file at `src/utils/report-collator.test.ts`. Import the `collateFailureReports` function. You will need to mock the file system and console. Install `mock-fs` if it's not already in the project (`npm install --save-dev mock-fs`) and set up `beforeEach` and `afterEach` blocks to apply and restore the mock file system. Also, mock `console.warn` using `jest.spyOn` to verify that warnings are logged correctly.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [
                "Install `mock-fs` if it's not already in the project (`npm install --save-dev mock-fs`)"
              ],
              "key_entities_dependencies": [
                "src/utils/report-collator.test.ts",
                "collateFailureReports",
                "beforeEach",
                "afterEach",
                "console.warn",
                "jest.spyOn"
              ],
              "technology_hints": [
                "mock-fs",
                "npm",
                "jest"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step correctly identifies the need to test the collation function, which aligns with the task. However, the proposed file path `src/utils/report-collator.test.ts` implies the function under test is in `src/utils/`. This contradicts the Project Constitution (Section 3.2, Task 2.15), which assigns failure collation responsibility to the `Task Runner` component, specifically within a module like `src/task-runner/results-aggregator.ts`. Placing this core logic in a generic `utils` directory violates the established component architecture and the 'Maintainability' NFR.",
              "sequence_critique": "The sequence of creating a test file, installing dependencies if needed, and setting up mocks with `beforeEach`/`afterEach` is logical and follows standard testing practices.",
              "clarity_critique": "The instructions to use `mock-fs` and `jest.spyOn` are clear. However, the clarity is undermined by specifying a file path that is inconsistent with the project's established architecture, potentially causing confusion or architectural drift."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Write a unit test for the success case with multiple failures. In your mock file system, create a temporary directory and place two or three valid `final_failure.json` files inside it. Call `collateFailureReports` with the paths to these mock files. Assert that a `.ironclad_failures.json` file is created in the specified output directory and that its content is a JSON array containing the objects from all the mock files.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "final_failure.json",
                "collateFailureReports",
                ".ironclad_failures.json"
              ],
              "technology_hints": [
                "mock file system",
                "JSON"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal. It directly tests a core responsibility of the Task Runner\u2014collating individual `final_failure.json` reports into the top-level `.ironclad_failures.json`\u2014as defined in Section 3.2 of the architecture document. The data structures involved (`FinalFailureReport`, `AggregatedFailureReport`) are consistent with the constitution.",
              "sequence_critique": "The sequence described (setup mock files, call the function, assert the output) follows the standard and logical Arrange-Act-Assert pattern for unit testing. No issues are identified.",
              "clarity_critique": "The instructions are clear, specific, and actionable. They precisely describe the test scenario, the necessary setup within a mock file system, the function to call, and the expected assertions on both the output file's existence and its content. An AI agent can execute this step without ambiguity."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Write unit tests for edge cases. Create one test where the input `failureReportPaths` array is empty; assert that no output file is written. Create another test with a mix of one valid report file, one path to a non-existent file, and one file containing invalid JSON. Assert that the final `.ironclad_failures.json` contains only the single object from the valid report and that `console.warn` was called twice.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "failureReportPaths",
                ".ironclad_failures.json",
                "console.warn"
              ],
              "technology_hints": [
                "JSON",
                "console.warn"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task and the project's non-functional requirements. Testing edge cases like empty inputs, non-existent files, and corrupted data for the failure collation logic directly supports the constitutional requirements for Reliability and Consistent Failure Reporting. It ensures the system behaves gracefully and predictably, which is crucial for a robust build tool.",
              "sequence_critique": "The step is logically sequenced. It correctly assumes that core functionality has been implemented and now focuses on testing for robustness and edge cases, which is a standard and necessary part of completing a feature.",
              "clarity_critique": "The step is exceptionally clear and actionable. It specifies the exact setup for two distinct test cases (empty input, mixed-validity input) and provides unambiguous assertions for both the final output file content and the expected side effects (warning logs). This level of detail is ideal for an AI coding agent."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Integrate the collation logic into the main `TaskRunner` component. In the primary execution method of the Task Runner, after all `Job` processes have completed, you will have a list of paths to the `final_failure.json` files for all failed jobs. If this list is not empty, invoke the `collateFailureReports` function, passing it the list of paths and the root repository directory path.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "TaskRunner",
                "Job",
                "final_failure.json",
                "collateFailureReports"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's objective and the architectural responsibilities of the `Task Runner` as defined in the Project Constitution (Section 3.2). It directly implements the collation of individual `final_failure.json` reports into the top-level `.ironclad_failures.json` file.",
              "sequence_critique": "The sequence is logical and correct. Placing the collation step after all jobs have completed and making it conditional on the existence of failures is the correct workflow. This ensures the failure report is generated before final global validation and exit code determination, as outlined in the architecture.",
              "clarity_critique": "The step is clear and highly actionable. It specifies the exact component (`TaskRunner`), timing (after job completion), condition (if failures exist), function to call (`collateFailureReports`), and the necessary arguments (list of paths, repository path), leaving no ambiguity for implementation."
            }
          }
        }
      ],
      "Task 2.17: Implement the final global validation step in the Task Runner (e.g., full project `tsc --noEmit`).": [
        {
          "id": "step_1",
          "description": "Create a new file `src/task-runner/global-validator.ts` to encapsulate the global validation logic. Define an exported async function `runGlobalValidation(repoDir: string): Promise<{ success: boolean; output: string; }>` within this new file. This function will eventually execute the global checks.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner/global-validator.ts",
                "runGlobalValidation",
                "repoDir"
              ],
              "technology_hints": [
                "TypeScript"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal and the project architecture. It correctly creates the file `src/task-runner/global-validator.ts` as specified in the constitution's `project_file_map` to house the logic for the final global validation step, which is a key responsibility of the Task Runner.",
              "sequence_critique": "The sequence is logical. Creating the file and defining the function signature is the correct first step for implementing this feature.",
              "clarity_critique": "The proposed function return type `Promise<{ success: boolean; output: string; }>` introduces an ad-hoc, anonymous type that conflicts with the canonical `ValidationResult` interface (`{ success: boolean; errors?: ValidationError[]; }`) defined in the Project Constitution. To maintain architectural consistency and ensure structured, machine-readable outputs, the function signature should be `runGlobalValidation(repoDir: string): Promise<ValidationResult>`. The implementation should be responsible for parsing the raw tool output into the structured `ValidationError` array."
            }
          }
        },
        {
          "id": "step_2",
          "description": "In `src/task-runner/global-validator.ts`, implement the `runGlobalValidation` function. Use Node.js's `child_process.spawn` to execute the TypeScript compiler (`tsc`) on the entire project. The command should be `tsc` with the argument `--noEmit`. Ensure the command is executed with the `cwd` (current working directory) set to `repoDir`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner/global-validator.ts",
                "runGlobalValidation",
                "child_process.spawn",
                "tsc",
                "--noEmit",
                "cwd",
                "repoDir"
              ],
              "technology_hints": [
                "Node.js",
                "child_process.spawn",
                "TypeScript compiler (tsc)"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal and the project's non-functional requirement for 'Post-Merge Validation'. It correctly identifies the command (`tsc --noEmit`), the execution method (`child_process.spawn`), and the target file (`src/task-runner/global-validator.ts`) as defined in the constitution.",
              "sequence_critique": "The step is logically sound as a self-contained action. However, it omits the critical subsequent logic needed within the function: capturing the exit code and output streams (`stdout`/`stderr`) of the spawned process. A complete implementation requires handling the process completion to determine success or failure.",
              "clarity_critique": "The instruction is clear about how to *initiate* the process but is critically incomplete regarding the function's *output*. It should explicitly state that the function must return a `ValidationResult` object (as defined in the constitution) by interpreting the child process's exit code and parsing its `stdout`/`stderr` to create `ValidationError` entries on failure. This omission makes the step not fully actionable for producing a usable result."
            }
          }
        },
        {
          "id": "step_3",
          "description": "Enhance `runGlobalValidation` to handle the output of the `tsc` process. Capture all data from `stdout` and `stderr` into a single string. The promise should resolve when the process exits. The resolved object `{ success: boolean; output: string; }` should have `success` set to `true` if the exit code is 0, and `false` otherwise. The `output` field should contain the captured `stdout` and `stderr`.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runGlobalValidation",
                "success",
                "output"
              ],
              "technology_hints": [
                "tsc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step's goal is well-aligned with the task of running a global validation command. However, the proposed return data structure `{ success: boolean; output: string; }` is an ad-hoc definition that is inconsistent with the project constitution. The constitution already defines a canonical `CommandRunResult` interface (`{ stdout: string; stderr: string; status: number | null; }`) which is purpose-built for capturing the raw output of a CLI command and should be used instead for architectural consistency.",
              "sequence_critique": "The step is logically sound within the context of implementing the `runGlobalValidation` function.",
              "clarity_critique": "The instructions are clear, specific, and actionable."
            }
          }
        },
        {
          "id": "step_4",
          "description": "In the main `TaskRunner` execution logic (e.g., in `src/task-runner/task-runner.ts`), import and call the `runGlobalValidation` function. This call should occur *after* all jobs have completed and successful outputs have been merged, but *before* determining the final exit code.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "TaskRunner",
                "src/task-runner/task-runner.ts",
                "runGlobalValidation",
                "exit code"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task's goal and the 'Reliability (Post-Merge Validation)' requirement in the project constitution. It correctly places the global validation check as a final quality gate for the integrated codebase.",
              "sequence_critique": "The specified sequence is logical and correct: global validation must occur after successful modules are merged and before the final success/failure status of the entire run is determined.",
              "clarity_critique": "The instruction is clear about *what* to call and *where*, but it omits the critical action of handling the function's return value. It should explicitly state that the boolean result (or result object) from `runGlobalValidation` must be captured and used to determine the final success status and exit code of the process."
            }
          }
        },
        {
          "id": "step_5",
          "description": "Integrate the result of `runGlobalValidation` into the Task Runner's final status reporting. Add clear logging messages indicating the start and result of the global validation. If `runGlobalValidation` returns `success: false`, log the captured output and ensure the Task Runner's final exit code is set to `101` as specified in the 'Exit-Code Taxonomy' in the architecture document (if no module generation failures occurred). If module failures also exist, `100` takes precedence.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "runGlobalValidation",
                "Task Runner",
                "success",
                "101",
                "Exit-Code Taxonomy",
                "architecture document",
                "100"
              ],
              "technology_hints": []
            },
            "step_critique": {
              "alignment_critique": "The step is perfectly aligned with the task and the overall project goal. It directly implements a key responsibility of the Task Runner (running global validation) as defined in the architecture document (Section 3.2). The instruction to use exit code `101` and the precedence rule (`100` takes precedence) are consistent with and a logical interpretation of the 'Exit-Code Taxonomy' in Appendix A.6, contributing to the 'Interoperability (CLI Exit Codes)' NFR.",
              "sequence_critique": "The logical sequence is correct. This step naturally occurs after all modules have been processed and merged, and immediately before the Task Runner process terminates with a final exit code.",
              "clarity_critique": "The instructions are exceptionally clear and actionable. They specify the exact exit code to use, the conditions for its use, and a deterministic precedence rule for handling multiple failure types. This level of detail is ideal for an AI coding agent."
            }
          }
        },
        {
          "id": "step_6",
          "description": "Create a new test file `src/task-runner/global-validator.test.ts`. Write unit tests for the `runGlobalValidation` function. Use a mocking library (e.g., Jest's `jest.mock`) to mock the `child_process` module. Create test cases that simulate `tsc` succeeding (exit code 0) and failing (non-zero exit code with error output on stderr), and assert that your function returns the correct `{ success, output }` object in each case.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "src/task-runner/global-validator.test.ts",
                "runGlobalValidation",
                "child_process",
                "{ success, output }",
                "stderr"
              ],
              "technology_hints": [
                "Jest",
                "jest.mock",
                "tsc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is highly aligned with the task of implementing a global validation check, as mandated by the architecture (Section 3.2) and the 'Reliability (Post-Merge Validation)' NFR. However, the proposed return signature `{ success, output }` is an ad-hoc structure. It deviates from the canonical `ValidationResult` interface (`{ success: boolean; errors?: ValidationError[] }`) established for other validation components. For consistency, the function should ideally parse the raw `output` string into an array of `ValidationError` objects to conform to the project's data model.",
              "sequence_critique": "The step is logically sequenced. Writing unit tests is an integral part of implementing a new function, and this step correctly places testing alongside implementation within the same task.",
              "clarity_critique": "The instructions are clear and highly actionable. They specify the exact test file to create, the function to target, the mocking strategy (`child_process`), the necessary test cases (success/failure exit codes), and the expected return structure to assert against."
            }
          }
        },
        {
          "id": "step_7",
          "description": "Finally, review the changes in both the `TaskRunner` and `global-validator` files. Ensure the logic is clear, error handling is robust, and logging provides sufficient context for a user to diagnose a global `tsc` failure. Refactor for clarity and add TSDoc comments where necessary.",
          "qa_info": {
            "resource_analysis": {
              "external_actions": [],
              "key_entities_dependencies": [
                "TaskRunner",
                "global-validator",
                "global tsc failure"
              ],
              "technology_hints": [
                "tsc",
                "TSDoc"
              ]
            },
            "step_critique": {
              "alignment_critique": "The step is well-aligned with the task's goal. Reviewing for clarity, robust error handling, and contextual logging directly implements the project's non-functional requirements for Maintainability, Reliability, and Observability as defined in the constitution.",
              "sequence_critique": "The step is logically positioned as a final quality and documentation pass after the core implementation and testing of the global validator would have been completed. The sequence is correct.",
              "clarity_critique": "The prompt is conceptually clear but could be more actionable for an AI agent. Terms like 'review' and 'ensure the logic is clear' are subjective. The prompt would be improved by specifying concrete checks, such as 'Ensure all child process invocations are wrapped in try/catch blocks' or 'Verify that failure logs include the full command executed and its stderr output, consistent with the Structured Logging NFR'."
            }
          }
        }
      ],
      "Task 2.18: Define and implement the system's exit code taxonomy based on run outcomes.": [],
      "Task 2.19: Develop a mock IMG server for use in automated end-to-end tests.": [],
      "Task 2.20: Write unit and integration tests for the Task Runner and Job components.": []
    },
    "Phase 3: Advanced Validation Engine and Global Integration Checks": {},
    "Phase 4: CLI Tooling, CI/CD Integration, and Production Hardening": {}
  },
  "constitution": {
    "project_name": "Ironclad Code Generation System",
    "core_mission": "To automate the generation of high-quality, validated, and test-covered software modules from formal architectural specifications, ensuring determinism and strict adherence to contracts.",
    "architectural_paradigm": "File-based Pipeline Orchestration",
    "primary_language_and_tech_stack": {
      "language": "TypeScript/Node.js",
      "backend_framework": "N/A (CLI Tooling)",
      "frontend_framework": "N/A (CLI Tooling)",
      "database": "File System"
    },
    "key_data_structures": [
      {
        "name": "ModuleContract",
        "description": "A formal JSON-based definition of a single module's interface, dependencies, and generation instructions."
      },
      {
        "name": "FailureReport",
        "description": "A structured JSON report detailing validation errors or generation issues for a module, aggregated for the entire run."
      },
      {
        "name": "InterfaceStub",
        "description": "A TypeScript interface file (*.ts) generated by the IBA, representing the contract surface of a module for type checking."
      },
      {
        "name": "BlueprintLock",
        "description": "A SHA-256 hash header embedded in all blueprint files to verify their integrity."
      },
      {
        "name": "ModuleInstructions",
        "description": "A structured object within a ModuleContract providing detailed, human-readable instructions for the IMG, including an overview, explicit generation steps, and edge cases to consider."
      },
      {
        "name": "FailureEntry",
        "description": "A detailed record of a single failure attempt during a module generation job's retry loop. It includes the validator that failed (e.g., tsc, dsl), a timestamp, and specific error details."
      },
      {
        "name": "FinalFailureReport",
        "description": "A structured report summarizing the permanent failure of a single module after all retry attempts. It contains the module name, total attempts, a summary of the final error, and the hash of the last prompt sent to the IMG."
      },
      {
        "name": "Parameter",
        "description": "Defines a single parameter for a function or a property for a data structure, consisting of a name and a type string. It is a sub-component of FunctionSignature and DataStructure."
      },
      {
        "name": "FunctionSignature",
        "description": "Defines the signature of a single function within a module's public API, including its name, parameters, return type, and an optional description. It is a key component of the ModuleContract."
      },
      {
        "name": "DataStructure",
        "description": "Defines a custom data structure (e.g., an interface or class) that is part of a module's contract, including its name and properties. It is an optional component of the ModuleContract."
      },
      {
        "name": "ValidatorType",
        "description": "A string literal union type ('json-schema' | 'tsc' | 'dsl' | 'edge-case' | 'test-runner') that enumerates the specific validation stages within the generation pipeline, used within a FailureEntry."
      },
      {
        "name": "ModuleContractSpecification",
        "description": "A detailed definition of the `ModuleContract` structure, explicitly adding the following fields identified in Task 1.2: `purpose` (string), `publicAPI` (string[]), `dependencies` (string[]), and `constructorParams` (string[])."
      },
      {
        "name": "AggregatedFailureReport",
        "description": "A type alias for an array of `FinalFailureReport` objects. This structure represents the complete, structured content of the top-level `.ironclad_failures.json` file, as defined in Task 1.2."
      },
      {
        "name": "ModuleContractSchema (Zod)",
        "description": "A schema object defined in TypeScript code using the `zod` library. It provides robust runtime parsing and type inference for the `ModuleContract` data structure, as introduced in Task 1.4, complementing the static JSON Schema definition."
      },
      {
        "name": "AdjacencyList",
        "description": "A type alias for `Map<string, string[]>` representing the directed edges of the module dependency graph. The key is a module name, and the value is an array of its dependencies. Introduced in Task 1.5 to serve as the core data structure within the `DependencyGraph`."
      },
      {
        "name": "DependencyGraph",
        "description": "A class that encapsulates the `AdjacencyList`. It provides a static `build` method to construct the graph from a map of module contracts and includes validation logic to detect unresolved dependencies. Introduced in Task 1.5."
      },
      {
        "name": "CycleError",
        "description": "A custom Error class that extends the base `Error`. It is thrown by the DAG validator and is designed to contain the specific list of module names (the path) that form a dependency cycle, as introduced in Task 1.6."
      },
      {
        "name": "FileWriteMap",
        "description": "A type alias for `Map<string, string | Buffer>` used by the IBA orchestrator. The key is the absolute destination file path, and the value is the file content to be written. This structure aggregates all generated and copied blueprint files before they are written to disk with a `BlueprintLock`, as introduced in Task 1.10."
      },
      {
        "name": "CommandRunResult",
        "description": "A structured object returned by the `runBlueprintCommand` test helper. It contains the `stdout` (string), `stderr` (string), and exit `status` (number | null) of a CLI command execution, used for integration testing. (Derived from Task 1.12, Step 4)."
      },
      {
        "name": "MockProjectConfig",
        "description": "A configuration object passed to the `createMockProject` test helper. It declaratively defines the files and content to be created in a temporary directory for setting up integration test scenarios. (Derived from Task 1.12, Step 10)."
      },
      {
        "name": "GeneratePromptOptions",
        "description": "An interface defining the input options for the prompt constructor. It includes the workspace directory, the path to the main template, the module name, and an optional path to a failure report for prompt amendment during retries. (Derived from Task 2.4, Step 3)."
      },
      {
        "name": "GeneratePromptResult",
        "description": "An interface defining the output of the prompt constructor. It contains the final, rendered prompt content as a string and its corresponding SHA-256 hash. (Derived from Task 2.4, Step 3)."
      },
      {
        "name": "ConcurrencyManager",
        "description": "A generic, reusable class designed to manage a worker pool for concurrent task execution. Its constructor accepts a concurrency limit, an array of task items, and an asynchronous task processor function. It provides a `run()` method that executes all tasks, respecting the concurrency limit and ensuring fault isolation by processing all tasks even if some fail. (Derived from Task 2.5, Step 2)."
      },
      {
        "name": "PromiseSettledResult<T>",
        "description": "A built-in TypeScript type that represents the outcome of a promise, which can be either fulfilled (`{ status: 'fulfilled', value: T }`) or rejected (`{ status: 'rejected', reason: any }`). It is formally adopted as the key data structure for the return value of the `ConcurrencyManager`, ensuring that the status of every individual task is captured. (Derived from the critique in Task 2.5, Step 2)."
      },
      {
        "name": "JobResult",
        "description": "A structured object representing the final outcome of processing a single module within the Task Runner's `processModule` method. It contains the module's name, a status (e.g., 'SUCCESS', 'FAILURE'), and, in the case of failure, the corresponding `FinalFailureReport`. (Derived from Task 2.5, Step 3)."
      },
      {
        "name": "ImgPromptRequest",
        "description": "An interface defining the request body sent to the IMG API, containing the prompt content. Example: `{ prompt: string }`. (Derived from Task 2.7, Step 2)."
      },
      {
        "name": "ImgSuccessResponse",
        "description": "An interface defining the structure of a successful response from the IMG API, containing the generated implementation and test code. Example: `{ implementationCode: string, testCode: string }`. (Derived from Task 2.7, Step 2)."
      },
      {
        "name": "ImgClientConfig",
        "description": "An interface for the IMG client's configuration object, containing the API endpoint URL and the API key. Example: `{ apiUrl: string, apiKey: string }`. (Derived from Task 2.7, Step 2)."
      },
      {
        "name": "ImgConfigError",
        "description": "A custom Error class that extends the base `Error`. It is thrown by the IMG client's configuration loader when required environment variables (e.g., `IMG_API_KEY`) are missing. (Derived from Task 2.7, Step 3)."
      },
      {
        "name": "ImgApiError",
        "description": "A custom Error class that extends the base `Error`. It is thrown by the IMG client when it receives a non-2xx HTTP response from the API. It is designed to store the HTTP status code and response data for detailed error reporting. (Derived from Task 2.7, Step 3)."
      },
      {
        "name": "IntermediateFailureReport",
        "description": "A type alias for an array of `FailureEntry` objects, representing the contents of the `failures.json` file within a single Job's workspace. It tracks all validation failures for a single module across its retry attempts. (Derived from Task 2.8, Step 3)."
      },
      {
        "name": "FailureEntrySchema (Zod)",
        "description": "A schema object defined in TypeScript code using the `zod` library. It provides runtime parsing and type inference for the `FailureEntry` data structure, used when reading and writing the intermediate `failures.json` file. (Derived from Task 2.8, Step 2)."
      },
      {
        "name": "FinalFailureReportSchema (Zod)",
        "description": "A schema object defined in TypeScript code using the `zod` library. It provides runtime parsing and type inference for the `FinalFailureReport` data structure, used when reading and writing the `final_failure.json` file. (Derived from Task 2.8, Step 2)."
      },
      {
        "name": "ValidationResult",
        "description": "An interface representing the outcome of a single validation engine's run. It contains a `success` boolean and an optional array of `ValidationError` objects. (Derived from Task 2.9, Step 1)."
      },
      {
        "name": "ValidationError",
        "description": "An interface for a structured error produced by a validation engine. It includes an optional `file`, `line`, and `column` number, and a required `message` string. (Derived from Task 2.9, Step 1)."
      },
      {
        "name": "ValidationResult (Conflicting Definition)",
        "description": "A conflicting definition of a validation result proposed in Task 2.10, Step 2: `{ validator: string; message: string; details?: any; }`. This structure is inconsistent with the primary `ValidationResult` interface defined in the constitution and should be reconciled."
      },
      {
        "name": "AjvValidatorResult",
        "description": "An ad-hoc interface representing the return value of the `validateModuleContract` function defined in Task 1.3. It has the shape `{ isValid: boolean; errors: any[] | null }` and should be reconciled with the more specific, canonical `ValidationResult` data structure."
      },
      {
        "name": "ValidationFailure (Simplified)",
        "description": "A simplified interface for a failure record proposed in Task 2.8, with properties like `validator`, `message`, and `details`. This conflicts with the more comprehensive, canonical `FailureEntry` data structure and should be reconciled."
      },
      {
        "name": "JobTask",
        "description": "An interface representing the input for a single job process, containing the `moduleName`, the `workspacePath`, and the path to the `jobScriptPath` to be executed. (Derived from Task 2.14, Step 2)."
      },
      {
        "name": "JobProcessOutput",
        "description": "An interface representing the raw output of a single child process execution, containing the `moduleName`, the `exitCode` (number), and the captured `stdout` and `stderr` strings. This is a low-level data structure used by the Task Runner to determine the final, interpreted JobResult. (Derived from the conflicting `JobResult` definition in Task 2.14, Step 2)."
      },
      {
        "name": "FinalFailureReport (Conflicting Definition)",
        "description": "A conflicting definition of the final failure report proposed in Task 2.13, which includes the full array of `ValidationFailure` objects. This contradicts the canonical definition of `FinalFailureReport` as a concise summary."
      },
      {
        "name": "AggregationCategorizationResult",
        "description": "An interface for the return value of the `processJobResults` function, containing two arrays: `successfulModules` (string[]) and `failedModuleWorkspaces` (string[]). (Derived from Task 2.15, Step 3)."
      },
      {
        "name": "ExitCodeTaxonomy",
        "description": "A TypeScript `enum` or constant object that defines the specific, non-zero exit codes for different categories of CLI failures (e.g., input validation, dependency cycle, job failure, global validation failure), as specified in the architecture document's Appendix A.6 and implemented in `src/core/exit-codes.ts`. (Derived from Task 2.18)."
      }
    ],
    "global_dependencies_and_interfaces": [
      {
        "name": "Ironclad Module Generator (IMG) API",
        "version": "External Service",
        "reason": "The core Large Language Model (LLM) service used for generating code and tests."
      },
      {
        "name": "TypeScript Compiler (tsc)",
        "version": "Project-defined",
        "reason": "Essential for static type checking and ensuring generated code conforms to interface contracts."
      },
      {
        "name": "JavaScript Test Runner (e.g., Jest/Mocha)",
        "version": "Project-defined",
        "reason": "Required for executing generated unit tests as part of the validation sequence."
      },
      {
        "name": "Git",
        "version": "System-provided",
        "reason": "Manages versioning of all specifications, contracts, and generated code."
      },
      {
        "name": "@types/node",
        "version": "Project-defined",
        "reason": "Provides TypeScript type definitions for the Node.js runtime, essential for type safety in a TypeScript/Node.js project."
      },
      {
        "name": "Commander.js",
        "version": "Project-defined",
        "reason": "A library for building the command-line interface, chosen to parse arguments and define the `blueprint` and `generate` commands."
      },
      {
        "name": "ts-node",
        "version": "Project-defined",
        "reason": "A development tool to execute TypeScript files directly, streamlining the development workflow by avoiding a separate compilation step during development."
      },
      {
        "name": "JSON Schema Validator (e.g., ajv)",
        "version": "Project-defined",
        "reason": "Required for validating the structure of `ModuleContract` JSON files against a formal schema, as implied by Task 1.3 and the 'json-schema' validator type."
      },
      {
        "name": "ts-jest",
        "version": "Project-defined",
        "reason": "A TypeScript preprocessor for Jest, required to execute tests written in TypeScript, directly supporting the integration of the chosen tech stack (TypeScript) and test runner (Jest)."
      },
      {
        "name": "@types/jest",
        "version": "Project-defined",
        "reason": "Provides TypeScript type definitions for the Jest testing framework, essential for type safety when writing unit and integration tests."
      },
      {
        "name": "zod",
        "version": "Project-defined",
        "reason": "A TypeScript-first schema declaration and validation library, introduced in Task 1.4 for robust runtime parsing and type inference of ModuleContract data."
      },
      {
        "name": "remark",
        "version": "Project-defined",
        "reason": "A powerful Markdown processor required to parse the `ARCHITECTURE_SPEC.md` file into an Abstract Syntax Tree (AST), as specified in Task 1.4."
      },
      {
        "name": "remark-parse",
        "version": "Project-defined",
        "reason": "A plugin for `remark` that handles the conversion of a Markdown string into a syntax tree, essential for the `ARCHITECTURE_SPEC.md` parser."
      },
      {
        "name": "unist-util-visit",
        "version": "Project-defined",
        "reason": "A utility for traversing the `unist` (Universal Syntax Tree) generated by `remark`, necessary for extracting specific information like the module list from the parsed `ARCHITECTURE_SPEC.md`."
      },
      {
        "name": "Yarn",
        "version": "System-provided",
        "reason": "An alternative package manager to npm, mentioned as an option for installing dependencies in Task 1.4."
      },
      {
        "name": "Prettier",
        "version": "Project-defined",
        "reason": "A code formatter used to ensure all generated TypeScript code (e.g., interface stubs) adheres to a consistent style, supporting the 'high-quality' output goal as identified in the critique of Task 1.7."
      },
      {
        "name": "Node.js Crypto Module",
        "version": "System-provided (Node.js built-in)",
        "reason": "Required for SHA-256 hashing to implement the `BlueprintLock` integrity check mechanism, as introduced in Task 1.8."
      },
      {
        "name": "Jest",
        "version": "Project-defined",
        "reason": "The chosen test runner for unit and integration testing, as implied by the use of `ts-jest` and the repeated specification in test-related tasks (e.g., Task 1.3, 1.4, 1.6)."
      },
      {
        "name": "mock-fs",
        "version": "Project-defined",
        "reason": "A development dependency used to mock the file system during testing of file I/O operations, such as the repository scaffolder in Task 1.9, avoiding actual disk writes."
      },
      {
        "name": "@types/mock-fs",
        "version": "Project-defined",
        "reason": "Provides TypeScript type definitions for the `mock-fs` library, ensuring type safety in tests that mock the file system."
      },
      {
        "name": "npm",
        "version": "System-provided",
        "reason": "The primary package manager for Node.js, used for initializing the project (`npm init`) and managing dependencies (`npm install`) as shown in Task 1.1. The constitution lists Yarn as an alternative, implying npm is a core tool that should be formally listed."
      },
      {
        "name": "tmp-promise",
        "version": "Project-defined",
        "reason": "A library for managing temporary directories and files in Node.js, required for isolated integration testing of file system operations as specified in Task 1.12."
      },
      {
        "name": "fs-extra",
        "version": "Project-defined",
        "reason": "Provides enhanced file system methods (like `pathExists`, `copy`) that simplify file system assertions in integration tests, as required by Task 1.12."
      },
      {
        "name": "chalk",
        "version": "Project-defined",
        "reason": "Required for producing colored console output (e.g., for user-friendly error messages) to improve the CLI's user experience, as specified in Task 2.1, Step 5."
      },
      {
        "name": "Node.js child_process Module",
        "version": "System-provided (Node.js built-in)",
        "reason": "Required for executing the main CLI entry point as a subprocess during integration testing, as specified in the test helper function in Task 1.12, Step 4."
      },
      {
        "name": "Node.js fs/promises Module",
        "version": "System-provided (Node.js built-in)",
        "reason": "Required for performing asynchronous file system operations, such as reading directories to discover module contracts, as specified in Task 2.2, Step 3."
      },
      {
        "name": "Node.js path Module",
        "version": "System-provided (Node.js built-in)",
        "reason": "Required for robust handling and construction of file paths across different operating systems, essential for both the `generate` and `blueprint` commands, as specified in Task 2.2, Step 3."
      },
      {
        "name": "handlebars",
        "version": "Project-defined",
        "reason": "Required for populating prompt templates with dynamic data using logic-less templates, as specified by the template syntax in the architecture document and the implementation in Task 2.4."
      },
      {
        "name": "axios",
        "version": "Project-defined",
        "reason": "Required for making HTTP POST requests to the external Ironclad Module Generator (IMG) API, as specified in Task 2.7."
      },
      {
        "name": "dotenv",
        "version": "Project-defined",
        "reason": "Required for loading environment variables from a `.env` file during local development, specifically for securely handling the `IMG_API_KEY` as specified in Task 2.7."
      },
      {
        "name": "nock",
        "version": "Project-defined",
        "reason": "A development dependency required for mocking HTTP requests to the external IMG API during integration testing, ensuring tests are isolated and deterministic, as specified in Task 2.7."
      },
      {
        "name": "@types/nock",
        "version": "Project-defined",
        "reason": "Provides TypeScript type definitions for the `nock` library, ensuring type safety in tests that mock the IMG API."
      },
      {
        "name": "ESLint",
        "version": "Project-defined",
        "reason": "A linter for TypeScript code, required to enforce coding standards and catch common errors, as mentioned in Task 2.10, Step 8."
      },
      {
        "name": "p-limit",
        "version": "Project-defined",
        "reason": "A library suggested in Task 2.5, Step 5 as an alternative for implementing the concurrency management pattern in the Task Runner, handling a pool of promises efficiently."
      },
      {
        "name": "Node.js os Module",
        "version": "System-provided (Node.js built-in)",
        "reason": "Required for the Task Runner to determine the number of CPU cores as a potential default for the concurrency limit, as suggested in Task 2.14, Step 4."
      },
      {
        "name": "express",
        "version": "Project-defined",
        "reason": "A web framework required for creating a mock IMG server for use in automated end-to-end tests, isolating tests from the real external API. (Derived from Task 2.19)."
      }
    ],
    "non_functional_requirements": [
      {
        "requirement": "Reliability (Determinism)",
        "constraint": "Given the same inputs (specs, contracts, context, templates, IMG version), the system must produce byte-for-byte identical outputs."
      },
      {
        "requirement": "Reliability (Fault Isolation)",
        "constraint": "Failure in one module's generation job must not halt the processing of other independent modules."
      },
      {
        "requirement": "Security",
        "constraint": "The external IMG API key must be handled securely via environment variables or a secrets manager, never stored in version control."
      },
      {
        "requirement": "Auditability",
        "constraint": "The BlueprintLock integrity hash on all blueprint files must be verifiable to prevent tampering."
      },
      {
        "requirement": "Scalability",
        "constraint": "The system must support parallel generation of independent modules, configurable via the IRONCLAD_MAX_PARALLEL environment variable."
      },
      {
        "requirement": "Maintainability",
        "constraint": "All system components must have clear responsibilities and well-defined interfaces to minimize coupling."
      },
      {
        "requirement": "Auditability (Prompt Integrity)",
        "constraint": "The SHA-256 hash of the final prompt sent to the IMG for any failed generation job must be recorded in the failure report for debugging, reproducibility, and auditing purposes."
      },
      {
        "requirement": "Reliability (Input Integrity)",
        "constraint": "The system must perform strict validation on primary input files like `ARCHITECTURE_SPEC.md`. A failure to find required structural elements (e.g., a 'Modules' section) must be treated as a fatal error that halts the entire process, as implied by the critique in Task 1.4."
      },
      {
        "requirement": "Reliability (Fail-Fast)",
        "constraint": "Critical input validation failures, such as a missing 'Modules' section in ARCHITECTURE_SPEC.md or unresolved module dependencies, must cause the process to terminate immediately and report a fatal error, as implied in Tasks 1.4 and 1.5."
      },
      {
        "requirement": "Interoperability (CLI Exit Codes)",
        "constraint": "The CLI must terminate with a non-zero exit code upon encountering a critical blueprint failure, such as a dependency cycle, to facilitate integration with scripting and CI/CD pipelines, as specified in Task 1.6."
      },
      {
        "requirement": "API Contract (Asynchronicity)",
        "constraint": "All methods generated in module interface stubs (e.g., I<ModuleName>.ts) must return a Promise, ensuring a consistent asynchronous API contract across all modules, as specified in the interface generation logic of Task 1.7."
      },
      {
        "requirement": "Reliability (Specific Error Handling)",
        "constraint": "In case of validation or parsing failures (e.g., file-not-found, invalid JSON, unresolved dependency), the system must throw specific, informative errors that clearly identify the source and nature of the problem, as required in the implementation details of Tasks 1.4 and 1.5."
      },
      {
        "requirement": "Maintainability (Code Style Consistency)",
        "constraint": "All generated TypeScript source code must be formatted by an automated tool (e.g., Prettier) to ensure a consistent style, enhancing readability and directly supporting the 'high-quality' output goal mentioned in the critique of Task 1.7."
      },
      {
        "requirement": "Reliability (Idempotency)",
        "constraint": "Core scaffolding operations, such as the directory creation performed by the `scaffoldRepository` function, must be idempotent. The system should not throw an error or fail if it is run multiple times on a target that is already partially or fully scaffolded, as required by the test case in Task 1.9."
      },
      {
        "requirement": "Reliability (BlueprintLock Verifiability)",
        "constraint": "The hashing algorithm used for `BlueprintLock` must be self-consistent. The process of verifying a file with a lock (`verifyBlueprintLock`) must succeed if the input is the direct output of the lock-adding process (`addBlueprintLock`), ensuring `verify(add(content))` is always true. The verification hash must be calculated on the full file content, including the header containing the expected hash, as implied by the critique in Task 1.8."
      },
      {
        "requirement": "Interoperability (Specific Exit Codes)",
        "constraint": "The CLI must use a taxonomy of specific, non-zero exit codes for different categories of failures (e.g., input validation error, dependency cycle, file system error). A generic exit code of '1' is insufficient and deviates from the more sophisticated requirements for CI/CD integration, as noted in the critique of Task 1.10."
      },
      {
        "requirement": "Testability (Code Coverage)",
        "constraint": "The testing framework must be configured to collect and report code coverage metrics. This is required to ensure a high level of testability and adherence to the project's quality standards, as implied by the critique of the basic Jest configuration in Task 1.11, which noted the omission of coverage settings."
      },
      {
        "requirement": "Testability (Test Isolation)",
        "constraint": "Integration tests that perform file system I/O must execute in isolated, temporary directories that are created before the test and cleaned up after, to prevent side effects between tests. (Derived from Task 1.12, Step 3)."
      },
      {
        "requirement": "Reliability (BlueprintLock Verification Logic)",
        "constraint": "The `verifyBlueprintLock` function must operate by: 1. Extracting the hash from the header line. 2. Stripping the header line from the file content. 3. Calculating the SHA-256 hash of the *remaining* content. 4. Comparing the extracted hash with the calculated hash. This clarifies the verification process and corrects the flawed premise of hashing the entire file including its own header. (Derived from the critique in Task 1.12, Step 7)."
      },
      {
        "requirement": "User Experience (CLI Output)",
        "constraint": "The CLI must use colored output (e.g., via `chalk`) to distinguish between informational messages, warnings, and errors, improving readability and user experience. (Derived from Task 2.1, Step 5)."
      },
      {
        "requirement": "Reliability (Configuration Graceful Degradation)",
        "constraint": "When parsing configuration values like environment variables (e.g., `IRONCLAD_MAX_PARALLEL`), the system must handle invalid or out-of-range inputs gracefully by logging a warning and falling back to a safe, documented default value, rather than failing. (Derived from Task 2.1, Step 6)."
      },
      {
        "requirement": "Reliability (Input Path Validation)",
        "constraint": "The `generate run` command must perform strict validation on its `<repository-path>` argument, ensuring the path exists and is a directory before proceeding. Failure must result in a user-friendly error and a specific non-zero exit code as per the system's exit code taxonomy. (Derived from Task 2.1, Step 5)."
      },
      {
        "requirement": "Developer Experience (Workflow Efficiency)",
        "constraint": "The development setup should support rapid iteration by avoiding a full, blocking recompilation step on every code change, for instance by utilizing a tool like `ts-node` for a 'dev' script. (Derived from the critique in Task 1.1, Step 9)."
      },
      {
        "requirement": "Reliability (Verification Robustness)",
        "constraint": "Verification logic, such as for the `BlueprintLock`, must be tested against negative scenarios including tampered content and content missing the expected markers, not just positive round-trip scenarios. (Derived from the critique in Task 1.8, Step 5)."
      },
      {
        "requirement": "Maintainability (Tooling Consistency)",
        "constraint": "The project must maintain consistency in its choice of libraries for common tasks (e.g., using `mock-fs` for all file system mocking) to avoid architectural drift and reduce cognitive overhead for developers. (Derived from the critique in Task 2.2, Step 8)."
      },
      {
        "requirement": "Reliability (Prompt Determinism and Parsability)",
        "constraint": "When consolidating multiple source files (e.g., IDL files) into a single string for an IMG prompt, each consolidated file's content must be preceded by a deterministic, machine-parsable header (e.g., `// --- File: <filename> ---`) to provide clear context to the LLM and ensure byte-for-byte identical prompts for the same inputs. (Derived from the critique in Task 2.4, Step 5)."
      },
      {
        "requirement": "Observability (Structured Logging)",
        "constraint": "The system must support structured (JSON) logging in addition to human-readable console output. This is required for machine-readability and integration with log analysis tools, fulfilling the observability requirements of the system. (Derived from the critique in Task 2.5, Step 4)."
      },
      {
        "requirement": "Reliability (Job Outcome Atomicity)",
        "constraint": "The Job worker process must be designed to be atomic in its outcome reporting. In the event of any failure, including unexpected crashes or exceptions, it must make a best effort to write a `final_failure.json` file to its workspace before terminating. A Job process must not terminate without producing either the expected generated files on success or a failure report on failure. (Derived from critique in Task 2.6, Step 8)."
      },
      {
        "requirement": "Reliability (Test Coverage Enforcement)",
        "constraint": "The test runner validation engine must be configured to fail if no tests are found in a generated test file (e.g., by setting `passWithNoTests: false` in Jest). This ensures that a failure by the IMG to produce any tests is caught as a validation error. (Derived from the critique in Task 2.10, Step 4)."
      },
      {
        "requirement": "Reliability (Overwrite Protection)",
        "constraint": "The system must check if a destination file (e.g., `src/modules/<ModuleName>.ts`) exists and is not a stub before copying/merging generated code. An attempt to overwrite a non-stub, user-modified file must result in a specific failure, as implied by the exit code taxonomy. (Derived from critique in Task 2.15, Step 4)."
      },
      {
        "requirement": "Reliability (Post-Merge Validation)",
        "constraint": "After merging all successful module outputs, the Task Runner must perform a final, global validation step (e.g., a full project `tsc --noEmit`) to ensure the integrated system is valid before declaring final success for the entire run. (Derived from critique in Task 2.15, Step 6)."
      },
      {
        "requirement": "Reliability (Signal Handling)",
        "constraint": "The Task Runner must be able to correctly handle and report on Job processes that terminate unexpectedly due to a system signal (e.g., SIGKILL, SIGTERM), not just those that exit with a non-zero code. Such terminations must be treated as job failures. (Derived from the critique in Task 2.14, Step 8)."
      }
    ],
    "project_file_map": {
      "contracts": {
        "type": "directory",
        "description": "Contains all module contract JSON files.",
        "children": {
          "<ModuleName>.json": {
            "type": "file",
            "description": "The formal contract for a single module."
          }
        }
      },
      "idl": {
        "type": "directory",
        "description": "Contains shared Interface Definition Language files (e.g., TypeScript types).",
        "children": {
          "*.ts": {
            "type": "file",
            "description": "Shared data structures (types, enums, interfaces)."
          },
          "index.ts": {
            "type": "file",
            "description": "A barrel file that exports all shared types from the `idl` directory, simplifying import statements in generated code, as assumed in Task 1.7."
          }
        }
      },
      "prompt_templates": {
        "type": "directory",
        "description": "Contains templates for constructing prompts sent to the IMG.",
        "children": {
          "*.tmpl": {
            "type": "file",
            "description": "A single prompt template."
          }
        }
      },
      "src": {
        "type": "directory",
        "description": "Contains all generated source code.",
        "children": {
          "modules": {
            "type": "directory",
            "description": "Contains module interface stubs and generated implementations.",
            "children": {
              "__tests__": {
                "type": "directory",
                "description": "Contains generated unit tests for modules.",
                "children": {
                  "<ModuleName>.test.ts": {
                    "type": "file",
                    "description": "Generated unit tests for a module."
                  }
                }
              },
              "I<ModuleName>.ts": {
                "type": "file",
                "description": "The TypeScript interface stub for a module, generated by the IBA."
              },
              "<ModuleName>.ts": {
                "type": "file",
                "description": "The generated implementation code for a module."
              }
            }
          },
          "index.ts": {
            "type": "file",
            "description": "The primary CLI entry point for the application, responsible for parsing commands and arguments using the Commander.js library."
          },
          "core": {
            "type": "directory",
            "description": "Contains core type definitions and non-generated logic for the Ironclad tool itself.",
            "children": {
              "types.ts": {
                "type": "file",
                "description": "Defines the core TypeScript types and interfaces for the system, such as ModuleContract and FailureReport variants."
              },
              "job.ts": {
                "type": "file",
                "description": "Contains the entry point and logic for a single module generation job, which is executed as a separate process by the Task Runner. (Derived from Task 2.6, Step 2)."
              },
              "job.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the Job worker script, mocking its external dependencies like the file system and process arguments. (Derived from Task 2.6, Step 9)."
              },
              "exit-codes.ts": {
                "type": "file",
                "description": "Defines the exit code enum/constants and provides a utility function to map error types to specific exit codes, implementing the taxonomy from the architecture specification. (Derived from Task 2.18)."
              },
              "exit-codes.test.ts": {
                "type": "file",
                "description": "Unit tests for the exit code mapping logic. (Derived from Task 2.18)."
              }
            }
          },
          "iba": {
            "type": "directory",
            "description": "Contains source code for the Ironclad Blueprint Architect (IBA) component.",
            "children": {
              "validators": {
                "type": "directory",
                "description": "Contains validation logic for contracts and other blueprint artifacts.",
                "children": {
                  "contractValidator.ts": {
                    "type": "file",
                    "description": "Implements the JSON schema validation logic for ModuleContract files."
                  },
                  "contractValidator.test.ts": {
                    "type": "file",
                    "description": "Unit tests for the ModuleContract validator."
                  },
                  "graph-validator.ts": {
                    "type": "file",
                    "description": "Implements the Directed Acyclic Graph (DAG) validation logic, including cycle detection, for the module dependency graph, as introduced in Task 1.6."
                  },
                  "graph-validator.test.ts": {
                    "type": "file",
                    "description": "Unit tests for the DAG validation logic, covering various cycle scenarios and valid graph structures, as introduced in Task 1.6."
                  }
                }
              },
              "parsers.ts": {
                "type": "file",
                "description": "Implements file parsing logic for `ARCHITECTURE_SPEC.md` and `ModuleContract` JSON files using libraries like remark and zod."
              },
              "parsers.test.ts": {
                "type": "file",
                "description": "Unit tests for the file parsers implemented in `parsers.ts`."
              },
              "types.ts": {
                "type": "file",
                "description": "A file proposed in Task 1.4 to house type definitions and Zod schemas for the IBA. Note: This placement conflicts with the constitution's definition of `src/core/types.ts` for core types and `src/schemas` for schema definitions."
              },
              "dependency-graph.ts": {
                "type": "file",
                "description": "Implements the DependencyGraph class and related logic for constructing and validating the module dependency graph, as specified in Task 1.5."
              },
              "dependency-graph.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the DependencyGraph logic, as specified in Task 1.5."
              },
              "index.ts": {
                "type": "file",
                "description": "The primary orchestrator file for the IBA component, responsible for sequencing the parsing, validation, and file generation steps, as implied by Task 1.6."
              },
              "interface-generator.ts": {
                "type": "file",
                "description": "Contains the logic for generating TypeScript interface stubs from ModuleContract data, as introduced in Task 1.7."
              },
              "interface-generator.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the interface stub generation logic, as introduced in Task 1.7."
              },
              "scaffolder.ts": {
                "type": "file",
                "description": "Contains the logic for creating the initial repository directory structure (e.g., `contracts`, `src/modules`), as specified in Task 1.9."
              },
              "scaffolder.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the repository scaffolding logic, using `mock-fs` to validate directory creation, as specified in Task 1.9."
              },
              "graph.ts": {
                "type": "file",
                "description": "A new file proposed in Task 1.11 to refactor and consolidate dependency graph construction and DAG cycle detection logic for improved testability. Note: This conflicts with the existing, more granular files `dependency-graph.ts` and `validators/graph-validator.ts`."
              },
              "graph.test.ts": {
                "type": "file",
                "description": "A new unit test file proposed in Task 1.11 to test the consolidated graph logic from `graph.ts`."
              },
              "validation.ts": {
                "type": "file",
                "description": "A new file proposed in Task 1.11 to refactor and consolidate `ModuleContract` validation logic. Note: This conflicts with the existing `validators/contractValidator.ts`."
              },
              "validation.test.ts": {
                "type": "file",
                "description": "A new unit test file proposed in Task 1.11 to test the consolidated validation logic from `validation.ts`."
              },
              "graph-validator.ts": {
                "type": "file",
                "description": "A file proposed in Task 1.6 to house dependency graph validation logic. Note: This path conflicts with the established `src/iba/validators/graph-validator.ts` and represents an architectural inconsistency."
              },
              "graph-validator.test.ts": {
                "type": "file",
                "description": "A test file proposed in Task 1.6 for the `graph-validator.ts` logic. Note: This path conflicts with the established test location for graph validation."
              }
            }
          },
          "schemas": {
            "type": "directory",
            "description": "Contains formal schema definitions used for validation.",
            "children": {
              "moduleContract.schema.ts": {
                "type": "file",
                "description": "Defines the JSON Schema for the ModuleContract data structure."
              }
            }
          },
          "utils": {
            "type": "directory",
            "description": "A new directory proposed in Task 1.8 to house general utility functions. Note: The constitution designates `src/core` for non-generated tool logic, and this new path represents a potential architectural conflict or refinement.",
            "children": {
              "blueprintLock.ts": {
                "type": "file",
                "description": "Contains the hashing and verification logic for the BlueprintLock mechanism, as specified in Task 1.8."
              },
              "blueprintLock.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the BlueprintLock hashing and verification logic, as specified in Task 1.8."
              }
            }
          },
          "cli.ts": {
            "type": "file",
            "description": "A new CLI entry point proposed in Task 1.10, intended to replace or supplement `src/index.ts`. It is responsible for instantiating the main `commander` program."
          },
          "commands": {
            "type": "directory",
            "description": "A new directory proposed in Task 1.10 to modularize CLI command logic, with each file defining a subcommand.",
            "children": {
              "blueprint.ts": {
                "type": "file",
                "description": "Contains the logic for the `blueprint` command and its `build` subcommand, including argument/option parsing and orchestrating the IBA workflow, as defined in Task 1.10."
              },
              "__tests__": {
                "type": "directory",
                "description": "Contains integration tests for the CLI commands, co-located with the command source code.",
                "children": {
                  "blueprint.integration.test.ts": {
                    "type": "file",
                    "description": "Contains integration tests for the `ironclad blueprint build` command, as specified in Task 1.12."
                  }
                }
              },
              "generate.ts": {
                "type": "file",
                "description": "Contains the logic for the `generate` command and its `run` subcommand, as specified in Task 2.1."
              },
              "generate.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the helper functions within `generate.ts`, as specified in Task 2.1. Note: The constitution's testing pattern suggests a path like `src/commands/__tests__/generate.unit.test.ts` for better organization."
              }
            }
          },
          "task-runner": {
            "type": "directory",
            "description": "A new directory to house the logic for the Task Runner component, as introduced in Task 2.2.",
            "children": {
              "module-discovery.ts": {
                "type": "file",
                "description": "Contains the logic for discovering module contracts, as specified in Task 2.2."
              },
              "module-discovery.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the module discovery logic, as specified in Task 2.2."
              },
              "workspace-manager.ts": {
                "type": "file",
                "description": "Contains the logic for the Task Runner's workspace manager, including creating and populating isolated job directories, as specified in Task 2.3."
              },
              "workspace-manager.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the workspace manager logic, using `mock-fs` to validate directory and symlink creation, as specified in Task 2.3."
              },
              "prompt-constructor.ts": {
                "type": "file",
                "description": "Contains the logic for constructing prompts from templates and context files, as specified in Task 2.4."
              },
              "prompt-constructor.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the prompt constructor logic, as specified in Task 2.4."
              },
              "job-runner.ts": {
                "type": "file",
                "description": "A module responsible for managing the lifecycle of all module generation jobs, including spawning and monitoring child processes. (Derived from Task 2.14, Step 1)."
              },
              "job-runner.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the `job-runner.ts` module, mocking the `child_process` module. (Derived from Task 2.14, Step 1)."
              },
              "results-aggregator.ts": {
                "type": "file",
                "description": "Contains the logic for processing the array of JobResults, determining success/failure, merging successful code, and collating failures. (Derived from Task 2.15, Step 2)."
              },
              "results-aggregator.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the results aggregation logic, using mock-fs to simulate job workspaces. (Derived from Task 2.15, Step 7)."
              },
              "global-validator.ts": {
                "type": "file",
                "description": "Contains logic for running global validation checks (e.g., a full project `tsc --noEmit`) after all successful modules have been merged into the main repository. (Derived from Task 2.17)."
              },
              "global-validator.test.ts": {
                "type": "file",
                "description": "Unit tests for the global validator logic. (Derived from Task 2.17)."
              },
              "task-runner.integration.test.ts": {
                "type": "file",
                "description": "High-level integration tests for the Task Runner, verifying the end-to-end flow from module discovery to results aggregation, using mocked Job processes. (Derived from Task 2.20)."
              }
            }
          },
          "types": {
            "type": "directory",
            "description": "A new directory proposed in Task 2.3 for type definitions, which conflicts with the established `src/core/types.ts` location.",
            "children": {
              "contract.ts": {
                "type": "file",
                "description": "A file proposed in Task 2.3 to define the `ModuleContract` interface. Note: This path conflicts with the constitution's established `src/core/types.ts` for core type definitions."
              }
            }
          },
          "lib": {
            "type": "directory",
            "description": "A new directory for generic, reusable library code that is not specific to a single component like IBA or Task Runner, as proposed in Task 2.5. Note: This represents a new architectural pattern not previously defined in the constitution.",
            "children": {
              "ConcurrencyManager.ts": {
                "type": "file",
                "description": "Contains the implementation of the generic `ConcurrencyManager` class for handling the worker pool and concurrent task execution. (Derived from Task 2.5, Step 2)."
              },
              "ConcurrencyManager.test.ts": {
                "type": "file",
                "description": "Contains unit tests for the `ConcurrencyManager` class, verifying throttling, parallelism, and fault isolation. (Derived from Task 2.5, Step 5)."
              }
            }
          },
          "img": {
            "type": "directory",
            "description": "A new directory to house all components related to the Ironclad Module Generator (IMG) client, as specified in Task 2.7.",
            "children": {
              "index.ts": {
                "type": "file",
                "description": "A barrel file that exports the public interface of the IMG client module, including the `callImg` function and relevant types. (Derived from Task 2.7, Step 11)."
              },
              "img.client.ts": {
                "type": "file",
                "description": "Contains the main IMG client logic, including the `callImg` function for making API requests. (Derived from Task 2.7, Step 5)."
              },
              "img.client.test.ts": {
                "type": "file",
                "description": "Contains unit and integration tests for the IMG client, using `nock` to mock API responses. (Derived from Task 2.7, Step 7)."
              },
              "img.config.ts": {
                "type": "file",
                "description": "Contains the logic for loading and validating the IMG client's configuration from environment variables. (Derived from Task 2.7, Step 4)."
              },
              "img.errors.ts": {
                "type": "file",
                "description": "Defines custom error classes (`ImgConfigError`, `ImgApiError`) for the IMG client. (Derived from Task 2.7, Step 3)."
              },
              "img.types.ts": {
                "type": "file",
                "description": "Defines the TypeScript interfaces for the IMG client's request, response, and configuration data structures. (Derived from Task 2.7, Step 2)."
              }
            }
          },
          "validation": {
            "type": "directory",
            "description": "A new directory for housing validation engines that are run by the Job worker process. (Derived from Task 2.9, Step 1).",
            "children": {
              "engines": {
                "type": "directory",
                "description": "Contains the implementation for each specific validation engine. (Derived from Task 2.9, Step 1).",
                "children": {
                  "tscValidator.ts": {
                    "type": "file",
                    "description": "Implements the validation engine that runs the TypeScript compiler (`tsc`) against the generated code in an isolated job workspace. (Derived from Task 2.9, Step 1)."
                  },
                  "tscValidator.test.ts": {
                    "type": "file",
                    "description": "A unit test file for the `tscValidator` engine, as specified in Task 2.9, Step 5."
                  }
                }
              },
              "types.ts": {
                "type": "file",
                "description": "Defines shared TypeScript types for the validation components, such as `ValidationResult` and `ValidationError`. (Derived from Task 2.9, Step 1)."
              },
              "__tests__": {
                "type": "directory",
                "description": "A new directory proposed in Task 2.10, Step 7 for housing tests for validation engines, which conflicts with the co-location pattern seen elsewhere in the project.",
                "children": {
                  "testRunnerEngine.test.ts": {
                    "type": "file",
                    "description": "Contains unit tests for the test runner validation engine, as specified in Task 2.10, Step 7."
                  }
                }
              },
              "testRunnerEngine.ts": {
                "type": "file",
                "description": "Implements the test runner validation engine using a programmatic API for a test framework like Jest, as specified in Task 2.10, Step 3."
              },
              "edgeCaseValidator.ts": {
                "type": "file",
                "description": "A file proposed in Task 2.11 to contain the logic for the edge case string check validator. Note: This path conflicts with the established pattern of placing engines in `src/validation/engines/`."
              },
              "edgeCaseValidator.test.ts": {
                "type": "file",
                "description": "A unit test file proposed in Task 2.11 for the edge case validator. Note: This path conflicts with the established test co-location pattern."
              }
            }
          },
          "prompt-constructor.ts": {
            "type": "file",
            "description": "A file proposed in Task 2.4 to encapsulate prompt generation logic. Note: This top-level placement conflicts with the established `src/task-runner/prompt-constructor.ts` path, which co-locates it with its parent component."
          },
          "prompt-constructor.test.ts": {
            "type": "file",
            "description": "A test file proposed in Task 2.4 for the prompt constructor. Note: This path conflicts with the established test location for the Task Runner's prompt constructor."
          },
          "job": {
            "type": "directory",
            "description": "A new directory proposed in the reasoning tree to contain Job-specific logic, which conflicts with the constitutional pattern of placing Job logic in `src/core/job.ts`. (Derived from Tasks 2.12 and 2.13).",
            "children": {
              "failure-types.ts": {
                "type": "file",
                "description": "A file proposed to contain failure-related data structures, which conflicts with the canonical definitions in `src/core/types.ts`. (Derived from Task 2.13, Step 1)."
              },
              "FailureReporter.ts": {
                "type": "file",
                "description": "A class proposed to be responsible for all file-based failure logging within a Job's workspace. (Derived from Task 2.13, Step 2)."
              },
              "FailureReporter.test.ts": {
                "type": "file",
                "description": "Unit tests for the `FailureReporter` class. (Derived from Task 2.13, Step 5)."
              },
              "validation": {
                "type": "directory",
                "description": "A directory proposed to house validation logic specific to the Job, which conflicts with the established top-level `src/validation/` directory. (Derived from Task 2.12, Step 1).",
                "children": {
                  "types.ts": {
                    "type": "file",
                    "description": "A file proposed to hold validation-related types, conflicting with the established `src/validation/types.ts`. (Derived from Task 2.12, Step 1)."
                  },
                  "orchestrator.ts": {
                    "type": "file",
                    "description": "Contains the logic for the validation sequence orchestrator. (Derived from Task 2.12, Step 3)."
                  },
                  "orchestrator.test.ts": {
                    "type": "file",
                    "description": "Unit tests for the validation orchestrator. (Derived from Task 2.12, Step 11)."
                  },
                  "schemaValidator.ts": {
                    "type": "file",
                    "description": "Stub file for the schema validator. (Derived from Task 2.12, Step 4)."
                  },
                  "tscValidator.ts": {
                    "type": "file",
                    "description": "Stub file for the TSC validator. (Derived from Task 2.12, Step 4)."
                  },
                  "dslValidator.ts": {
                    "type": "file",
                    "description": "Stub file for the DSL validator. (Derived from Task 2.12, Step 4)."
                  },
                  "edgeCaseValidator.ts": {
                    "type": "file",
                    "description": "Stub file for the edge case validator. (Derived from Task 2.12, Step 4)."
                  },
                  "testRunnerValidator.ts": {
                    "type": "file",
                    "description": "Stub file for the test runner validator. (Derived from Task 2.12, Step 4)."
                  }
                }
              }
            }
          }
        }
      },
      ".tmp": {
        "type": "directory",
        "description": "Contains temporary files and workspaces for the generation process.",
        "children": {
          "ironclad_tasks": {
            "type": "directory",
            "description": "Contains isolated workspaces for each module generation Job.",
            "children": {
              "<ModuleName>": {
                "type": "directory",
                "description": "The isolated workspace for a single module generation Job. It is populated by the Task Runner with symlinks to necessary blueprint files and serves as the current working directory for the Job process. (Derived from Task 2.3).",
                "children": {
                  "contracts": {
                    "type": "directory",
                    "description": "Contains symlinks to the primary module's contract and all of its dependency contracts. (Derived from Task 2.3)."
                  },
                  "idl": {
                    "type": "directory",
                    "description": "A symlink to the main repository's `idl` directory. (Derived from Task 2.3)."
                  },
                  "prompt_templates": {
                    "type": "directory",
                    "description": "A symlink to the main repository's `prompt_templates` directory. (Derived from Task 2.3)."
                  },
                  "src": {
                    "type": "directory",
                    "description": "Contains symlinks to interface stubs and will be the destination for generated code. (Derived from Task 2.3).",
                    "children": {
                      "modules": {
                        "type": "directory",
                        "description": "Contains symlinks to the primary module's interface stub and all of its dependency stubs. (Derived from Task 2.3).",
                        "children": {
                          "__tests__": {
                            "type": "directory",
                            "description": "A directory created within the job workspace to hold the generated test files. (Derived from critique in Task 2.3, Step 3).",
                            "children": {
                              "<ModuleName>.test.ts": {
                                "type": "file",
                                "description": "The generated unit test file for the module, written by the Job worker process based on the IMG response. It is a primary artifact to be validated. (Derived from Task 2.12, Step 9)."
                              }
                            }
                          },
                          "<ModuleName>.ts": {
                            "type": "file",
                            "description": "The generated implementation file for the module, written by the Job worker process based on the IMG response. It is a primary artifact to be validated. (Derived from Task 2.12, Step 8)."
                          }
                        }
                      }
                    }
                  },
                  "prompt.txt": {
                    "type": "file",
                    "description": "The fully constructed prompt for the IMG, generated by the Task Runner and read by the Job worker. (Derived from Task 2.4)."
                  },
                  "prompt.hash": {
                    "type": "file",
                    "description": "The SHA-256 hash of `prompt.txt`, used by the Job worker to verify integrity. (Derived from Task 2.4)."
                  },
                  "failures.json": {
                    "type": "file",
                    "description": "An intermediate report, created and appended to by the Job worker, that logs each validation failure within the retry loop. (Derived from Task 2.8)."
                  },
                  "final_failure.json": {
                    "type": "file",
                    "description": "The final, structured report for a permanently failed Job, written by the Job worker on the last failed attempt. (Derived from Task 2.8)."
                  },
                  "system_context.json": {
                    "type": "file",
                    "description": "A symlink to the main repository's `system_context.json` file. (Derived from Task 2.3)."
                  },
                  "validation_dsl_spec.md": {
                    "type": "file",
                    "description": "A symlink to the main repository's `validation_dsl_spec.md` file. (Derived from Task 2.3)."
                  },
                  "tsconfig.validation.json": {
                    "type": "file",
                    "description": "A temporary TypeScript configuration file dynamically generated by the `tscValidator` engine. It is used to run `tsc` with specific validation-focused settings within the isolated job workspace and is deleted after use. (Derived from Task 2.9, Step 2)."
                  }
                }
              }
            }
          }
        }
      },
      ".ironclad_failures.json": {
        "type": "file",
        "description": "The final aggregated report of all module generation failures for a run."
      },
      "ARCHITECTURE_SPEC.md": {
        "type": "file",
        "description": "The primary input file defining the overall system structure and modules."
      },
      "system_context.json": {
        "type": "file",
        "description": "Provides global context to the IMG for all modules."
      },
      "validation_dsl_spec.md": {
        "type": "file",
        "description": "Defines custom validation rules that generated code must satisfy."
      },
      "dist": {
        "type": "directory",
        "description": "Contains compiled JavaScript output from the 'src' directory, as configured in tsconfig.json."
      },
      ".env": {
        "type": "file",
        "description": "Stores local environment variables, such as the IMG API key. This file is ignored by Git to protect secrets."
      },
      ".gitignore": {
        "type": "file",
        "description": "Specifies intentionally untracked files to be ignored by Git, such as node_modules, build artifacts, and environment files."
      },
      "*.log": {
        "type": "file",
        "description": "A pattern for log files generated during runtime or development, which should be ignored by version control."
      },
      "package.json": {
        "type": "file",
        "description": "The Node.js project manifest file, defining dependencies, scripts, and the executable 'bin' entry for the CLI."
      },
      "tsconfig.json": {
        "type": "file",
        "description": "Configuration file for the TypeScript compiler (tsc), defining compilation options, target version, and file paths."
      },
      "__tests__": {
        "type": "directory",
        "description": "A new top-level directory for project-level tests and test fixtures, separate from generated module tests.",
        "children": {
          "fixtures": {
            "type": "directory",
            "description": "Contains static data files used as input for tests.",
            "children": {
              "contracts": {
                "type": "directory",
                "description": "Contains example ModuleContract JSON files for testing the validator.",
                "children": {
                  "validContract.json": {
                    "type": "file",
                    "description": "An example of a valid ModuleContract file that conforms to the schema."
                  },
                  "invalidContract.json": {
                    "type": "file",
                    "description": "An example of an invalid ModuleContract file that violates the schema."
                  }
                }
              },
              "specs": {
                "type": "directory",
                "description": "Contains static `ARCHITECTURE_SPEC.md` fixture files for testing the Markdown parser.",
                "children": {
                  "valid-spec.md": {
                    "type": "file",
                    "description": "An example of a valid specification file with a 'Modules' section."
                  },
                  "no-modules-section.md": {
                    "type": "file",
                    "description": "An example spec file that is missing the required 'Modules' section."
                  },
                  "empty-list.md": {
                    "type": "file",
                    "description": "An example spec file where the 'Modules' section exists but has no items."
                  }
                }
              }
            }
          },
          "mocks": {
            "type": "directory",
            "description": "A new directory to contain mock servers and other complex test utilities.",
            "children": {
              "img-server.ts": {
                "type": "file",
                "description": "A mock IMG server (e.g., using Express or a similar library) for use in automated end-to-end tests of the full generation pipeline, isolating tests from the real external API. (Derived from Task 2.19)."
              }
            }
          }
        }
      },
      "test-data": {
        "type": "directory",
        "description": "A temporary directory for test fixtures, as proposed in Tasks 1.4 and 1.5. Note: This conflicts with the constitution's established `__tests__/fixtures` directory.",
        "children": {
          "contracts": {
            "type": "directory",
            "description": "Contains contract files for testing the JSON parser.",
            "children": {
              "valid-contract.json": {
                "type": "file",
                "description": "A valid contract file for testing successful parsing."
              },
              "invalid-syntax.json": {
                "type": "file",
                "description": "A contract file with a JSON syntax error for testing error handling."
              },
              "invalid-schema.json": {
                "type": "file",
                "description": "A contract file with a schema validation error for testing error handling."
              }
            }
          },
          "specs": {
            "type": "directory",
            "description": "Contains architecture specification files for testing the Markdown parser.",
            "children": {
              "valid-spec.md": {
                "type": "file",
                "description": "A valid spec file for testing successful parsing of the module list."
              },
              "no-modules-section.md": {
                "type": "file",
                "description": "A spec file missing the 'Modules' section for testing error/edge cases."
              },
              "empty-list.md": {
                "type": "file",
                "description": "A spec file with an empty 'Modules' list for testing edge cases."
              }
            }
          }
        }
      },
      "jest.config.js": {
        "type": "file",
        "description": "The configuration file for the Jest testing framework, introduced in Task 1.11 to define presets like `ts-jest` and the test environment."
      }
    }
  },
  "last_processed_phase": "Phase 2: Core Generation Pipeline: Task Runner, Job Process, and Basic Validation",
  "last_processed_task": "Task 2.17: Implement the final global validation step in the Task Runner (e.g., full project `tsc --noEmit`)."
}